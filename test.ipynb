{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokeniser import Tokeniser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "  Ent⨳ity NormalisationIn Indic Languages\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  Tasmay Pankaj Tibrewal\n",
    "  1. Introduction\n",
    "  Entity normalization is central to many NLP tasks. In Indic languages, the challenge amplifies because we must handle multiple scripts (Devanagari, Tamil, Telugu, etc.), plus localized words for months, currency, numeric expansions, etc. Our end goal is to take sentences containing dates, currencies, and scientific units and produce fully spelled-out text in the same language script. This single problem touches on multilingual NER, text normalization, script detection, numeric expansions, and more.\n",
    "  In this project, we explored three broad strategies:\n",
    "  Agentic (Prompt-based)\n",
    "  Algorithmic (Manual rule-based)\n",
    "  Fine-Tuned LLM (Supervised approach on a synthetic dataset)\n",
    "  We also produced a dataset of roughly 1,600 synthetic examples spanning 10 major Indian languages and multiple domains. Below is an in-depth account of every step.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  2. Data Curation in Extreme Detail\n",
    "  Because good data underpins any approach to entity normalization, we begin by dissecting the data generation and splitting procedures meticulously.\n",
    "  2.1 Motivations and Requirements\n",
    "  Why Synthetic Data?\n",
    "  Rarity of Real Datasets: We needed examples that specifically showcased transformations from numeric or symbolic forms to spelled-out forms in each script. \n",
    "  Controlled Diversity: By instructing a generative model to produce sentences with multiple entity forms (dates, currencies, units) in each language, we can systematically cover a wide range of scenarios.\n",
    "  Domain Variation: We wanted to ensure coverage in “medical,” “news,” “financial,” “scientific,” “legal,” etc. contexts, something that real data might not guarantee without extensive curation.\n",
    "  Language Coverage\n",
    "  We decided to cover 10 languages: Hindi, Tamil, Telugu, Kannada, Malayalam, Odia, Bengali, Gujarati, Punjabi, and Marathi. Each has unique scripts and morphological nuances. For example:\n",
    "  Hindi & Marathi share Devanagari but differ in certain vocabulary.\n",
    "  Gujarati and Bengali have distinct scripts but might share some conceptual expansions for numbers.\n",
    "  Domain Coverage\n",
    "  Data is generated from different domains, ensuring that the output is varied and diverse. These are:\n",
    "  Scientific\n",
    "  Medical\n",
    "  Financial\n",
    "  Literature\n",
    "  General\n",
    "  Technical\n",
    "  Academic\n",
    "  News\n",
    "  Legal\n",
    "  Geography\n",
    "\n",
    "\n",
    "  Entity Diversity\n",
    "  Dates: We needed multiple patterns—DD/MM/YYYY,MM/DD/YYYY, DD-MM-YY, 2nd Jan, ‘March 15, 1990’, etc. This ensures the final model or algorithm can adapt to different real-world date notations.\n",
    "  Currencies: $120, ₹500, INR 700, Rs. 250, €300, ¥1500, etc. The top 10 global currencies, each possibly spelled out differently in each language script.\n",
    "  Scientific Units: From simple (10kg) to compound (10 km/h), plus temperature (20°C), volume (2 litre), weight (lbs, tonne), and more.\n",
    "  2.2 Prompt Engineering for Data Generation\n",
    "  The data was synthetically created using Google AI Studio (Gemini), with carefully structured prompts. A typical master prompt looked something like:\n",
    "  The prompts can be found in the data generation prompts tab. An iterative approach was used to create the data in the chat. Model used for synthetic data generation is `gemini-2.0-flash-thinking-01-21` as it had long context support, is free of cost and has a huuge output window (~65K tokens).\n",
    "  This iterative approach let us produce ~1,600 examples, ensuring distribution across:\n",
    "  Domains: 10 domains in total.\n",
    "  Languages: 10 languages in total.\n",
    "  Temperature: 0.1, 0.4, 0.7, 1.0 (Some examples are short and direct, while others are long or more creative).\n",
    "  2.3 Data Inspection & Quality Checks\n",
    "  Format Validation: Each synthetic record was verified to ensure it had the keys [\"sl. no.\", \"language\", \"input\", \"output\", \"domain\"]. We also introduced a gen_temperature field to track which temperature setting originated in the sample.\n",
    "\n",
    "\n",
    "  Edge Cases:\n",
    "  Some lines had no entity. (E.g., “यह एक सामान्य वाक्य है।” in Hindi, with no numeric content.)\n",
    "  Some lines had multiple (3–4) entities.\n",
    "\n",
    "\n",
    "  Hallucinations: Occasionally, Gemini would produce incomplete JSON or truncated text. We filtered or corrected these by re-prompting or manually discarding them if they were too broken.\n",
    "\n",
    "\n",
    "  Redundancy: Redundancy was low, it was ensured that the whole model had the context of the entire chat. Certain words were found to be repeated in a few sentences; however, either the application of words was in a different context, or there was enough diversity between the sentences. All sentences could not be manually verified, but a sample portion was randomly checked (about 100).\n",
    "  2.4 Splitting into Train & Eval\n",
    "  2.4.1 State Definition\n",
    "  Each sample had a (language, domain, gen_temperature) tuple. We call this a “state”. We aimed for each state to appear in both train and eval in a 3:1 ratio. However, we also had exceptions:\n",
    "  If a state had only 1 sample, we randomly assigned it with 50% probability to train or eval.\n",
    "  If a state had 2 or more samples, we tried a 3:1 ratio. But if that left a state with 0 in eval, we forced at least one sample into eval.\n",
    "  Hence, we ended up with ~1,185 train samples and ~415 eval samples.\n",
    "  Note: A potential shortcoming of the data could be the lack of multiple elements present in each state. Thus requiring a bit more diversity, class balancing and a larger dataset.\n",
    "  2.4.2 Statistical Distribution\n",
    "  We carefully examined these to ensure no major domain-language cluster was starved. The distribution of the total data can be found in the data distribution tab.\n",
    "  2.5 Final Dataset & Access\n",
    "  Size: 1,600 total.\n",
    "  Train: ~1,185.\n",
    "  Eval: ~415.\n",
    "  Format: Provided as CSV with columns sl_no, language, input, output, domain, gen_temperature. Also shared on Hugging Face at Tasmay-Tib/sarvam-entity-recognition-gemini-2.0-flash-thinking-01-21-distill-1600.\n",
    "  Note: Being synthetic, it might not perfectly reflect real usage in morphological or domain-specific complexities. However, it’s still valuable for a first pass at the normalization task.\n",
    "\n",
    "\n",
    "\n",
    "  2.6 Issues Found in the Dataset (Impacting Model Performance)\n",
    "  Limited Very Long Sentences\n",
    "  The dataset mostly contains short to medium-length sentences (under ~100 tokens). Consequently, extremely long sentences (150–200+ tokens) are underrepresented.\n",
    "\n",
    "\n",
    "  Restricted Decimal Number Variety\n",
    "  Although decimal numbers appear in the dataset, they are not comprehensively represented (e.g., 2.5 but rarely 2.75, 3.14159, etc.). This relative sparseness leads to the model mishandling more complex decimal expansions.\n",
    "\n",
    "\n",
    "  Rare Date Formats\n",
    "  Formats like “2 taarik March 2016 ko…” are infrequent. Most examples stick to more standardized forms (DD/MM/YYYY, DD-MM-YYYY, textual month names in scripts, etc.). Hence, the model might fail to parse or transform dates expressed in colloquial or semi-transliterated styles.\n",
    "\n",
    "\n",
    "  Complex or Uncommon Unit Handling\n",
    "  Rare or domain-specific units (e.g., mmHg, mEq/L) are not well-represented. The dataset focuses on more common units (kg, mg, km/h, etc.), so the model may hallucinate or omit expansions for those complex, less frequent units.\n",
    "\n",
    "\n",
    "  Insufficient Numeric Range\n",
    "  Synthetic examples typically use smaller or moderately sized numbers. Very large numbers or close numeric values (e.g., 74 vs. 75) appear only occasionally. This can lead the model to confuse near-similar values, revealing a gap in numeric variety within the training data.\n",
    "\n",
    "\n",
    "  Insufficient number of examples\n",
    "  Although the dataset was relatively diverse, still a larger dataset covering all different sentence types, like uncommon date types, multiple examples for each state. More diversity per language, larger sentences, sentences with decimals and larger numeric ranges, etc. Would have provided a good base for training. Something like ~10K queries would push the model to its maximum.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  3. Three Approaches to Entity Normalization\n",
    "  We tried three approaches to map input -> normalized output for the same dataset. Below, each is explained in detail.\n",
    "  3.1 Agentic (Prompt-Based) Approach\n",
    "  3.1.1 Conceptual Overview\n",
    "  This method uses a single large language model, e.g., unsloth/Meta-Llama-3.1-8B, sarvamai/sarvam-1, and Qwen/Qwen2.5-3B. We set up a chat-like scenario:\n",
    "  We prompted the model in a non-instruct manner since we were using the base model versions. We created a movie-script-like prompt. Containing the back-and-forth between the two main characters. This included:\n",
    "  The main character describing the task\n",
    "  The main character giving an example\n",
    "  The helper character giving a feedback of what they understand\n",
    "  The main character asking to transform a sentence\n",
    "  The helper character giving a response of the sentence\n",
    "  This cycle getting repeated (multi-shot prompting)\n",
    "  We take input the sentence\n",
    "  We analyze the language of the sentence (using an algorithmic language detector - in algorithmic approach) and use the language’s specific prompt to feed the sentence for transformation.\n",
    "  The model attempts to produce an immediate, direct transformation.\n",
    "  Note: The language detector was used to use the stored pre-translated prompt in the same language of the input text, since the model was facing a lot of issues with translation if the prompt included multiple languages (i.e. instruction in one language, examples in other and final sentence in a third one). Also, since this method was only experimental and performed poorly, only Hindi examples were tested (since we had only crafted the Hindi prompt). Still, if the prompt is translated to other languages and stored, the system can also be tested on other language samples. Hindi was chosen just for testing and demonstration purposes, and since this method was not followed, other language samples were not added.\n",
    "  3.1.2 Limitations & Observations\n",
    "  Off-Topic or Extra Text:\n",
    "  The model sometimes appended partial dialogues or new sentences, where it continued the text of the script.\n",
    "  Or it repeated the user’s instructions instead of producing a direct final result.\n",
    "\n",
    "  Poor Numeric Accuracy:\n",
    "  E.g., ¥120 → 120 dollars or messing up the spelled-out date for certain languages.\n",
    "  No specialized training was done, so it’s guesswork.\n",
    "\n",
    "\n",
    "  Inconsistent Across Languages:\n",
    "  In Tamil or Odia, the model might default to transliteration or partial expansions in English.\n",
    "  Variation in performance was high from one prompt to another.\n",
    "  3.1.3 Model performance and agentic use\n",
    "  Llama performed quite well compared to Qwen and Sarvam. For the others, it often converted the entities to words in English and often forgot to convert numbers to words. Thus, an iterative preset set of prompts was stored, which incorporated the model’s new responses and at each step, into updated prompts based on the movie-script structure, instructing on the removal of English words and conversion of numbers to words. This improved Qwen and Sarvam's performance relatively well.\n",
    "\n",
    "  All the prompts for this section can be found in the Agentic prompt tab.\n",
    "  Conclusion: The agentic approach is the easiest to set up but gave the weakest results in systematically normalizing multi-lingual numeric data. It’s a fallback method if one can’t or won’t do fine-tuning or code a rule-based pipeline, but it is not recommended for serious usage.\n",
    "  3.2 Algorithmic (Rule-Based) Approach \n",
    "  3.2.1 High-Level Flow\n",
    "  Script & Language Detection\n",
    "  We first identify which Indic language (Hindi, Tamil, Telugu, etc.) we’re dealing with, typically via script range checks or secondary heuristics.\n",
    "\n",
    "\n",
    "  Date conversion\n",
    "  Dates are recognised by pattern matching in the sentence\n",
    "  Then the forms of these dates are recognised, and a placeholder is attached indicating that it is a date token, to be converted to words in a different manner (later)\n",
    "\n",
    "\n",
    "  Regex & Pattern Searches\n",
    "  We define layered, specialized regexes for dates, currencies, units, and decimal/whole numbers.\n",
    "  Each date match is replaced or “tagged” with a placeholder (e.g. :$date, etc.) while we store the structured parse data in some dictionary.\n",
    "\n",
    "\n",
    "  Language-Specific Spelled-Out Expansions\n",
    "  For each placeholder, we look up the expansions in the appropriate script: e.x. for Hindi: \"$\" -> \"डॉलर\", mg -> \"मिलीग्राम\", etc.\n",
    "  For the numbers, we modify a num2words function from a pre-built library to get the word for the number after the conversion process.\n",
    "\n",
    "\n",
    "  Re-Assembly\n",
    "  After all placeholders are recognized, we replace them one by one in the text. We ensure that spacing, punctuation, or original text ordering is preserved.\n",
    "\n",
    "\n",
    "  Output\n",
    "  We finalize the text with all expansions, checking for leftover unrecognized patterns or possible partial collisions.\n",
    "  Below, we break down each stage in more granular detail.\n",
    "  3.2.2 Language & Script Identification\n",
    "  Most of the algorithm’s expansions are language/script-dependent—for instance, “dollar” in Hindi is “डॉलर,” but in Tamil it’s “டாலர்.” Therefore, we must detect which language we’re dealing with.\n",
    "  Script Range\n",
    "  For each character, check if it falls in a known block (e.g. Devanagari: U+0900–U+097F). If 80%+ of the text’s characters are in the Devanagari range, we guess it’s Hindi or Marathi.\n",
    "  Gurmukhi block: likely Punjabi, etc.\n",
    "  A script confidence score is calculated based on total words found in script X vs script Y.\n",
    "\n",
    "\n",
    "  Disambiguation (if multiple languages share the same script, e.g. Hindi vs. Marathi)\n",
    "  Marathi tends to use a given set of characters or alphabets (ex: े or virama/halant) \n",
    "  Similarly Hindi has some common set of alphabets (ex: ा or nasalization marks)\n",
    "  Similarly, vowel marks have different frequencies in each language.\n",
    "\n",
    "\n",
    "  Probability Calculation \n",
    "  Language’s script-based probabilities are directly calculated based on script distribution and language-specific properties \n",
    "  Each script is previously mapped to a language; thus, the language’s score is directly affected.\n",
    "  If the number of languages corresponding to the script is one, then a direct score is directly given for the percentage of the characters of the script out of total chars\n",
    "  For Hindi and marathi, we start with a base score of 50 and a max adjustment per feature score of 50 (we check for 3 features, two features favoring each language (one is common for both), and for the occurrence of each feature, we find its percentage occurrence, and adjust the score count accordingly\n",
    "  Maximum score adjustment could be ~100 (for complete confidence in two favoring and zero confidence in opposing features, which could take up the maximum language score to be 150). But practically, that is not likely because of consonants and other language-based characters present in the sentence, taking the individual scores down (since they are based on character probability) or increasing the opposing scores.\n",
    "  Still, a maximum threshold of 90 and a minimum threshold of 10 is kept to keep the probabilities from getting skewed.\n",
    "\n",
    "\n",
    "  Further Robustness and Disambiguation\n",
    "  Despite language-based features, we look for a more robust method of language detection\n",
    "  This includes n-gram probability extraction, common word markers count per language, and dictionary-based stop-word matching.\n",
    "  Dictionary-based matching\n",
    "  A dictionary of each language is created based on content available on GitHub, in NLP libraries, or it is just generated using Gemini (for at least 100-250 most common words)\n",
    "  Based on that, simple word matching is performed and checked for the presence of dictionary stop words in the sentence, and a score is calculated based on the percentage of common words in each language.\n",
    "  After the stopwords data was created, it was stored in .txt files with name format (language_stop_words.txt), and then all of this was stored in a zip called 'stop_words_archive.zip'.\n",
    "\n",
    "\n",
    "  Common marker-based matching\n",
    "  We can look for common words or suffixes (“है”, “की”, “में” for Hindi vs. “आहे”, “असतो” in Marathi, “நான்”, “நீ”, “அவள்” in Tamil, etc.).\n",
    "  These are different from dictionary words, since they are looking at which words were present from which language and trying to get a score on the number of unique words per language / total words based on 100 or 200 most common words.\n",
    "  This is just using the markers, which are to be commonly found across, and trying to measure an occurrence score (how frequently markers occur) per language after dividing it by total marker frequency.\n",
    "\n",
    "\n",
    "  Ngram based matching\n",
    "  Ngram is the concept of using n-length character combinations and creating a probabilistic match on top of it.\n",
    "  This requires the total unique set of common markers and unique stop words (from the dictionary) that are joined together with spaces.\n",
    "  Then, the spaces are replaced by (n-1) ‘_’ signs. For example: ‘Hi I am Tasmay’ for n=3 is ‘Hi__I__am__Tasmay’ where each space was replaced with two consecutive underscores ‘__’. \n",
    "  This now creates a text on which we can iterate over a window of length, and for each unique window, we can tally the counts.\n",
    "  After we have the total window counts and the individual count of the unique windows, we have built an n-gram model.\n",
    "  Now, we proceed to do the same thing with the input text and match with the available n-grams. If a match is present, we tally the log probability of the match in our score. Else, we add a log of 1e-20 (a very small number instead of 0 to smoothen the probability)\n",
    "  We divide the score by the total number of n-grams found in the language to get an average score for each language (average score instead of total score to measure the relative quality of each language and not just the quantity part. It may also be that the dictionary/marker size of a language was larger, thus creating a higher absolute match of the n-grams for the language.\n",
    "  Then, we normalize the scores between 0 and 1 by the maximum language score by an exponential normalization technique (escore-max_score).\n",
    "  After this, a final probability score is calculated by dividing language scores by the total scores.\n",
    "  We also experimented with various other smoothening techniques because of the low probabilities and the difficult-to-handle nature of n-gram matches (but eventually settled with this)\n",
    "  For the value of n, we started with n=3 and experimented with n=2 and n=4 and found n=4 to be the best performer. Higher values were not considered since they have quite a high probability of shooting into next words (which may make the use case not ideal).\n",
    "  Note: If we guess incorrectly, expansions might reference the wrong dictionary (e.g., “डॉलर” vs. “डालर”), but typically the matching system was made distinct enough for recognizing major languages accurately (Tamil, Telugu, Kannada, etc.).\n",
    "  3.2.3 Number conversion\n",
    "  For the number-to-text conversion we did:\n",
    "  We use a pre-built library (indic-num2words) to convert numbers into Indian languages.\n",
    "  We even built an improved num2words function incorporating the previous one, to adjust for decimal numbers and date-based numbering as well.\n",
    "  Decimal numbers just included identifying the dots attached to the numbers and then iterating towards the next break, converting them into numbers digit by digit instead of the whole number at once.\n",
    "  For date-based numbering it involved dealing with languages that do include them (Hindi, Gujarati, Marathi, Bengali, Panjabi, Oriya) and that do not (Tamil, Telugu, Kannada, Malayalam) differently. \n",
    "  There was no difference in the number-to-text conversion on the second set.\n",
    "  For the first set, it required separating the number in hundreds instead of thousands (if the hundreds’ value was greater than 0). Ex: 1920 is not ‘ek hazaar nau so bees’ (one thousand nine hundred and twenty) in hindi, but ‘unees sau bees’ (nineteen hundred and twenty). On the contrary, if the hundred position’s value is 0, say for 2020, then the text is ‘do hazaar bees’ (two thousand and twenty) instead of ‘bees sau bees’(twenty hundred and twenty). \n",
    "\n",
    "  3.2.4 Date Matching & Recognition\n",
    "  In the algorithmic approach, dates are processed slightly differently from other numeric or unit-based patterns. Instead of combining them with ordinary numeric tokens directly, we use a specialized pipeline to recognize, interpret, and transform date strings into a standardized textual format, appending the placeholder:$date at the end. Below is a detailed look at how it works, referencing the relevant Python code.\n",
    "  3.2.4.1 Core Ideas and Flow\n",
    "  Regex Identification\n",
    "  The code uses a regular expression to locate any substring that might be a date. A typical pattern is something like:\n",
    "  This aims to find up to three numeric parts, possibly separated by ‘-’, ‘/’, ‘.’, or ‘,’, with optional whitespace in between. These parts can represent:\n",
    "  Two-part dates (e.g., MM/YY, DD/MM, MM/DD, etc.).\n",
    "  Three-part dates (e.g., DD/MM/YY(YY), YYYY/MM/DD, etc.).\n",
    "\n",
    "\n",
    "  Parser Functions\n",
    "  After extracting a candidate date substring, we run it through parsing logic:\n",
    "  parse_date_parts(parts, original_candidate)\n",
    "  Splits the numeric chunks into 2 or 3 elements and dispatches to either parse_three_part_date or parse_two_part_date.\n",
    "  parse_three_part_date(parts)\n",
    "  Tries permutations like DD/MM/YY, MM/DD/YY, YYYY/MM/DD, or a fallback of YY/MM/DD. It ensures that each day, month, or year is valid (e.g., month in 1..12, day ≤ days in that month).\n",
    "  parse_two_part_date(parts, original_candidate)\n",
    "  Allows “half-written” forms (e.g., 05/2005 or 15/04) only if the substring uses / (and not - or . since 1.2 may mean the number and 1-2 may mean something like 1 to 2). This function tries DD/MM, then MM/DD, then MM/YY. The year is guessed with a short-year rule (< 25 => 2000+yy, else 1900+yy).\n",
    "\n",
    "\n",
    "  Validation & Format Tag\n",
    "  The parser checks each combination to ensure that the day is within the maximum allowed for that month (accounting for leap years if the year is fully known). If no valid date structure is found, we discard the candidate and leave it as-is.\n",
    "\n",
    "\n",
    "  Conversion to Textual Format\n",
    "  If a valid date is recognized, we then:\n",
    "  Possibly omit the day or year if they’re not present (for half–written forms).\n",
    "  Use mapping from month number (1–12) to a month name in the language of choice (e.g., in Hindi, 3 -> “मार्च”).\n",
    "  Attach the placeholder:$date at the end. E.g., if we parse 15/03/1990, and the language is set to “Hindi,” the code might transform it to “15 मार्च 1990:$date”.\n",
    "  This ensures that subsequent numeric expansions can handle the “year” portion differently (e.g., special expansions like “उन्नीस सौ नब्बे” instead of “एक हजार नौ सौ नब्बे”).\n",
    "  Regex Replacement\n",
    "  We use a function: replace_dates_in_text() for conversion of the dates in the text into date tagged entities (with recognised formats and converted month words)\n",
    "\n",
    "  It uses a replacer callback that calls convert_date_str(...) on each matched substring. If convert_date_str returns None (invalid date), we revert to the original candidate. Otherwise, we embed the textual date with:$date.\n",
    "  3.2.4.2 Step-by-Step with Key Functions\n",
    "  replace_dates_in_text:\n",
    "  Finds potential 2- or 3-part numeric combos.\n",
    "  For each match:\n",
    "  candidate = match.group(1)\n",
    "  converted = convert_date_str(candidate, lang)\n",
    "  If converted is not None, we replace the original substring with that textual form (including $date).\n",
    "\n",
    "\n",
    "  convert_date_str(date_str, lang=\"hindi\"):\n",
    "  Cleans the string (removing extra spaces around separators).\n",
    "  Splits on [-/.,].\n",
    "  Calls parse_date_parts(parts, date_str).\n",
    "  If parse is successful, it yields (day, month, year, format_tag).\n",
    "  We retrieve the month name from month_names[lang.lower()] or default English names.\n",
    "  Construct the output string as:\n",
    "  \"<day> <month_text> <year>:$date\", omitting day or year if they’re None.\n",
    "\n",
    "\n",
    "  parse_three_part_date(parts):\n",
    "  Attempt each pattern in turn:\n",
    "  DD/MM/YY, MM/DD/YY, YYYY/MM/DD, fallback YY/MM/DD.\n",
    "  Validate the day, month, year. If found valid, return (day, month, year, '...').\n",
    "\n",
    "\n",
    "  parse_two_part_date(parts, original_candidate):\n",
    "  Only triggers if we see / exclusively (no - or .).\n",
    "  Try DD/MM if the second part ≤ 12.\n",
    "  Then MM/DD if the first part ≤ 12.\n",
    "  Finally, if those fail, interpret it as MM/YY.\n",
    "  Return partial (day, month, year, ...), where some might be None.\n",
    "\n",
    "\n",
    "  Date Validation:\n",
    "  Helper functions:\n",
    "  is_leap_year(year) → check if year is leap.\n",
    "  max_day_for_month(month, year) → get the day-limit for that month (handles February).\n",
    "  valid_day_for_month(day, month, year) → ensures day ≤ max allowed.\n",
    "  convert_year_generic(year_str) → short-year logic (if < 25 => 2000 + y, else 1900 + y).\n",
    "  3.2.4.3 Example Partial Conversions\n",
    "  \"15/03/1990\" (Hindi)\n",
    "  Splits → [\"15\",\"03\",\"1990\"].\n",
    "  parse_three_part_date tries dd/mm/yy(yy) → day=15, month=3, year=1990. Valid.\n",
    "  Becomes \"15 मार्च 1990:$date\".\n",
    "\n",
    "\n",
    "  \"05/2023\" (Hindi, half–written)\n",
    "  Splits → [\"05\",\"2023\"].\n",
    "  Only uses /, so it tries dd/mm, mm/dd, or mm/yy.\n",
    "  If 2023 is interpreted as a year via mm/yy logic, it’s “mm=05, year=2023,” so day is None.\n",
    "  Final: \"मई 2023:$date\".\n",
    "\n",
    "\n",
    "  \"12.25.2003\"\n",
    "  Because . is present, we treat it as “three-part numeric,” so either DD.MM.YYYY or MM.DD.YYYY or fallback.\n",
    "  If it’s recognized valid, we convert month=25 => invalid. This returns None, so we revert to the original substring.\n",
    "  Thus, the date substring is replaced in the text with a placeholder-labeled expansion, e.g., \"15 मार्च 1990:$date\". Later, the numeric expansions can see the suffix:$date and apply specialized year expansions (like “उन्नीस सौ नब्बे” vs. “एक हजार नौ सौ...”) in the final pass.\n",
    "\n",
    "\n",
    "  3.2.5 Regex & Pattern Matching\n",
    "  After we’ve identified the language (or at least the script range) in Section 3.2.2, the next step is to scan the text for dates, recognize the date format, convert the month into a word, and tag the date with ‘:$date’ placeholder. This is then followed by the regex and pattern matching part, where we scan recognized patterns—currencies, units, numbers, etc.—and convert them into words. This is typically accomplished by:\n",
    "  Tokenizing the text into chunks (words, punctuation, whitespace).\n",
    "  Combining certain tokens that logically belong together (e.g., comma-separated numbers like 2,000, decimal-separated numbers like 3.14, date patterns like 25/12/2022, currency tokens like ₹500, etc.).\n",
    "  Converting the units, numbers, dates and currency tokens into words (numbers with the help of num2words part) and rest with an extensive set of dictionary containing currency and unit symbols/shorthands mapped to words in each language.\n",
    "  Token Splitting (split_string)\n",
    "  The script’s function:\n",
    "  Uses re.findall to extract:\n",
    "  sequences of word characters,\n",
    "  single non-word, non-whitespace characters (punctuation)\n",
    "  spaces.\n",
    "  This ensures the text is broken into tokens: words, punctuation, and whitespace, each captured separately. It’s important because we often want to preserve exact spacing and punctuation when reassembling the final string.\n",
    "  Combining Date Tokens (combine_date_tokens)\n",
    "  This function specifically looks for a pattern like [ '1995', ':', '$', 'date' ] and merges them into a single token: \"1995:$date\".\n",
    "  Rationale: We interpret :$date as a marker that signals a year or partial date string is truly meant to be a date that should be spelled out differently (like for “day month year” expansions in our improved num2words function).\n",
    "  It iterates through tokens, skipping whitespace, and tries to detect the three consecutive tokens \":\", \"$\", \"date\" in order. If found, it merges them with the preceding numeric token.\n",
    "  This keeps the date references compact, e.g., \"1995:$date\" or \"02/05/1998:$date\", so we can do specialized expansions later (like “उन्नीस सौ पंचानवे” or “दो मई उन्नीस सौ अट्ठानवे,” etc.).\n",
    "\n",
    "\n",
    "  Combining Comma-Separated Numbers (combine_comma_separated_numbers)\n",
    "  Often in Indian numeric formatting, you see 1,000, 2,50,000, etc. The code merges [ '2', ',', '000' ] into [ '2000' ].\n",
    "  This is a simple pass that checks if a token is a digit, and if the subsequent tokens are “, plus more digits`,” merges them all into a single numeric token.\n",
    "  E.g.: ['2', ',', '000'] → ['2000'].\n",
    "  Combining Dot-Separated Numbers (combine_dot_separated_numbers)\n",
    "  Similar to commas, it merges [ '3', '.', '14' ] into ['3.14'].\n",
    "  Specifically, it checks if a token is purely digits and if the next tokens form a “. + digits” pattern. If so, it concatenates them into e.g. \"3.14\"`.\n",
    "  This step helps us handle decimals in an earlier pass.\n",
    "  Currency Combination (combine_currency_tokens)\n",
    "  Pattern A: <number> <currency token>\n",
    "  Pattern B: <currency token> <number>\n",
    "  The code checks:\n",
    "  If a token is a recognized currency symbol or abbreviation ($, ₹, usd, inr, etc.), it checks whether it’s followed or preceded by a digit token.\n",
    "  We keep a global dictionary, e.x.:\n",
    "    1) currency_normalization['₹'] = 'inr' or currency_normalization['rs'] = 'inr' or  currency_normalization['inr'] = 'inr', … etc.\n",
    "  2) currency_language_mapping['usd']['hi'] = \"डॉलर\"\n",
    "  This helps us track all different currency symbols and shorthands, from which we can convert them to words in the respective language already identified earlier.\n",
    "  The function merges them into a single token with the numeric part plus the spelled-out currency (like \"500 डॉलर\").\n",
    "  If a currency token is standalone, it might simply convert $ → “डॉलर” in the chosen language.\n",
    "  One nuance: trailing dots are removed (e.g., “Rs.” is recognized as “rs,” then mapped to “inr,” etc.).\n",
    "  Because the user might place the currency either before or after the numeric part, the function checks for both patterns.\n",
    "  All edge cases involving spaces and dots are checked, for more robustness.\n",
    "  Unit Combination (combine_unit_tokens)\n",
    "  Pattern A: <number> <unit token>\n",
    "  Pattern B: <unit token> <number>\n",
    "  The script has a big unit_normalization, unit_language_mapping and a unit_variants dict, e.g.:\n",
    "    1) unit_variants[\"meter\"] = ['m', 'mtr', 'mtrs', 'metre', 'meter', 'metres', 'meters'], this contains shorthands and symbols (in thousands) for a range of different units (more than 100).\n",
    "      2) unit_normalization[\"metre\"] = \"meter\", this contains the units listed out in an exhaustive manner, with some common alternate versions. \n",
    "      3) unit_language_mapping consists of all of these units in unit_normalisation and maps them to different languages based on their language code.\n",
    "    e.x.: unit_language_mapping[“meter”][“hi”] = \"मीटर\"\n",
    "  If we find “500mg,” we first see the number “500” and the unit “mg.” Once recognized, we produce something like “पाँच सौ मिलीग्राम” in Hindi.\n",
    "  The function tries to accumulate multi-character units (like “km/h,” “kg,” “tonnes”). It also accounts for possible spacing or punctuation (., etc.) in between.\n",
    "  Because the user might place the unit either before or after the numeric part, the function checks for both patterns. If only the unit is found, only the unit is converted to word form.\n",
    "  All edge cases involving spaces and dots are checked for more robustness.\n",
    "  Checking if Token is Numeric (is_number)\n",
    "  This is a helper that checks if the token is a number, ensuring that if we have one (or zero) decimal point in the token, it’s considered numeric.\n",
    "  If it’s purely digits or digits with one dot, we treat it as a candidate number.\n",
    "  If there are any spaces, it is not considered a number since spaces could mean that the dot is actually a full stop, and by default, spaces would not have been incorporated into the numerical token.\n",
    "  Main Pipeline to Convert Tokens (convert_numbers_to_words)\n",
    "  Finally (this is the function to convert the normalized sentence after date replacement (shown later) into words, mostly involving numbers, currencies, placeholders, and units along with simple text):\n",
    "  def convert_numbers_to_words(text, lang='hi'):\n",
    "      tokens = split_string(text)\n",
    "      tokens = combine_date_tokens(tokens)\n",
    "      tokens = combine_comma_separated_numbers(tokens)\n",
    "      tokens = combine_dot_separated_numbers(tokens)\n",
    "      tokens = combine_currency_tokens(tokens, lang=lang)\n",
    "      tokens = combine_unit_tokens(tokens, lang=lang)\n",
    "      ...\n",
    "      # second pass to expand leftover numeric tokens\n",
    "\n",
    "  Tokenize.\n",
    "  Combine date placeholders, comma numbers, decimal numbers, currency tokens, and unit tokens.\n",
    "  Expand any pure numeric tokens using the function improved_num_to_word(...).\n",
    "  Reassemble them while preserving spacing (by rejoining them carefully—note it uses ''.join(new_tokens) but makes sure whitespace tokens remain intact).\n",
    "  This chain of transformations effectively merges partial tokens into single tokens representing recognized entities (dates, decimals, currency references, unit references) so they can be spelled out or processed further. This is a post-processing step to the date-to-word conversion step, which involves recognizing the date pattern, understanding the date format, replacing the month with its word, and attaching a date placeholder.\n",
    "  3.2.6 Placeholder Replacement\n",
    "  We store the date placeholder:$date, in the date recognition part which is removed from the final converted sentence. The concept is as follows:\n",
    "  Dates can become, e.x.: “12 May 1998:$date,” then we handle them in a separate pass.\n",
    "  Alternatively, we merge numeric + currency into a single token, e.x.: \"500 डॉलर\" which is effectively a “placeholder” for the currency expansion.\n",
    "  Where placeholders appear:\n",
    "  Specifically for dates, we use patterns like:$date appended to the final token in combine_date_tokens. Then, in improved_num_to_word(...), if it sees:$date, it triggers date-specific expansions (like “सन् 1947 ईस्वी” or “उन्नीस सौ सैंतालीस”).\n",
    "  Hence, the placeholder mechanism is integrated into the numeric expansions, ensuring a date-year is spelled out according to each language’s century rules. If no placeholder is found, it’s treated as a normal numeric token.\n",
    "  3.2.7 Detailed Example\n",
    "  Now we’ll walk through a complete pipeline example that incorporates date recognition, numeric expansions, currency/unit detection, and reassembly. Consider the input:\n",
    "  \"अनिल का जन्म 15/03/1990 को हुआ। उसने $2,000 बचाए थे, और दो दिन बाद 18/03/1990 को (मित्रों से 100lb उधार लेकर) 2.5km चला।\"\n",
    "\n",
    "  Step 1: Date Recognition\n",
    "  The function replace_dates_in_text with the regex:\n",
    "  pattern = r'(\\d{1,4}\\s*[-/.,]\\s*\\d{1,4}(?:\\s*[-/.,]\\s*\\d{1,4})?)'\n",
    "  finds:\n",
    "  \"15/03/1990\"\n",
    "  \"18/03/1990\"\n",
    "\n",
    "\n",
    "  For each match, we call convert_date_str.\n",
    "  \"15/03/1990\" → parse as DD/MM/YYYY. day=15, month=3, year=1990 → \"15 मार्च 1990:$date\".\n",
    "  \"18/03/1990\" → day=18, month=3, year=1990 → \"18 मार्च 1990:$date\".\n",
    "  The text becomes:\n",
    "  \"अनिल का जन्म 15 मार्च 1990:$date को हुआ। उसने $2,000 बचाए थे, और दो दिन बाद 18 मार्च 1990:$date को (मित्रों से 100lb उधार लेकर) 2.5km चला।\"\n",
    "  Step 2: Converting Numbers and Entities\n",
    "  We pass the above string into convert_numbers_to_words(text, lang='hi'), which:\n",
    "  Tokenize:\n",
    "  Splits on words, punctuation, whitespace (like \"अनिल\", \" \", \"का\", \" \", etc.).\n",
    "\n",
    "\n",
    "  combine_date_tokens:\n",
    "  Looks for patterns [ '1990', ':', '$', 'date' ] → merges → [\"1990:$date\"].\n",
    "  E.g., the substring [ \"15\", \" \", \"मार्च\", \" \", \"1990\", \":\", \"$\", \"date\" ] becomes [ \"15\", \" \", \"मार्च\", \" \", \"1990:$date\" ].\n",
    "  Now we have tokens like \"15\", \"मार्च\", \"1990:$date\", etc.\n",
    "\n",
    "\n",
    "  combine_comma_separated_numbers and combine_dot_separated_numbers:\n",
    "  Merges [ '2', ',', '000' ] into [ '2000' ] if any.\n",
    "  Merges [ '2', '.', '5' ] into [ '2.5' ].\n",
    "  In our example, $2,000 → tokens might be [\"$\", \"2\", \",\", \"000\"] → eventually [\"$\", \"2000\"]and  2.5km → [\"2\", \".\", \"5\", \"km\"] → eventually [\"2.5\", \"km\"].\n",
    "\n",
    "\n",
    "  combine_currency_tokens:\n",
    "  Sees a pattern like [\"$\", \"2000\"]. $ -> \"usd\", but with currency_language_mapping[\"usd\"][\"hi\"] = \"डॉलर\".\n",
    "  Eventually merges into a single token \"2000 डॉलर\" or sets up for numeric expansion.\n",
    "\n",
    "\n",
    "  combine_unit_tokens:\n",
    "  For [ \"2.5\", \"km\" ], we check if “km” is recognized. Then we produce something like \"2.5 किलोमीटर\".\n",
    "  If “lb” is recognized → 'lb' -> 'पाउंड' in Hindi or 'पाउण्ड' depending on the dictionary.\n",
    "  So [\"100\", \"lb\"] might become \"100 पाउंड\".\n",
    "\n",
    "\n",
    "  Final Numeric Expansion:\n",
    "  If a token is \"2.5\", we call improved_num_to_word(\"2.5\", \"hi\") → “दो दशमलव पाँच.”\n",
    "  If a token is \"2000\" → “दो हज़ार.”\n",
    "  If a token is \"1990:$date\", improved_num_to_word sees :$date and does special date-year expansions (like “उन्नीस सौ नब्बे” in Hindi). The presence of :$date triggers a different rule that might handle century logic (for Indo-Aryan languages we might do “सन् ... ईस्वी”).\n",
    "  Hence we get a final text something like:\n",
    "  \"अनिल का जन्म पंद्रह मार्च उन्नीस सौ नब्बे को हुआ। उसने दो हज़ार डॉलर बचाए थे, और दो दिन बाद अठारह मार्च उन्नीस सौ नब्बे को (मित्रों से एक सौ पाउंड उधार लेकर) दो दशमलव पाँच किलोमीटर चला।\"\n",
    "  Notice each numeric piece (15, 1990:$date, 2000, 100, 2.5) is spelled out in Hindi.\n",
    "  The month expansions (“मार्च”) came from date recognition, and the year expansions used the date-based numeric logic with :$date placeholders.\n",
    "  Currency $ turned into “डॉलर,” and lb turned into “पाउंड.”\n",
    "  In summary, the final pipeline for a complex input with multiple date references is:\n",
    "  replace_dates_in_text to parse, interpret, and transform date patterns into a textual <day> <month> <year>:$date form.\n",
    "  convert_numbers_to_words to handle date placeholders, numeric expansions, currency expansions, and unit expansions.\n",
    "  Reassemble tokens carefully to preserve the original spacing and punctuation.\n",
    "  This fully algorithmic approach ensures no hallucination while guaranteeing correct expansions for recognized patterns—dates especially, thanks to a specialized date parser and textual month-labelling.\n",
    "  3.2.8 Error Handling & Fallbacks\n",
    "  Unknown Patterns: If we see something like “4.2c/s” but “c/s” isn’t in the unit dictionary, we skip or partially convert only the number.\n",
    "\n",
    "\n",
    "  Partial Overlaps: If a date also includes a currency symbol (rare but possible, e.g., “12/03-1990$”), the system might incorrectly parse. Typically, we do multiple passes or design one big combined regex to avoid collisions. Though a very thorough and detailed approach is kept to tackle special symbols, ‘-’, ‘$’, spaces, numbers, etc. And a very detailed method is laid out for the procedure to go about for parsing. Still, in certain cases, a rule-based approach may not be enough (either due to the absence of certain rules, not thought for (which can be manually added for a more robust system), or due to the semantic nature of parsing the sentence for which rules may not suffice)\n",
    "\n",
    "\n",
    "  Language Mismatch: If the text is partially English or code-mixed, expansions might still appear in the guessed language script. We can either do partial expansions or fallback.\n",
    "\n",
    "\n",
    "  Spacing: We must ensure that after removing the pattern from the text, we place the expanded output with correct spacing. Some code uses sub with capturing groups to handle the spacing elegantly (e.g., a capturing group for leading/trailing spaces).\n",
    "  3.2.9 Strengths & Drawbacks Revisited\n",
    "  Strengths:\n",
    "  No Hallucination: We only output expansions for recognized patterns.\n",
    "  Absolute Accuracy for known forms: If “$120” is in the dictionary, we do it right 100% of the time.\n",
    "  Lightweight: No GPU or large model needed. Typically runs in near real-time.\n",
    "\n",
    "\n",
    "  Drawbacks:\n",
    "  Coverage: Any new format or rare domain (like “c/s”, “ZAR 500”) must be manually added.\n",
    "  Maintenance: For 10 languages, each new date or currency style becomes a chunk of new code or dictionary expansions.\n",
    "  Code Switching or multi-lingual sentences are not easily handled, as each script’s expansions might conflict or overlap.\n",
    "  Despite these drawbacks, many real-world systems rely on rule-based expansions where stable, guaranteed correctness is paramount. For new or unstructured data, an LLM can fill coverage gaps, but the rule-based method remains a strong fallback or post-processor.\n",
    "  3.2.10 Potential Extensions & Hybrid Approaches\n",
    "  Hybrid Pipeline:\n",
    "  Let the LLM generate expansions, then parse that output with a “checker mode” of this rule-based system. If expansions deviate from recognized patterns, correct them.\n",
    "\n",
    "\n",
    "  Auto-Generation of Regex:\n",
    "  Some advanced systems attempt to parse user logs to auto-update the dictionary or patterns for new currencies/units. This reduces maintenance.\n",
    "\n",
    "\n",
    "  Language ID:\n",
    "  For code-mixed text (“He spent $120 रूपये”), we can attempt segment-level detection. e.g., if a chunk is in Devanagari, we treat expansions in Hindi, else in English. This can become complex in practice.\n",
    "  3.2.11 Key Takeaways\n",
    "  The algorithmic system is extremely precise for in-distribution, recognized patterns.\n",
    "  Multiple passes or a single mega-regex can capture dates, currency, units, decimal expansions, numeric expansions, etc.\n",
    "  The approach requires carefully curated dictionaries (month names, currency expansions, unit expansions) for each Indic language.\n",
    "  For real production usage, pairing a rule-based system with an LLM’s more generalized coverage can yield near-total reliability.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  3.3 Fine-Tuned LLM Approach\n",
    "  Goal: Teach an LLM the transformation from numeric forms to spelled-out forms by exposing it to hundreds of examples from the synthetic dataset.\n",
    "  3.3.1 Base Model\n",
    "  We used “unsloth/Meta-Llama-3.1-8B” in 4-bit quantization. It’s large enough to handle multi-lingual tasks decently but still possible to fine-tune in ~21GB VRAM (with 24 batch size). The quantization (bitsandbytes 4-bit) ensures minimal memory usage. Memory required for model inference is <6GB on GPU.\n",
    "  3.3.2 Data & Prompt Format\n",
    "  We fed each training example in the prompt. So that the model can easily understand what is to be done and can train understanding the information. The sample prompt can be found in the finetuning prompt tab.\n",
    "  We included a single example in the prompt heading as well (“15/03/1990 को…”). The training loop sees 1,185 of these examples, each in multiple epochs. The eval set is 415 examples.\n",
    "  Here’s an updated version of the Hyperparameters & Training Cost section, clarifying that the so-called “crashed” run wasn’t forcibly stopped but did, in fact, yield surprisingly good (though unstable) results:\n",
    "  3.3.3 Hyperparameters & Tuning Runs\n",
    "  We conducted multiple major training runs, tuning key hyperparameters such as learning rate, weight decay, warmup steps, and LR schedulers. Below is a breakdown of how those runs evolved and why.\n",
    "  Common Settings Across Runs\n",
    "  Compute Metrics: We monitored Training Loss, Evaluation Loss (primary), plus CHRF, CHRF++, BLEU, WER, CER on the eval set.\n",
    "  Optimizer: AdamW (8-bit) for reduced memory usage.\n",
    "  Max Sequence Length: 2048 (though typical input lengths rarely exceeded ~594 tokens).\n",
    "  Packing: False (no multi-sample packing).\n",
    "  dataset_num_process: 2 (minor parallelism).\n",
    "  Per-Device Train Batch Size: Usually 16 early on, and then increased to 24 in later runs when we realized we had leftover VRAM.\n",
    "  Gradient Accumulation: With gradient accumulation of 4, the effective batch size is 16 * 4 = 64 or 24 * 4 = 96.\n",
    "  Epochs: Initially up to 10, but often set to 7 or 8 in later runs to save time and refine.\n",
    "  Precision: bf16 = True (with fp16 = False) for improved numerical stability.\n",
    "  Seed: 3407 for reproducibility.\n",
    "  Logging Steps: 1 (frequent logging).\n",
    "  Eval Steps: Typically 2, changed to 4 in some runs for time savings.\n",
    "  Load Best Model at End: True, ensuring the best eval_loss checkpoint is reloaded.\n",
    "  Save Strategy: By steps, typically save_steps = 4.\n",
    "  Metric for Best Model: eval_loss (lower is better).\n",
    "  Note: Another custom metric was used to check the model performance (based on eval and train loss): cutom_metric (squared_eval_to_train_loss_ratio = eval_loss2/train_loss): 0.1312. It minimizes both the eval_loss and the ratio of eval to train loss (signifying overfit). this mostly matches the best performance across metrics (thus, this metric, when good, is often when all the other given metrics are in their best spots). found to be consistent from experimentation across 46 training runs.\n",
    "  One major drawback of this is that it often goes wrong on sudden peaks in train loss.\n",
    "  an improvement is to use (eval_loss2)/(min(train_lossj )) for j ranging from 1 to i. This is often a better estimate. here eval_lossx and train_lossx signifies the respective losses at step = x.\n",
    "  An even better estimate ranges from max(0, i-k) to i. where k is a hyper-parameter decided by the user based on the volatility of the training run and the number of total steps.\n",
    "  Individual Runs\n",
    "  Orange is ‘sarvam_training_run_main2’ and Yellow is ‘sarvam_training_run_main1’.\n",
    "  Below is an overview of the major runs (numbered 1 through 5), plus notes on a “crashed” run that ended unexpectedly but still showed strong partial performance.\n",
    "  Run 1\n",
    "  Learning Rate: 2e-4\n",
    "  Warmup Steps: 20\n",
    "  Weight Decay: 0.01\n",
    "  LR Scheduler Type: Linear\n",
    "  Rationale:\n",
    "  A moderate LR (2e-4) with ample warmup (20 steps) and linear decay—intended to prevent early instability. This was the baseline for comparison.\n",
    "  Result:\n",
    "  Training was stable but somewhat slow to converge.\n",
    "  Indicated that we could push the LR higher to speed up improvement.\n",
    "  Run 2\n",
    "  Learning Rate: 4e-4 (doubling from run1)\n",
    "  Warmup Steps: 20\n",
    "  Weight Decay: 0.01\n",
    "  LR Scheduler Type: Linear\n",
    "  max_grad_norm: 1.0 (to prevent exploding gradients)\n",
    "  dataloader_pin_memory: True (faster GPU transfer)\n",
    "  Rationale:\n",
    "  Since run1 was stable but slow, we doubled LR to 4e-4. We kept warmup at 20 steps for a controlled slope and introduced grad norm clipping to safeguard training.\n",
    "\n",
    "  Result:\n",
    "  Quicker convergence than run1 without major instabilities.\n",
    "  Reasonable final losses, but we still suspected we could push LR even more.\n",
    "  Run 3 (Crashed, but Produced Good Results)\n",
    "  Learning Rate: 4e-4\n",
    "  Warmup Steps: 5 (much lower)\n",
    "  Weight Decay: 0.01\n",
    "  LR Scheduler: Cosine\n",
    "  Rationale:\n",
    "  Here, we tested a short warmup to ramp LR up very quickly, plus a cosine schedule for a smoother later-phase decay. The plan was to accelerate early learning.\n",
    "  Result:\n",
    "  The training ended unexpectedly early (“crashed”), presumably due to an abrupt LR ramp plus not enough warmup steps.\n",
    "  Partial logs showed performance/metrics that were too good before it crashed, suggesting the model was learning quickly but on the edge of stability.\n",
    "  On retrying the run, results were produced that were good but not as good as the crashed version.\n",
    "  Run 3 (New Variation with slight changes from previous)\n",
    "  Learning Rate: 1.5e-3 (much higher than 4e-4)\n",
    "  Warmup Steps: 7 (a bit more than 5, but still short)\n",
    "  Weight Decay: 0.03\n",
    "  LR Scheduler: Cosine\n",
    "  Rationale:\n",
    "  Post-“crash,” we tried an even higher LR but compensated with more weight decay (0.03) to rein in potential overfitting and partial gradient explosion. We also used 7 warmup steps, since the lr would increase quite fast, and to compensate for it.\n",
    "  Result:\n",
    "  Very fast convergence and decent final results, but with spikes in the loss curve.\n",
    "  Still could not match the results from the “lucky” crash run.\n",
    "  Run 4\n",
    "  Learning Rate: 2e-3 (increased further)\n",
    "  Weight Decay: 0.001 (significantly lower)\n",
    "  Warmup Steps: 10\n",
    "  LR Scheduler: Cosine with Restarts\n",
    "  Epoch: 8\n",
    "  Rationale:\n",
    "  From experience, a high LR can accelerate initial learning, but you risk overshoot. Cosine with restarts reboots the LR periodically, allowing re-convergence if the model starts overfitting or flattening out. The lower weight decay (0.001) counters the prior run’s heavy penalty (0.03), which showed to be experimentally worse.\n",
    "  Result:\n",
    "  Rapid initial improvement.\n",
    "  8 epochs gave the system, a shorter train time, and a faster learning rate degradation by cosine curve.\n",
    "  Some overshoot at earlier epochs, but it typically settled near the end.\n",
    "  Results similar to that of the crashed run\n",
    "  More unstable training run with more peaks, and a lower eval loss to due overfitting later on in the run.\n",
    "  Run 5\n",
    "  Learning Rate: 1.6e-3\n",
    "  Weight Decay: 0.005\n",
    "  Warmup Steps: 8\n",
    "  LR Scheduler: Cosine with Restarts\n",
    "  Epoch: 7\n",
    "  Rationale:\n",
    "  Refining from run4:\n",
    "  Slightly lower LR (1.6e-3) for more stability.\n",
    "  Weight decay at 0.005, balancing the extremes of 0.03 vs. 0.001, preventing overfitting.\n",
    "  Lower number of warmup steps, for a faster rise (required early) and earlier degradation start off (for later stability) of the learning rate.\n",
    "  7 epochs to keep the total time short and further the effect of a faster declining learning rate on model overfit and training stability.\n",
    "  Result:\n",
    "  One of our best overall runs, with high performance across all metrics, better stability and a lower overfit.\n",
    "  Had a better performance (arguably) than the crashed 3rd run.\n",
    "  Found to be reproducible in the reproducibility runs\n",
    "  Chosen as our main final model for subsequent usage (GGUF conversion, inference tests, predictions, etc).\n",
    "\n",
    "  Potential Future Tuning\n",
    "  Shorter Runs: Running only 2–3 epochs if we want a quick improvement over the base, and a way faster lr decline for a way more stable run.\n",
    "  Richer Data: If the dataset grows to ~10k examples, we might need even more epochs or refined schedules.\n",
    "  3.3.4 Training Time & Cost\n",
    "  All these experiments were done on an Nvidia L4 GPU. Notable points:\n",
    "  Steps/Epoch: ~84, given an effective batch size of 96 (24 per device × 4 gradient accumulation).\n",
    "  Time/Epoch: ~2 hr 20 min (140 minutes), factoring in both forward/backward passes and partial evaluation.\n",
    "  Eval Steps: ~42 each epoch, taking ~89 seconds each → ~62 min total for eval.\n",
    "  Total for 7 Epochs: ~202 minutes (~3 hr 22 min)\n",
    "  For 20 step (best step before overfit begins, by custom metric): ~20 * 140 / 84 → ~ 33 minutes and 20/2 = 10 eval steps ~ 10 * 89 → ~15 min for eval. Thus a total runtime of ~ 48 minutes.\n",
    "  Detailed Cost Calculation\n",
    "  Assuming the platform’s billing model uses “units”:\n",
    "  Cost of L4: 2.4 units/hour = 0.04 unit/minute.\n",
    "  1 Unit = $10/100 = 10 cents.\n",
    "  7-Epoch Training (~202 minutes):\n",
    "  202 min × 0.04 unit/min × 10 cent/unit = $0.808\n",
    "  ~ Rs. 70 (if $1 ~ Rs. 86.5).\n",
    "  Model Checkpoint (~48 min to get best partial step):\n",
    "  48 min × 0.04 × 0.1 = $0.192 (~Rs. 16.6).\n",
    "  Total for multiple runs or additional trials (total 5 major runs + 1 reproducibility run + 1 crashed run + 39 other runs = 46 runs): ~$9.6 (~Rs. 830.4) if we sum up extended experiments.\n",
    "  Resource Standpoint:\n",
    "  The model loads in ~5–6GB of RAM and handles short inference requests at minimal overhead.\n",
    "  After training, we merged the LoRA adapter into the base model and quantized to Q4_K_M (GGUF).\n",
    "  We can deploy on CPU via llama.cpp, incurring no monthly GPU hosting cost on a free Hugging Face Space. The model loads in ~2GB of RAM (without –mlock, which forces offloading to stop, increasing inference speeds) due to storage offloading and memory mapping. This approach marries cheap GPU-based training with free CPU-based inference.\n",
    "  System Setup time - initially, model download, running .sh files, etc., ~6-7 minutes of initial setup time with < 15 seconds of model load time on refresh (model load time and running the first prompt for caching in RAM). Hugging face stops the face on 48 hours of no usage, then the full setup is again required on loading. Response time during inference is drastically reduced due to prompt caching and multi-threading. (However in the spaces only two cores are present and thus it is recommended to host on higher core systems, modern systems with 8 or 16 cores, would give much faster results)\n",
    "  3.3.5 Results & Potential Issues\n",
    "  Accuracy:\n",
    "  For typical patterns (DD/MM/YYYY, $120, 500mg), near absolute correctness.\n",
    "  Model Metrics at checkpoint:\n",
    "  train_loss: 0.101\n",
    "  eval_loss: 0.11551\n",
    "  cer: 0.12292\n",
    "  wer: 0.09581\n",
    "  bleu: 0.87392\n",
    "  chrf: 94.0154\n",
    "  chrf++: 93.78756\n",
    "  cutom_metric (squared_eval_to_train_loss_ratio = eval_loss2/train_loss): 0.1312\n",
    "  Fumbles on:\n",
    "  Rare or strange date formats or complex scientific units.\n",
    "  Very large sentences (~200+ tokens)\n",
    "  Confuses between similar numbers and large decimal expansions\n",
    "  Over-generation (the model might keep generating instructions after finishing)\n",
    "  In short, the fine-tuned approach is flexible and covers more variety than the rule-based approach (if it was in the training data). However, if it sees something truly outside its training distribution, it might guess or hallucinate.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  Reproducibility run plots:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  4. Training & Evaluation Metrics\n",
    "  4.1 Metrics Chosen\n",
    "  We meticulously tracked:\n",
    "  Training Loss & Eval Loss: Classic measure of how well the model fits the data.\n",
    "  BLEU: Word n-gram precision. Good for measuring overall textual overlap.\n",
    "  CHRF/CHRF++: Character-based F-scores, essential in highly inflected or morphological languages.\n",
    "  CER: Character Error Rate, i.e., edit distance at the character level. If CER is 0.12, it means 12% of the characters differ from the reference.\n",
    "  WER: Word Error Rate. Possibly ~0.08–0.1, meaning 8–10% of the words differ from reference.\n",
    "  Why so many? Because partial expansions might get a high BLEU but fail at the character level. CHRF, CER, WER each provide different vantage points on alignment with the reference.\n",
    "  4.2 Example Metric Snapshots\n",
    "  A “best checkpoint” from run5 might show:\n",
    "  train_loss = 0.101\n",
    "  eval_loss = 0.115\n",
    "  bleu = 0.874\n",
    "  chrf = 94.015\n",
    "  chrf++ = 93.788\n",
    "  cer = 0.123\n",
    "  wer = 0.096\n",
    "  Interpretation: ~12.3% of characters differ from reference, which is quite decent for large multi-lingual expansions. The model is basically correct ~88–90% of the time for tricky expansions.\n",
    "  4.3 Additional Logs\n",
    "  We also tracked:\n",
    "  grad_norm: to see if the gradients were exploding or not.\n",
    "  learning_rate: verifying the scheduler’s shape.\n",
    "\n",
    "  5. Model Deployment & GGUF Conversion\n",
    "  5.1 Why Convert to GGUF?\n",
    "  Once the model is fine-tuned, we want to deploy it for inference in a cost-free environment (like CPU-only Hugging Face Spaces). The “gguf” format, used by llama.cpp, is a CPU-friendly quantized format that:\n",
    "  Reduces memory usage further than typical GPU quantization.\n",
    "  Permits near real-time inference for short sequences on 2 CPU cores.\n",
    "  Minimizes hosting cost. (We avoid renting GPUs indefinitely.)\n",
    "  Hence we used unsloth’s built-in function push_to_hub_gguf(...) with quantization_method=\"q4_k_m\". This yields a ~5–6GB model file.\n",
    "  5.2 HF Spaces Setup in Depth\n",
    "  5.2.1 Repository Structure\n",
    "  We have a space like Tasmay-Tib/sarvam-ai-entity-normalisation on Hugging Face. Its files include:\n",
    "  app.py (Streamlit app):\n",
    "\n",
    "\n",
    "  A text input for the user.\n",
    "  A function infer() that crafts the prompt (the same “instruction + input” used in training).\n",
    "  Submits this prompt to the local server via a curl POST request to http://localhost:8081/completion.\n",
    "  Displays the returned text to the user.\n",
    "  init.sh:\n",
    "\n",
    "\n",
    "  Clones llama.cpp.\n",
    "  Builds llama-server.\n",
    "  Downloads the .gguf model from Hugging Face.\n",
    "  Launches llama-server on port 8081.\n",
    "  init2.sh:\n",
    "\n",
    "\n",
    "  If the environment restarts, re-check if llama.cpp is present, re-launch the server.\n",
    "  requirements.txt: Minimally lists dependencies (requests package is the only one).\n",
    "\n",
    "\n",
    "  5.2.2 Free Tier Challenges\n",
    "  On free Spaces, the container stops after inactivity. We used an environment variable ran_script_once to skip re-downloading each time. But if the container is truly restarted, it has to recompile. Users might wait a few minutes for the server to initialize. After that, requests are fairly quick for short sentences.\n",
    "  The whole setup takes about 6-7 minutes to complete. On loading the model once, it stays for the next 48 hours, else Hugging Face will turn it off. Since no permanent storage is subscribed to it will have to reload if started after that. Otherwise, the model takes less than 15 seconds to load.\n",
    "  In the free tier a big challenge is inference speeds, for which we have switched off the ram offloading option (enabling the whole model to be loaded on RAM), enabled multi-threading and implemented prompt caching for the instruction, example part of the prompt to be preloaded, for quick loading of the model.\n",
    "  5.2.3 End-User Experience\n",
    "  Wait-time: The server gets setup (if not already), or the model is spun up for serving and the prompt cached (if it is already setup).\n",
    "  User: “I have a sentence: 20/02/2023 को मैंने ₹500 में 3lbs चीनी खरीदी।”\n",
    "  They press “Submit” or click enter on the input box.\n",
    "  The App: constructs the big prompt and does curl --data '...json...' http://localhost:8081/completion.\n",
    "  llama-server: loads the model in CPU (already loaded when shown to the user), runs the inference, returns the spelled-out text.\n",
    "  The App: displays something like “बीस फरवरी दो हजार तेईस को मैंने पाँच सौ रुपये में तीन पाउंड चीनी खरीदी।”\n",
    "  User: “Copy Output” button to get it in the clipboard.\n",
    "\n",
    "\n",
    "\n",
    "  6. Comparative Analysis & Discussion\n",
    "  6.1 Which Method Excels Where?\n",
    "  Agentic:\n",
    "  Pros: Zero additional code or training. Just prompt engineering.\n",
    "  Cons: Inconsistent, no guaranteed coverage, messy for multi-lingual expansions.\n",
    "  Algorithmic:\n",
    "  Pros: Deterministic, absolutely correct for known patterns, easy to interpret.\n",
    "  Cons: Requires large, ever-expanding sets of rules. Misses out-of-vocabulary patterns.\n",
    "  Fine-Tuned LLM:\n",
    "  Pros: Adapts to minor format changes or partial transliteration, more “intelligent” coverage if it’s in distribution.\n",
    "  Cons: If it encounters something unseen, it might guess or hallucinate. Some minor numeric errors, especially for larger numbers or decimals.\n",
    "  In Real Life, A system might use a hybrid approach: Let the LLM produce expansions, then pass it through the rule-based checker, and for one, just pass the input to a rule-based checker or corrector. Then, have some sort of text classification system, maybe BERT-based or LLM-based, to classify which is better.\n",
    "  6.2 Synthesis of Costs, Complexity, & Reliability\n",
    "  The rule-based approach is cheap to run but expensive to expand to new domains.\n",
    "  The fine-tuning approach costs a moderate amount (~$10 for training), but once done, it is easy to deploy in CPU format.\n",
    "  The agentic approach is easy to test but rarely meets production-level reliability.\n",
    "  A fourth method is using an instruct-trained model for deployment, but that won’t require any inputs from this task’s perspective and would require a simple prompt. Thus, that is not demonstrated here.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  7. Scope for Improvement\n",
    "  We see multiple opportunities for future expansions:\n",
    "  Larger & More Varied Dataset\n",
    "  Expand from 1,600 to 10k or 20k samples, ensuring more unusual currencies (SAR, RUB, BRL?), more advanced scientific units (kWh, mmHg, bar), and more irregular date formats. Possibly include code-switched data or incomplete transliteration.\n",
    "  Improve the dataset on aspects mentioned before:\n",
    "  Long sentences\n",
    "  Complex Units\n",
    "  Decimal based longer numbers\n",
    "  Larger variety of numbers\n",
    "  Including rarer date formats\n",
    "  Integrate real or partially real data (like Indian news articles or official docs) to reduce the synthetic bias, or use data from multiple models.\n",
    "  Multi-Task or Instruction Tuning\n",
    "  Tying entity normalization with other tasks like translation, summarization, or classification can yield a more robust model.\n",
    "  Or incorporate a “chain-of-thought” approach with step-by-step expansions for each numeric entity.\n",
    "  A general instruct tuning approach may also be tried, which can then be further finetuned for entity normalisation task. (However this would cost much more)\n",
    "  Fine-Grained Domain Coverage\n",
    "  Cover a larger variety of data e.g., medical domain might have specific units like “mg/dL,” “mmHg,” “IU,” “mL/h,” which are rarely used in finance or general domains. Each domain can get its own expansions.\n",
    "  Hybrid Pipeline\n",
    "  Let the model do its best, then verify or correct it with a rule-based approach. If the model’s expansions deviate from recognized patterns, the system can highlight or fix them, thus combining the best of both worlds.\n",
    "  Better Language Detection\n",
    "  Some user inputs might have code-mixed text, e.g., “He purchased $120 में 2kg दूध.” The system must handle partial English and partial Hindi. A robust code-mixing detection method might be required. (This is expected to move generalisation by a lot)\n",
    "\n",
    "\n",
    "  8. Conclusion & Refrences\n",
    "  8.1 Conclusion\n",
    "  Through this project, we deeply explored the domain of multi-lingual entity normalization for Indic languages—focusing on dates, currencies, and scientific units. We produced a synthetic dataset of 1,600 examples, balanced across 10 languages and multiple domains, each carefully split into train and eval sets. Then we implemented:\n",
    "  Agentic Method: Straight prompts, suboptimal.\n",
    "  Algorithmic Method: Rule-based, extremely accurate for known patterns but limited coverage.\n",
    "  Fine-Tuned LLM: A 4-bit LLaMA 3.1 model trained on our dataset. This approach delivered high accuracy on the evaluation set. It still had some corner-case issues (decimal expansions, rare date forms, etc.).\n",
    "  Finally, we converted the SFT model to a GGUF format for CPU hosting, deployed on a Hugging Face Space with a Streamlit front end and llama.cpp–based server. The entire pipeline is relatively cost-effective, with total training cost under $10 on an L4 GPU and about ($0.81 for one run), and free CPU inference on HF Spaces.\n",
    "  Next Steps: We plan to add more data, handle code-mixed text, increase the vocabulary for the algorithmic approach, and consider a “hybrid approach” that merges LLM outputs with rule-based validation for near-perfect coverage in real production.\n",
    "  8.2 References & Links\n",
    "  Primary Links:\n",
    "  Synthetic Dataset: Tasmay-Tib/sarvam-entity-recognition-gemini-2.0-flash-thinking-01-21-distill-1600\n",
    "  Fine-Tuned Model: Tasmay-Tib/sarvam-entity-normalisation-llama-3.1-8b\n",
    "  GGUF Model: Tasmay-Tib/sarvam-entity-normalisation-llama-3.1-8b-gguf\n",
    "  HF Spaces Deployment: Tasmay-Tib/sarvam-ai-entity-normalisation\n",
    "  Secondary Links:\n",
    "  Main Model training notebook with all three approaches: https://colab.research.google.com/drive/16_c--?usp=sharing\n",
    "  Wandb Plots for train runs (6 major ones out of 45 total): https://api.wandb.ai/links/tasmaytibrewal-iit-kharagpur/\n",
    "  Model Inferencing Colab notebook: https://colab.research.google.com/drive/-?usp=sharing\n",
    "  GGUF Model Inferencing Colab notebook: https://colab.research.google.com/drive/?usp=sharing\n",
    "  Synthetic Dataset Creation chat (Google AI Studio): https://aistudio.google.com/app/prompts?state=%7B%%22:%5B%%22%5D,%22action%22:%22open%22,%22userId%22:%22107745987607842002805%22,%22resourceKeys%22:%7B%7D%7D&usp=sharing\n",
    "  HF Spaces Deployment GitHub Repo: https://github.com/Tasmay-Tibrewal/GGUF-HF-deployment\n",
    "  GGUF conversion notebook: https://colab.research.google.com/drive/?usp=sharing\n",
    "  Final Model Reproducibility Notebook: https://colab.research.google.com/drive/?usp=sharing\n",
    "  Wandb Plots for Reproducibility run: https://api.wandb.ai/links/tasmaytibrewal-iit-kharagpur/\n",
    "  Prediction creation notebook: https://colab.research.google.com/drive/?usp=sharing\n",
    "  GGUF Model zip file: https://drive.google.com/file/d//view\n",
    "  Stop-words zip file: https://drive.google.com/file/d//view?usp=sharing\n",
    "  Model Predictions (eval, train, and total data preds, in normal and excel format, which are utf-8 and utf-8-sig encoded. Excel reads utf-8-sig easily, thus good for viewing, utf-8 is the standard encoding method used for programming purposes, thus it is given as well):\n",
    "  1. Eval:\t\t\t\t\t\t\t\t\t\t\t      - eval_data_001_predictions.csv (utf-8 encoded): https://drive.google.com/file/d/--le/view?usp=sharing          - eval_data_001_predictions_excel.csv (utf-8-sig encoded): https://drive.google.com/file/d//view?usp=sharing\n",
    "  2. Train:\t\t\t\t\t\t\t\t\t\t\t      - train_data_001_predictions.csv (utf-8 encoded): https://drive.google.com/file/d/1---/view?usp=sharing         - train_data_001_predictions_excel.csv (utf-8-sig encoded): https://drive.google.com/file/d/-/view?usp=sharing\n",
    "  3. Total:\t\t\t\t\t\t\t\t\t\t\t      - data_001_predictions.csv (utf-8 encoded): https://drive.google.com/file/d//view?usp=sharing          - data_001_predictions_excel.csv (utf-8-sig encoded): https://drive.google.com/file/d/-/view?usp=sharing\n",
    "  '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''DEEP FAKES\n",
    "    Deep fakes, coming from “Deep Learning” + “Fakes”, are images, videos, or audio manipulated using deep learning algorithms which map a face onto a given target, resulting in a convincing output in the same environment of the target but with the face given; or to extract facial expressions from a given input to generate a manipulated result.\n",
    "\n",
    "    It has various uses, such as in the entertainment industry, imposing actors’ faces onto other performers such as stuntmen or lip-syncing for multiple languages using dialogue replacement, for marketing, as used in ITC for Sunfeast or the one used by Cadbury in “NotACadburyAd”, both featuring Shah Rukh Khan, it can be used in ed-tech as well where historical figures can be mapped onto teachers making it a fun learning experience; furthermore, it can be used in the gaming industry for realistic content along with other industries.\n",
    "\n",
    "    The Sunfeast ad included a mapping of your face onto someone else’s face already in the ad. Whereas the Cadbury ad included Shah Rukh Khan branding your business, which basically included altering the portions of audio, modelled on his voice, and lip-syncing his video for the audio. Zomato’s “Mann Kiya, Zomato Kiya” ad which included Hrithik Roshan asking for food from a popular restaurant based on the city you were in, using dialogue replacement. Even we used deepfakes for publicity during freshers selections in 2023. \n",
    "\n",
    "    It is also being used in social media to generate funny memes and “obviously fake” impressions of celebrities or iconic moments. There are even fake news channels which have just AI generated content on YouTube.\n",
    "\n",
    "    Deep fake isn’t just limited to faces or people, researchers from MIT deepfaked a whole city.\n",
    "\n",
    "    Deepfaked Boston’s Back Bay neighborhood by an AI model trained on images of Aleppo, by MIT Researchers\n",
    "\n",
    "\n",
    "    It can be used in propaganda, spreading misinformation and fake news. This has already been used to play out global geopolitics, especially by countries such as China, in pro-China news channels. In fact Volodymyr Zelensky, in a video circulated, apparently surrendered. In an era where information travels at lightning speed through social media and online platforms, deep fakes can be used to alter public perception, manipulate elections, or even incite violence. \n",
    "\n",
    "    The ability to fabricate convincing videos of public figures saying or doing things they never did raises serious concerns about the erosion of trust and the credibility of digital content. MIT even released a deepfake detection experiment to see how many people get deceived.\n",
    "\n",
    "    Deep fakes, moreover pose a threat to privacy and personal security. By superimposing someone's face onto explicit or compromising content, individuals can be targeted and blackmailed. Another worrisome aspect of deep fakes is their potential impact on the justice system. With the ability to create fabricated evidence, including audio or video recordings, the credibility of digital information presented in courtrooms could be called into question. It is also being extensively used to pull off scams, like the infamous Elon musk crypto scam and even heists.\n",
    "\n",
    "    It is most widely used, by far in the pornographic industry, estimates suggest upto 96% of its usage in the space. There are many reports of actors being targeted in such videos and sometimes even normal people are victims of this, especially women.\n",
    "\n",
    "    “By blurring the line between fact and fiction, deep fake technology could undermine public trust in recorded images and videos as objective depictions of reality.” - said a letter to the director of National Intelligence by the US Congress. It not only means that images, audios and videos have a possibility of being fake, but also shatters our trust in them, leading to questioning of evidence that is actually real. Lawmakers all across the world are scratching their heads about how to deal with this technology. US government even involved the FBI and the Defense Advanced Research Projects Agency (DARPA) which is a military agency to tackle this issue, which used an AI trained on deep fakes to tackle this issue.\n",
    "\n",
    "    A technology created by AI researchers focuses on combining a recurrent neural network (RNN) and a convolutional neural network (CNN) to enable the programme to detect whether or not a video has been altered. An image-analyzing deep learning algorithm is called a CNN. An RNN is a kind of neural network that is frequently used in applications like speech recognition, natural language processing, and translation. It employs sequential or time series data. Guera and Delp (2018) showed that a CNN and an RNN feedback loop is a useful combination for reliably identifying deep fakes. This technique basically depends on teaching AI to detect subtle linguistic and visual inaccuracies and inconsistencies in deep fake content, just like a human would. Another promising approach to the technological detection of deepfakes is this method.\n",
    "\n",
    "    But with time, these are improving, too exponentially, high resolution hyper-realistic depictions of reality, convincing even to the best trained human eyes, how good can good be, better than reality? Can it warp the reality we are surrounded with and we trust. How good can the AI models to detect deep fakes get, can they spot what human eyes can’t and what stops newer deep fake models to be trained just to defeat these detection models. Current best detection models show a 94.4% accuracy, that too on 2020 data, considering computational improvements, newer techniques, efficient, better and faster algorithms will perform the same on best quality deep fakes from 2023. Even a 94.4% accuracy means a carefully constructed deepfake, trained on extremely large datasets using state of the art models for a very large time would beat it.\n",
    "\n",
    "    But how do deep fakes work (from absolute basic all the way up)?\n",
    "    Deep fake uses a combination of Generative Adversarial Networks (GANs) and Auto-encoders.\n",
    "\n",
    "    Generative Adversarial Networks (GANs) :\n",
    "    GANs are pretty much just two models (generally CNNs) in combination. One acts as a discriminator while the other is a generator.\n",
    "\n",
    "    Now what are these complicated words, GANs, CNNs, Encoders?\n",
    "\n",
    "\n",
    "    Intro to ML\n",
    "    Consider a model. A model is some function which outputs a value on some input. A more complex version of this is y=wx+b, where w is called weight and represents slope of a line and b is the bias or the y-intercept.\n",
    "\n",
    "    This simple function is used in linear regression models. Regression is when we have to predict some output using an input. \n",
    "\n",
    "    It may not be possible to do so every time, but you want to be close to predicting the output, as much as possible. Say house prices vs area of house. \n",
    "\n",
    "    Now, this may be correlated linearly or exponentially (i.e. y=w1x + w2x2 + …. +wnxn +b)\n",
    "    [image on right], \n",
    "\n",
    "    Now this is just with one feature or type of input, there may be multiple dependencies or features. Thus \n",
    "    y = w11x1 + w21x12 + …. +wn1x1n \n",
    "    w12x1 + w22x12 + …. +wn2x1n + b\n",
    "    [Ex: House price dependent on both area and number of bedrooms]\n",
    "\n",
    "    Now this is the most simple machine learning model, linear regression.\n",
    "\n",
    "    We start with random values of weights and biases and optimise the algorithm to maximise accuracy and minimise the error.\n",
    "\n",
    "    We can calculate error over all data points and sum it up as the cost (measure of how bad the model performs). Generally for regression we use Mean squared error.\n",
    "\n",
    "    h(x) = y [predicted] = wx + b;\n",
    "    Error = (h(x) - y)2\n",
    "    Cost = J(x) = 1/n * Σ(h(xi) - yi)2; where xi and yi represent the ith input and output respectively.\n",
    "    To maximise the performance of the model, we minimise this cost or error for which we use optimisation algorithms such as gradient descent.\n",
    "\n",
    "    To optimise we calculate the value of weights and biases for which cost has minimum value.\n",
    "\n",
    "    Gradient descent calculates the partial derivative of the cost function, with respect to the weights and biases [gradient vector of the weights], and updates the weights by subtracting them with a negative multiple of it.\n",
    "\n",
    "    w := w - α * ∂J(w)/∂w; the gradient or derivative represents the slope and -α * ∂J(w)/∂w is some negative multiple of it, if updated correctly it converges to a minima.\n",
    "\n",
    "\n",
    "\n",
    "    This is how ML simply works, set some parameters with random values and optimise it until cost becomes low.\n",
    "\n",
    "\n",
    "    Neural Networks\n",
    "    This is a bit complicated and would urge you to watch this series for a better understanding.\n",
    "\n",
    "    Neural networks are something which scientists modelled to replicate how the human brain works. It contains layers and nodes within them called neurons linked to each other.\n",
    "\n",
    "    A simple neural network is this\n",
    "\n",
    "    This contains many input neurons and 1 output neuron, basically a single-layer network.\n",
    "\n",
    "    The output neuron accepts inputs and gives an output.\n",
    "\n",
    "\n",
    "    h(x) = w1x1 + w2x2 + w3x3 + …. + b; thus the simplest neural network is just linear regression.\n",
    "    A more complicated neural network has hidden layers.\n",
    "\n",
    "    Generally we also add a non-linearity function to account for non-linear dependencies.\n",
    "\n",
    "    h(x) = σ(w1x1 + w2x2 + w3x3 + …. + b) or RELU(w1x1 + w2x2 + w3x3 + …. + b)\n",
    "\n",
    "    σ(x) = Sigmoid function = 1/(1+e-x); RELU(x) = max(0,x).\n",
    "\n",
    "    This one has two hidden layers. (considering 2 neurons in each layer and 1 output)\n",
    "\n",
    "\n",
    "    h1 = σ(w1x1 + w2x2 + b1)\n",
    "    h2 = σ(w3x1 + w4x2 + b2 )\n",
    "\n",
    "    h3 = σ(w5h1 + w6h2 + b3)\n",
    "    h4 = σ(w7h1 + w8h2 + b4)\n",
    "\n",
    "    y = σ(w8h3 + w9h4 + b5)\n",
    "\n",
    "    All this compounding of outputs, eventually results in a complex space represented by the model. Optimised by gradient descent using backpropagation this can theoretically represent all patterns and even memorise it, for a large enough training dataset and model architecture.\n",
    "\n",
    "    Linear regression has a paraboloid or equivalent cost function whereas neural networks have complex cost functions, which may even have a local minima. (We use various techniques such as multiple iterations of random initialisation of weights and moments to tackle this)\n",
    "\n",
    "    Thus our goal for large networks is not to completely optimise, but to reach to a good enough solution.\n",
    "\n",
    "    Back-propagation in neural networks (intuition and calculation)\n",
    "    Here we basically want to account for the weights to be updated by moving back layer by layer.\n",
    "    Intuition\n",
    "    Here we are classifying an image of a hand-written two, using a neural network. (Image has values of pixels to predict the output)\n",
    "\n",
    "    Ideally it should output 1 but it outputs 0.2. So we have to increase its activation (value of neuron). We also have to decrease value of other numbers. Thus we have a quantitative idea of increase/decrease required in neurons of output layer.\n",
    "\n",
    "    To do this we change value of not only weights and bias of this layer, but also the neurons of previous one.\n",
    "\n",
    "    That leads to change in the previous layer weights and biases, along with that in the last to previous layer, and so on.\n",
    "\n",
    "    The increase in weights are done such that we maximise our increase/decrease required. To do this we change them proportionally to the activation of the neuron they are connected to. This maximises the effect of increase.\n",
    "\n",
    "    For a 0.9 activation if we increase its weight from 0.1 to 0.3 we get a change of 0.09 to 0.27, net change of 0.18, whereas for a 0.1 activation the same results in a net change of 0.02. Thus we increase to get the most effect, thus proportional to the activation.\n",
    "\n",
    "    Similarly neuron activations would be increased proportional to value of weights.\n",
    "\n",
    "    This cycle continues until input layer.\n",
    "\n",
    "    Calculations\n",
    "    wi := wi - α * ∂J(W,B)/∂wi; for all wi in W [vector of weights].\n",
    "    bi := bi - α * ∂J(W,B)/∂bi; for all bi in B [vector of weights].\n",
    "\n",
    "    We give a naming convention to neurons:\n",
    "\n",
    "    The last layer activation is aL; then aL-1 and so on.\n",
    "\n",
    "    aL = σ(zL) = σ(wLaL-1 + bL)\n",
    "    Here z is the fn which goes through non-linear function to give activation, \n",
    "\n",
    "    zL = wLaL-1 + bL\n",
    "    L represents the last layer, wL and bL are the weights and bias for the last layer.\n",
    "\n",
    "    ∂J(W,B)/∂wiL = ∂J/∂aL * ∂aL/∂zL * ∂zL/∂wiL; [wiL being the ith weight for last layer]\n",
    "\n",
    "    ∂J(W,B)/∂wiL = ∂J/∂aL * ∂aL/∂zL * ∂zL/∂wiL = ∂J/∂anL * ∂anL/∂znL * ∂znL/∂wiL ; n is the neuron to which the weight is connected, i is the ith weight.\n",
    "\n",
    "    J(W,B) = 1/k*Σ(h(xi) - yi)2 = Σi = 1 to k Σj = 1 to m  (ajL - yj)i2, k being the number of training examples and m being the number of neurons on the last level.\n",
    "\n",
    "    aL = σ(zL);  zL = wLaL-1 + bL; \n",
    "\n",
    "    ∂J/∂anL = 2*(anL - yn)i \n",
    "    ∂anL/∂znL = σ’(zL)\n",
    "    ∂znL/∂wiL = apL-1 ; p is the neuron from which weight was required\n",
    "\n",
    "    Thus, ∂J(W,B)/∂wiL = 2*(anL - yn)i * σ’(zL) * apL-1 \n",
    "\n",
    "    Similarly for neurons in previous layers:\n",
    "    ∂J(W,B)/∂wiL-1 = Σj=1 to m ∂J/∂ajL * ∂ajL/∂zjL * ∂zjL/∂anL-1 * ∂anL-1/∂znL-1 * ∂znL-1/∂wiL-1 ; m being number of neurons on last level\n",
    "\n",
    "    ∂znL-1/∂wiL-1 = apL-2 ; p is the neuron from which weight was required\n",
    "    ∂anL-1/∂znL-1 = σ’(zL-1)\n",
    "    ∂zjL/∂anL-1 = wjnL-1; wjn is weight from jth neuron to nth neuron.\n",
    "    ∂ajL/∂zjL = σ’(zjL)\n",
    "    ∂J/∂ajL = 2*(anL - yn)i\n",
    "\n",
    "    ∂J(W,B)/∂wiL-1 = Σj=1 to m 2*(anL - yn)i * σ’(zjL) * wjnL-1 * σ’(zL-1) * apL-2 ;\n",
    "\n",
    "    Similarly,\n",
    "    ∂J(W,B)/∂biL-1 = Σj=1 to m 2*(anL - yn)i * σ’(zjL) * wjnL-1 * σ’(zL-1) * 1;\n",
    "\n",
    "    We can do this for all weights and biases. These partial derivatives for weights gives us the gradient of the vector W. \n",
    "    W := W - α * ∇ J(W,B)\n",
    "    B := B - α * ∇ J(W,B)\n",
    "\n",
    "    We update W and B vectors with a negative multiple of their gradients [gradient is the steepest slope upward for a function]. This done through multiple iterations leads to convergence.\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Convolutional Neural Networks\n",
    "    What is a convolution?\n",
    "    Convolution is a mathematical operation similar to what multiplication or division is.\n",
    "    It is represented by “∗” sign. Watch this video for a better understanding.\n",
    "\n",
    "    (a∗b)n = (a∗b) (n) = Σi,j i+j= n ai ⋅ bj = Σi = 0 to n ai ⋅ bn-i = a0 bn + a1 bn-1 +a2 bn-2  + . . .  + an-1 b1+ an b0\n",
    "    Here a and b are a list, vector or a matrix containing elements a0, a1, . . ., am and a0, a1, . . ., ap respectively where m,p >= n.\n",
    "\n",
    "\n",
    "    Thus for a complete list:\n",
    "    (a∗b) = [(a∗b)0, (a∗b)1, (a∗b)2, . . . , (a∗b)p+m]\n",
    "\n",
    "    a = [1, 2, 3, 4];  b = [5, 6, 7, 8, 9]\n",
    "\n",
    "    (a∗b) = [ (1*5), (1*6 +2*5) , (1*7 + 2*6 + 3*5) , (1*8 + 2*7 + 3*6 + 4*5) , (1*9 + 2*8 + 3*7 +4*6) , (2*9 + 3*8 + 4*7) , (3*9 + 4*8), (4*9)] \n",
    "    Thus,\n",
    "    (a∗b) = [5,  16, 34, 60, 70, 70, 59, 36]\n",
    "\n",
    "    Visual Process:\n",
    "    A                    :                 [1, 2, 3, 4]              [1, 2, 3, 4]            [1, 2, 3, 4]        [1, 2, 3, 4]\n",
    "    B (Reversed):  [9, 8, 7, 6, 5]              [9, 8, 7, 6, 5]            [9, 8, 7, 6, 5]        [9, 8, 7, 6, 5]\n",
    "    (a∗b) =                        [     1*5 ,                1*6 +2*5,      1*7 + 2*6 + 3*5, 1*8+ 2*7+ 3*6+ 4*5,\n",
    "\n",
    "    A                    : [1, 2, 3, 4]       [1, 2, 3, 4]           [1, 2, 3, 4]             [1, 2, 3, 4]\n",
    "    B (Reversed):  [9, 8, 7, 6, 5]       [9, 8, 7, 6, 5]          [9, 8, 7, 6, 5]             [9, 8, 7, 6, 5]\n",
    "    (a∗b) = , 1*9 + 2*8 + 3*7 +4*6 , 2*9 + 3*8 + 4*7,      3*9 + 4*8,                 4*9]\n",
    "\n",
    "    For functions:\n",
    "                    ∞\n",
    "    (f∗g) (t) = ∫   f(x) f(t-x) dx\n",
    "                            -∞  \n",
    "                        ∞\n",
    "    (f∗g) (2) = ∫    f(x) f(2-x) dx = some const. value; Thus for (f∗g) (t), each point is an\n",
    "                            -∞\n",
    "    Integral for all x from -∞ to +∞ for a particular value of t.\n",
    "    2D convolutions\n",
    "    A 2D Matrix, also called kernel is used to convolute over a 2d matrix in the same way it does over a 1D matrix.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ∗                    =\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Matrix A                                   Matrix B                       Matrix (A ∗ B) = C\n",
    "            \n",
    "\n",
    "    C11 = A11B11 + A12B12 + … + A21B21 + A22B22 + …. + AnnBnn\n",
    "\n",
    "    C12 = A11B12 + A12 B13 + …. + AnnBn(n+1)\n",
    "\n",
    "    And so on….\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Image Convolutions\n",
    "\n",
    "    This kernel is used for \n",
    "    a normal blur filter \n",
    "    on images.\n",
    "    Basically it averages out \n",
    "    The nearby pixel values.\n",
    "\n",
    "\n",
    "    This kernel is used for \n",
    "    a gaussian blur filter \n",
    "    on images.\n",
    "    This takes weighted average of neighbouring pixel values, according to gaussian function.\n",
    "\n",
    "    These kernels are used for Horizontal and vertical edge detection \n",
    "    on images.\n",
    "\n",
    "\n",
    "\n",
    "    Convolutions are popularly used in Image processing and kernels are of various types.\n",
    "\n",
    "    Problems with our original neural network model on detecting numbers from hand-written images:\n",
    "    Computationally expensive: For High-res images, with millions of pixels, there would be millions of weights to be optimised, it would be a very high dimensional data and would take up huge computation power.\n",
    "    Poor training: High dimensional datasets require very large datasets to train, and accuracy is poor compared to other more efficient models, such as CNN and requires a considerably higher number of data points to reach the same accuracy.\n",
    "    Low parameter efficiency: Requires a large number of parameters as compared to CNNs, which share them for pixels while convoluting. This leads to computational and storage inefficiency.\n",
    "    Hierarchical Feature Learning and Spatial Pattern Recognition in CNNs: CNNs are capable of learning hierarchical features from raw pixel values. Lower layers learn basic features like edges and textures, while higher layers learn more complex features and object representations. Standard feedforward neural networks do not consider the spatial relationships between pixels in an image. Images have a grid-like structure where the arrangement of pixels carries important information. Convolutional Neural Networks (CNNs) are specifically designed to handle such grid-like data and capture spatial patterns through convolutional layers.\n",
    "\n",
    "    Basically CNNs are able to recognise patterns, they pick up certain patterns and get the most activated for them.  They understand the underlying facial structure for faces, or predict images just how we thought a standard neural network would do while modelling it. While looking at the results of the neural network,  it was more arbitrary, this could be because it is a very high dimensional data, so it would require way more training examples to find patterns, but if such huge datasets exists to satisfy the tens or hundreds of millions of weights and biases on which it needs to be trained, then they potentially could produce pretty solid results. \n",
    "\n",
    "    On the contrary, CNN captures patterns like a human brain would, by recognizing edges in earlier layers, patterns then, and features and structure of the image in proceeding layers. They can convert the image into a latent space, which is like a low-dimensional representation of the high-dimension data that the model has captured. We can ideally associate a face image’s latent space with basic features such as \n",
    "    emotions, age, gender, etc.\n",
    "\n",
    "    These types of results are possible because of the architecture of CNN models, which consists of kernels with random initial weights and biases convoluting over images finding patterns such as edges, early on and moving to complex patterns as we go further.\n",
    "\n",
    "    CNN Architecture:\n",
    "\n",
    "    CNN consists of different types of layers:\n",
    "    1) Convolutional Layers: These consist of a set of kernel weights, randomly initialised and optimized later through gradient descent.\n",
    "\n",
    "    w1 , w2, . . ., w9 are a set of weights of each kernel that we use. We also have a bias term attached to output of each kernel. Thus for a 3x3 kernel we have 9 weights and 1 bias.\n",
    "    We use one kernel for each image in the previous layer for each image in the current layer.\n",
    "\n",
    "\n",
    "    Basically it means that while going from layer 1 to layer 2 for each image channel in layer 2 (say n2) we have weights for all image channels in layer 1 (say n1), thus in total we have n1*n2 kernels for layer 1 to 2.\n",
    "\n",
    "    Thus we have a total of n1*n2*10 weights and biases for the layer. As compared to n1*n2*28*28 weights if we had used pixels individually.\n",
    "\n",
    "    We further, similar to a simple feed-forward neural network also use an activation function to introduce non-linearity.\n",
    "    Otherwise it would just be kernels over kernels, which just produce a linear result.\n",
    "\n",
    "    Z22 = w1 X11 + w2 X12 + w3 X13 + w4 X21 + w5 X22 + w6 X23 + w7 X31 + w8 X32 + w9 X33+ b\n",
    "    (we are convoluting over X22, we keep the centre of our kernel on X22)\n",
    "\n",
    "    A = RELU(Z) = σ(Z) \n",
    "\n",
    "    A is the activation of the pixel part of a particular image in the next layer, for which we convoluted through the kernel.\n",
    "\n",
    "    But there is a problem, when we try to compute Z11, some parts of the kernel are out of the region with the pixels. \n",
    "\n",
    "    This happens with all the corner, edges of the image and we commonly use zero-padding to counter this.\n",
    "\n",
    "    What is zero padding?\n",
    "    We add some extra rows and columns wherever needed with values as zero such that the output is of the same size.\n",
    "\n",
    "    Suppose we have a 3x3 matrix for a 28x28 inout image then we wilkl get a 26x26 output image thus we use a 1 layer zero padding, effectively making the image a 29x29 image, \n",
    "\n",
    "    This is the example of a 5x5 matrix with a single layer zero padding.\n",
    "\n",
    "    The output remains a 5x5 matrix.\n",
    "\n",
    "    There are other types of padding as well but zero-padding remains the most widely used one.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    2) Pooling layers\n",
    "    Pooling layer is when we combine some squares into one using some properties.\n",
    "    There are many types of pooling, the most common is max-pooling which takes max of all cells as the output value, there is also min pooling, average pooling, etc.\n",
    "\n",
    "    Pooling is done to reduce the overall amount of information and to save on memory/parameters. Although information is lost, the crux of the image is retained.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    3) Flattening\n",
    "    Finally when the size of the enough is small enough, the image is flattened into a vector (with a single row) and final neural network layer(s) is applied to it.\n",
    "\n",
    "    4) ANN layers:\n",
    "    Normal feed-forward neural network layers act as additional layers to the network.\n",
    "\n",
    "\n",
    "    5) Hyperparameters:\n",
    "    There are several hyperparameters in a CNN. They include \n",
    "\n",
    "    Number of Layers\n",
    "    Width of the layers\n",
    "    Kernel size [nxn]: Depends on the size of image, mostly n is an odd number, can range from 3x3, 5x5, 7x7, 9x9, 11x11, 13x13]. Larger the size more the information is lost.\n",
    "    Padding: How much extra space is added around the image for convolution operations, and with what values, usually zero-padding is used to preserve the size (called Same padding), otherwise Valid padding (no padding), other types include replication padding, reflection padding, etc.\n",
    "    Pooling Type and Kernel size: Type of pooling used max pooling, average pooling and min pooling and kernel used for pooling, larger the kernel more is the information lost.\n",
    "    Stride: The step-size taken by convolution/pooling operations is called stride. By default stride for convolutions is 1 and for pooling is the size of the poolling kernel.\n",
    "\n",
    "    6) Backpropagation: Similar to that in ANN (Artificial Neural Networks), we backpropagate following chain rule across layers, however navigating for convolutional architecture may not be straightforward and a challenge on its own, to keep it concise it is not covered in the scope of this blog.  \n",
    "\n",
    "    I have attached the explanation of backpropagation here.\n",
    "\n",
    "    7) Fast Convolution Implementation:\n",
    "\n",
    "    The convolution process can be extremely slow for large networks, across thousands of input images, across millions of parameters, for thousands of iterations and hundreds of steps.\n",
    "\n",
    "    So we implement it in a matrix form. Where we have the convolutional vector and a flattened input vector, and we perform the operation as their matrix multiplication.\n",
    "\n",
    "    Say we have a 4*4 input image and a 3*3 kernel, then we can construct the vectors as follows:\n",
    "\n",
    "    Kernel (K11, K12, …, K21, .., K33)     Image(I11, I12, …, I21, .., I44)        Output (O11, O12, O21, O22)\n",
    "\n",
    "\n",
    "                                        \n",
    "                                                                                                \n",
    "    Ideally we could have done O11 = K11I11 + K12I12 + .. + K13I13 and iteratively repeated for O11 - O1n then O21 - O2n and till Onn.\n",
    "\n",
    "    But it is computationally expensive, so we express each element in O as function of two matrices since matrix operations are highly optimized:\n",
    "\n",
    "\n",
    "\n",
    "    O11 =  Dot product of K11 with I, O12 = Dot (K12, I), O13 = Dot (K13, I);  O14 = Dot (K14, I)\n",
    "\n",
    "\n",
    "    Matrix K11                        Matrix K12\n",
    "\n",
    "    This is faster, but dot product can be represented further as dot of flattened K11 and I vectors. \n",
    "\n",
    "    Here, \n",
    "    K11 = [1, -5, 4, 0, 0, 3, 1, 0, -3, -2, 0, 0, 0, 0, 0, 0]T = A \n",
    "    If = [10, 25, 20, 2, 25, 15, 18, 5, 5, 17, 20, 10, 1, 12, 25, 7]T = B\n",
    "\n",
    "    But dot of 2 vectors (A.B) can further be represented as \n",
    "\n",
    "    (AT x B) = (K11 )T * If = O11\n",
    "\n",
    "    Similarly, O12 = (K12)T * I and so on…\n",
    "\n",
    "    Thus a new vector can be constructed: \n",
    "    C = [(K11)T, (K12)T, …, (Knn)T]T\n",
    "    ; where K11 - Knn are not the elements of Kernel K, but Kernel K zero-padded to fill the shape of Image I at its different convolutional positions and then flattened out as a row. Here C itself is a column vector but its elements are row vectors thus making a 2D matrix.\n",
    "\n",
    "    \n",
    "    Matrix C\n",
    "\n",
    "\n",
    "    Here, If is the flattened-out image column vector, thus we can represent the convolution operation: K * I = O, C x If = Of, where O is the output matrix (not the null matrix) and Of is the flattened Output matrix.\n",
    "\n",
    "    K is a 3x3 matrix, I is a 4x4 matrix thus O is a 2x2 output; If is a 16x1 flattened vector, and C is a 4 x 16 (16 for 16 elements of I, 4 for 4 positions (2x2) in Ouput) matrix.\n",
    "\n",
    "    Thus C x If produces a  [(4x16) x (16x1) = 4 x 1] which is of the same size as the flattened matrix, this can be reshaped to get the output.\n",
    "\n",
    "    Why is this process more efficient?\n",
    "    For an mxm Kernel and an nxn Image, instead of convoluting (n-(m-1)) * (n-(m-1)) times, we can directly compute C vector by first padding K with zeros, to get C1 and then adding a zero at start and removing from end, equivalent of iterating K along a row of I; then after n-(m-1) elements, we add n 0 values to the start and remove n 0 zero values from the end, this process is repeated to get (n-(m-1))^2 elements of C; which is then multiplied with I using highly optimized matrix multiplication algorithms.\n",
    "\n",
    "    8) Reconstructing Images through Transposed Convolutions:\n",
    "\n",
    "    Now we can express a convolution operator as matrix multiplication of the convolution vector with the image. Of = C x If (where ‘*’ represents the convolution operation); in general case O is of order (n-(m-1))2 * 1, C is of order (n-(m-1))2 * (n2) and If is of order n2 * 1.\n",
    "\n",
    "    Multiplying the eqn with matrix D (of order n2 *  (n-(m-1))2) :\n",
    "\n",
    "    DxOf = DxCxIf \n",
    "\n",
    "    DxC results in an n2 * n2 vector, and further multiplying with If results in an n2 * 1 vector, DxOf results in an n2 * 1 vector as well.\n",
    "    Now we come to the concept of generalized inverses, for every matrix A of order p*q there exists its generalized inverse of order q*p; which may not be unique to A, and not reversible in nature but it does exist.\n",
    "\n",
    "    So ideally we can consider D as the generalized inverse of C, where DxC = In^2 (Unit vector of Order n2), and In^2 x If = If, thus D is represented as Cg, the generalized inverse of C, the equation changes to:\n",
    "\n",
    "    Cg x Of = If, Cg is of order n2 *  (n-(m-1))2 and Of is of order (n-(m-1))2 * 1\n",
    "\n",
    "    Backpropagation can easily be done, by rephrasing the equation, transposing D and taking a dot with Of thus representing IfT as the dot of (Cg)T with Of and then the gradient can be calculated easily (the corresponding elements of Of are the gradients).\n",
    "\n",
    "    So we try to learn the vector Cg, but even if we learn something close enough, we can easily reconstruct the image vector. This has an additional benefit, through multiple iterations of convolutions and transposed convolutions (also called deconvolutions), we try to converge to the value of Kernel, which itself is changing,  and thus because of the delay in converging, reach a matrix similar to Cg in terms of values, but not exactly Cg thus instead of exactly reconstructing an image, we reconstruct an image quite similar to the image, introducing inherent variation.\n",
    "\n",
    "    Encoder-Decoder Networks and U-Nets\n",
    "\n",
    "    Encoder-decoder networks are a fundamental architecture in the field of deep learning, widely used for tasks that involve mapping inputs to outputs, where the dimensionality of the input and output can vary. This architecture is particularly prevalent in applications like machine translation, image captioning, and sequence-to-sequence prediction tasks.\n",
    "\n",
    "    Encoder\n",
    "    The encoder part of the network takes the input data and compresses the information into a context vector (also known as a feature vector or state vector). This vector aims to encapsulate the essence of the input information in a fixed-size representation, regardless of the input size. The encoder processes the input through one or more layers (which can be fully connected layers, convolutional layers, or recurrent layers, depending on the nature of the input data) to produce this context vector.\n",
    "\n",
    "    Decoder\n",
    "    The decoder part of the network is responsible for taking the context vector generated by the encoder and translating it into the desired output format. The decoder essentially learns to generate the output data from the compressed information while potentially considering additional inputs during the generation process. Similar to the encoder, the decoder can consist of various types of layers tailored to the specific requirements of the output data.\n",
    "\n",
    "    This can simply be a Neural Network, used to compress the data and learn its implicit representation in a highly efficient manner, and used to regenerate it when required.\n",
    "\n",
    "    For computer vision tasks, this network is generally represented by a complex convolutional network, using convolutions and pooling/strided or even pus convolutions to compress an image and transposed convolutions to decompress it.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    This has various potential uses, it can act as a generation network, an image segmentation model, a de-noising and an upscaling network as well.\n",
    "\n",
    "    But why to compress?\n",
    "    Why do we compress the layer in narrower layers if we can just make broader and larger networks which will probably produce better outcomes.\n",
    "\n",
    "    A network of the same number of neurons can just learn to multiply itself by one and give the input image as the output, thus the network is inherently:\n",
    "    Not efficient at parameterization\n",
    "    Not learning properly\n",
    "\n",
    "    One can then say to reduce the number of pixels by one and then construct the network, but yet again that means that all parameters are more or less copying the input and one pixel is actually constructed, in other terms the network is not understanding the basis of an image, not constructing the latent space as it ideally shall construct.\n",
    "\n",
    "    Thus there is a tradeoff, between capacity of the network to actually learn the latent representation along with its efficiency and the overall model capacity. Not only this, but there is also a tradeoff between information preserved, and that lost, because if the network is reduced by a lot, large amount of information loss is possible.\n",
    "\n",
    "    One may argue that other than generation tasks say for segmentation models, where we are not producing the input image but rather separating different types of objects within the image. There, one may say that we won’t face the tradeoffs between representation and capacity, since model can not just copy the inputs. Although this is true to an extent it can very well be argued that the model is still not efficient at parametrization and thus not learning effectively. In simple terms a large model may produce the correct result, but it is not that process to produce the result that is correct or most ideal, ideal being subjective to what we desire, which may be a good latent space representation and an implicit understanding of the image.\n",
    "\n",
    "    A different technique to use larger networks for generation tasks could be by using what we call as de-noising auto encoders. These add noise to the input and try outputin the denoised image, thus turning it into a denoising task. As mentioned above for segmentation tasks, since the model can not directly copy the image, these models will learn the representation relatively better for larger networks.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    U-Nets\n",
    "    Inspired from its shape, U-Nets are a type of convolutional neural network (CNN) architecture that resembles the letter \"U\". They were originally designed for biomedical image segmentation tasks but have since found applications in various areas requiring precise and detailed pixel-level predictions, such as satellite image analysis, autonomous vehicle perception, and even art style transfer. The U-Net architecture is particularly notable for its effectiveness in working with a small amount of data, a common scenario in medical imaging.\n",
    "\n",
    "\n",
    "\n",
    "    This model includes convolutional layers, and pooling layers for encoding and transposed/deconvolutional and skip connection layers for decoding information.\n",
    "\n",
    "    The main challenge faced by a standard encoder-decoder network is to overcome the loss of information. A distinctive feature of U-Nets is the use of skip connections that directly concatenate feature maps from the encoder to the corresponding layers in the decoder. These connections provide the decoder with detailed local information from the input image, which, when combined with the global context acquired during downsampling, allows for more precise segmentation. \n",
    "\n",
    "    Skip connections can be implemented by cropping the images to the desired size and then concatenating the channels with the upsampled channels.\n",
    "\n",
    "    A potential downside for skip connections is that it can directly learn to use the values from skip connections and not use the encoded information, thus making the whole network useless, as discussed in the previous section for large networks.\n",
    "\n",
    "    There are various mechanisms to counter this such as noising and denoising, data augmentation, and regularization using dropout layers, which drop out some neurons essentially forcing the model to learn representation throughout and also initially freezing and gradually unfreezing the skip connection layers after training the model.\n",
    "\n",
    "    The loss functions used for this model could be binary/categorical cross entropy for image segmentation tasks, but for image generation tasks we can use mean squared error loss, since it is similar to regression of a value, although other losses may very well be used depending on the application.\n",
    "\n",
    "    GANs (Generative Adversarial Networks)\n",
    "    Generative Adversarial Networks (GANs) are a groundbreaking and influential class of neural networks designed for generative modeling, a type of unsupervised learning. Introduced by Ian Goodfellow and his colleagues in 2014, GANs have revolutionized the field of artificial intelligence, particularly in tasks involving generating highly realistic images, videos, music, and even text.\n",
    "\n",
    "    Basic Concept\n",
    "    The core idea behind GANs is relatively straightforward but profound. A GAN consists of two neural networks, the Generator and the Discriminator, which are trained simultaneously through a competitive process:\n",
    "\n",
    "    Generator (G): This network learns to generate data (e.g., images) that resemble the real data. Its goal is to produce outputs indistinguishable from genuine data to the extent that the Discriminator cannot reliably tell the difference.\n",
    "\n",
    "    Discriminator (D): In contrast, the Discriminator learns to distinguish between the real data (from the training dataset) and the fake data produced by the Generator. Essentially, it acts as a critic that gets better and better at identifying what's real and what's not.\n",
    "\n",
    "    Training Process\n",
    "    Training a GAN involves a delicate balance where the Generator and the Discriminator improve in tandem through an adversarial process, often described as a \"minimax\" game. Here's a simplified overview of the steps involved:\n",
    "\n",
    "    Training the Discriminator: Initially, the Discriminator is trained with a batch of data containing both real and fake images (generated by the Generator). The goal is to maximize its ability to correctly label the images as real or fake.\n",
    "\n",
    "    Training the Generator: Next, the Generator is trained to fool the Discriminator. The Generator's output is fed to the Discriminator, and the Generator is updated based on how well the Discriminator was able to distinguish the fake data from the real data. The objective is to minimize the Discriminator's accuracy, thereby improving the Generator's ability to produce realistic data.\n",
    "\n",
    "    Iterative Improvement: This process is repeated in numerous iterations, with both networks improving over time. The Generator learns to produce increasingly realistic data, while the Discriminator becomes better at distinguishing real from fake.\n",
    "\n",
    "\n",
    "    As studied earlier, the discriminator could be a standard CNN model that is trained to distinguish between real images and fake images as generated by the generator.\n",
    "\n",
    "\n",
    "    Two types of losses are calculated, the generator loss and the discriminator loss, first a discriminator loss is calculated, and the discriminator is trained, and then the generator is trained.\n",
    "\n",
    "    As stated, we are in a minimax game and trying to minimize the losses.\n",
    "\n",
    "    We can write the loss as a function:\n",
    "\n",
    "    V(D,G) = Ex~Pdata(x) [log D(x)] + Ez~Pnoise(z) [log (1 - D(G(z))]\n",
    "\n",
    "    Here V is the loss for the model, E stands for expectation, first part is the discriminator loss independent of the generator x~Pdata(x) shows that the input is sampled from the input space distribution and its log loss is calculated whereas the second part which represents the discriminator loss dependant of the generator z~Pnoise(z) is the noise sampled from the distribution of noise (generally gaussian distribution), and its log loss is calculated, considering the fact that (y = 0) for the generator for this case.\n",
    "\n",
    "    For the discriminator, we want to maximize this since we want it to be the most accurate. (i.e value to be near 0). Whereas for the generator we want it to generate realistic output and thus we want to maximize the error of the discriminator. (i.e. value to be highly negative).\n",
    "\n",
    "    Now we have two options, one is to maximize the discriminator loss D and then minimize the generator loss G and the other to minimize G and then maximize D:\n",
    "\n",
    "    minGmaxD V(D,G)= Ex~Pdata(x)[log D(x)] +Ez~Pnoise(z)[log (1 - D(G(z))]\n",
    "    maxDminG V(D,G)= Ex~Pdata(x)[log D(x)] +Ez~Pnoise(z)[log (1 - D(G(z))]\n",
    "\n",
    "    Minimising G could directly lead to inverse training of discriminator, and not necessarily good generation capabilities, which then paired with maximizing could just mean neutralizing the effects and not guarantee convergence.\n",
    "\n",
    "    Whereas maximizing D would first train the generator to understand the difference between real and fake images and then training the generator would lead to enhanced generation capabilities. This process when re-iterated multiple times would ideally lead to infinite capabilities for the generator where the discriminator cannot differentiate between real and fake.\n",
    "\n",
    "    This iterative process can be implemented by first training the discriminator model on one epoch then the generator and so on. (one epoch through mini-batch gradient descent)\n",
    "\n",
    "    Note: Taking multiple steps towards convergence for the discriminator model, as in mini-batch gradient descent or running a few epochs at a time is a better option, considering the fact that the discriminator would actually have a sense of what is real and what is fake after a few steps as compared to one and generator can actually improve as compared to worsening the discriminator* and so that we do not again run into the non-convergence issue mentioned above.\n",
    "\n",
    "    *This concept is understandable because an improved discriminator indicates that the entire model is nearing convergence, suggesting that further enhancements to the generator are likely to bring the model closer to convergence. Conversely, with an average discriminator, which suggests that the model is not yet in a convergence zone, training the generator could actually deteriorate the discriminator's performance. This is because there are numerous ways for the discriminator to worsen, making it a more probable outcome. \n",
    "\n",
    "\n",
    "\n",
    "    Generative Process:\n",
    "    Random noise is sampled from a pre-set distribution and a generator network, could be convolutional layers, dense layers, auto-encoders, or even U-Nets are used. \n",
    "\n",
    "    Generally, the random noise given as input is smaller then the output size and is present just to induce variation in the generations, and could come from the motivation that while drawing random thoughts, events influence it even for humans.\n",
    "\n",
    "    A model trained with this architecture will only produce one type of output, but if we want it to be trained on multiple types, then we can use an image/text embedding as input for the generator as well.\n",
    "\n",
    "    The reason why GANs are inherently better than U-Nets, CNNs, and other generative models is their ability to not train on direct losses but on understanding the implicit representations of both the generative and discriminative part and use them as adversarial of one another to generate images that realistic instead of images that match the output. As stated before, provided enough computing power it could ideally lead to infinite generation capabilities. \n",
    "\n",
    "    For normal generative models, the process of learning on custom-defined loss functions and matching on input images from a given dataset often leads to low sharpness of the image and very averaged-out features, of the ones present in the training set. This problem is not present in GANs, since they allow multiple answers, as compared to other networks.\n",
    "\n",
    "    An example of the low sharpness output for MSE models for the next video frame prediction, because of averaging out predictions from training cases. GANs can predict more accurately because their training is not based on given data loss, but on what could be a suitable prediction for the discriminator.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Generating Deepfakes:\n",
    "\n",
    "    But How are deepfakes generated?\n",
    "\n",
    "    Two different encoder networks are trained, one to generate faces of A and another for B. Then the encoded versions of image A and B are switched between the networks, this results in encoder 1 generating image of B with face of A and vice versa for encoder 2.\n",
    "\n",
    "    This encoder-decoder network could be a U-Net and the output can further be paired with a discriminator network, thus training the whole network as a GAN, further improving performance.\n",
    "\n",
    "    This type of deepfake architectures have a problem, which is that networks with same architectures might have different way/method of final representation, i.e. different way of encoding, hence it is better to use a unified encoder network and separate decoder networks. \n",
    "\n",
    "    Further predictive masks can be used to highlight regions which need more attention to detail and then the process of reconstruction and can be used to guide the disciminator network on where to zero in while detecting flaws. For the reconstruction part the masked part can be fed in to a network trained to just reconstruct the facial features and expression, instead of the whole image.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown token marker is not present in the 'data' (token map). Adding it there. Setting its token id to 131072, token count to 0 and increasing 'max_token_id' to 131072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenising words: 100%|██████████| 16796/16796 [00:00<00:00, 592543.78it/s]\n"
     ]
    }
   ],
   "source": [
    "tokeniser = Tokeniser()\n",
    "tokens, n_toks = tokeniser.tokenise(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DE', 'EP', ' ', 'FAK', 'ES', '\\n', ' ', ' ']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18764, 18764)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_toks, len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8481"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.count(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tiktoken._educational import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_breaks(text):\n",
    "    # Load the encoding for GPT-4\n",
    "    enc = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "    \n",
    "    # Encode the text to get token IDs\n",
    "    token_ids = enc.encode(text)\n",
    "    \n",
    "    # Decode each token to get its subword string\n",
    "    subwords = [enc.decode([token]) for token in token_ids]\n",
    "    \n",
    "    return subwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4o_tokens = get_word_breaks(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['DE', 'EP', ' F', 'AK', 'ES'], 10403)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt4o_tokens[:5], len(gpt4o_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, False, False, True, True)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = regex.compile(r'[^\\p{L}\\p{N}]+')\n",
    "bool(pattern.fullmatch('9')), bool(pattern.fullmatch('a')), bool(pattern.fullmatch('B')), bool(pattern.fullmatch(' ')), \\\n",
    "bool(pattern.fullmatch('/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7964"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_tokens = 0\n",
    "for i in gpt4o_tokens:\n",
    "    for j in i:\n",
    "        if not bool(pattern.fullmatch(j)):\n",
    "            alpha_tokens += 1\n",
    "            break\n",
    "alpha_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7827"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for i in tokens:\n",
    "    if len(regex.split(r'[^\\p{L}\\p{N}_]+', i)) > 1:\n",
    "        continue\n",
    "    count += 1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = tokeniser.token_ids(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[46746, 29659, 29659, 67133, 131072, 118813, 29659, 111880, 92770, 115964]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131072"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokeniser.max_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_np = tokeniser.one_hot_tokens(token_ids, op='np')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       ...,\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26319, 131073)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(one_hot_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "del one_hot_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_torch = tokeniser.one_hot_tokens(token_ids, op='torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([26319, 131073])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        ...,\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(one_hot_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\n",
      "',' ',' ','Ent','<|unknown_token|>','ity',' ','Normal','isa','tionIn',' ','Ind','ic',' ','Languages','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "',' ',' ','Tas','may',' ','Pan','kaj',' ','Ti','bre','wal','\n",
      "',' ',' ','1','.',' ','Introduction','\n",
      "',' ',' ','Entity',' ','normalization',' ','is',' ','central',' ','to',' ','many',' ','NLP',' ','tasks','.',' ','In',' ','Ind','ic',' ','languages',',',' ','the',' ','challenge',' ','amp','lifies',' ','because',' ','we',' ','must',' ','handle',' ','multiple',' ','scripts',' ','(','Dev','ana','gari',',',' ','Tamil',',',' ','Telugu',',',' ','etc','.),',' ','plus',' ','localized',' ','words',' ','for',' ','months',',',' ','currency',',',' ','numer','ic',' ','expansions',',',' ','etc','.',' ','Our',' ','end',' ','goal',' ','is',' ','to',' ','take',' ','sentences',' ','containing',' ','dates',',',' ','currencies',',',' ','and',' ','scientific',' ','units',' ','and',' ','produce',' ','fully',' ','spelled','-','out',' ','text',' ','in',' ','the',' ','same',' ','language',' ','script','.',' ','This',' ','single',' ','problem',' ','touches',' ','on',' ','mul','tilingual',' ','NER',',',' ','text',' ','normalization',',',' ','script',' ','detection',',',' ','numer','ic',' ','expansions',',',' ','and',' ','more','.','\n",
      "',' ',' ','In',' ','this',' ','project',',',' ','we',' ','explored',' ','three',' ','broad',' ','strategies',':','\n",
      "',' ',' ','Agen','tic',' ','(','Prompt','-','based',')','\n",
      "',' ',' ','Algorithm','ic',' ','(','Manual',' ','rule','-','based',')','\n",
      "',' ',' ','Fine','-','Tun','ed',' ','LL','M',' ','(','Super','vised',' ','approach',' ','on',' ','a',' ','synthetic',' ','dataset',')','\n",
      "',' ',' ','We',' ','also',' ','produced',' ','a',' ','dataset',' ','of',' ','roughly',' ','1',',','600',' ','synthetic',' ','examples',' ','spanning',' ','10',' ','major',' ','Indian',' ','languages',' ','and',' ','multiple',' ','domains','.',' ','Below',' ','is',' ','an',' ','in','-','depth',' ','account',' ','of',' ','every',' ','step','.','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "',' ',' ','2','.',' ','Data',' ','Cur','ation',' ','in',' ','Extreme',' ','Detail','\n",
      "',' ',' ','Because',' ','good',' ','data',' ','under','pins',' ','any',' ','approach',' ','to',' ','entity',' ','normalization',',',' ','we',' ','begin',' ','by',' ','disse','cting',' ','the',' ','data',' ','generation',' ','and',' ','splitting',' ','procedures',' ','meticulously','.','\n",
      "',' ',' ','2','.','1',' ','Moti','vations',' ','and',' ','Requirements','\n",
      "',' ',' ','Why',' ','Synthetic',' ','Data','?','\n",
      "',' ',' ','Ra','rity',' ','of',' ','Real',' ','Data','sets',':',' ','We',' ','needed',' ','examples',' ','that',' ','specifically',' ','showcased',' ','transformations',' ','from',' ','numer','ic',' ','or',' ','symbolic',' ','forms',' ','to',' ','spelled','-','out',' ','forms',' ','in',' ','each',' ','script','.',' ','\n",
      "',' ',' ','Cont','rolled',' ','Diversity',':',' ','By',' ','instru','cting',' ','a',' ','generative',' ','model',' ','to',' ','produce',' ','sentences',' ','with',' ','multiple',' ','entity',' ','forms',' ','(','dates',',',' ','currencies',',',' ','units',')',' ','in',' ','each',' ','language',',',' ','we',' ','can',' ','systematic','ally',' ','cover',' ','a',' ','wide',' ','range',' ','of',' ','scenarios','.','\n",
      "',' ',' ','Domain',' ','Variation',':',' ','We',' ','wanted',' ','to',' ','ensure',' ','coverage',' ','in',' ','<|unknown_token|>','medical',',','<|unknown_token|>',' ','<|unknown_token|>','news',',','<|unknown_token|>',' ','<|unknown_token|>','financial',',','<|unknown_token|>',' ','<|unknown_token|>','scientific',',','<|unknown_token|>',' ','<|unknown_token|>','legal',',','<|unknown_token|>',' ','etc','.',' ','contexts',',',' ','something',' ','that',' ','real',' ','data',' ','might',' ','not',' ','guarantee',' ','without',' ','extensive',' ','cur','ation','.','\n",
      "',' ',' ','Language',' ','Coverage','\n",
      "',' ',' ','We',' ','decided',' ','to',' ','cover',' ','10',' ','languages',':',' ','Hindi',',',' ','Tamil',',',' ','Telugu',',',' ','Kann','ada',',',' ','Mala','yalam',',',' ','Od','ia',',',' ','Bengali',',',' ','Gujarat','i',',',' ','Punjabi',',',' ','and',' ','Mara','thi','.',' ','Each',' ','has',' ','unique',' ','scripts',' ','and',' ','morphological',' ','nuances','.',' ','For',' ','example',':','\n",
      "',' ',' ','Hindi',' ','&',' ','Mara','thi',' ','share',' ','Dev','ana','gari',' ','but',' ','di','ffer',' ','in',' ','certain',' ','vocabulary','.','\n",
      "',' ',' ','Gujarat','i',' ','and',' ','Bengali',' ','have',' ','distinct',' ','scripts',' ','but',' ','might',' ','share',' ','some',' ','conceptual',' ','expansions',' ','for',' ','numbers','.','\n",
      "',' ',' ','Domain',' ','Coverage','\n",
      "',' ',' ','Data',' ','is',' ','generated',' ','from',' ','different',' ','domains',',',' ','ensuring',' ','that',' ','the',' ','output',' ','is',' ','varied',' ','and',' ','diverse','.',' ','These',' ','are',':','\n",
      "',' ',' ','Scientific','\n",
      "',' ',' ','Medical','\n",
      "',' ',' ','Financial','\n",
      "',' ',' ','Literature','\n",
      "',' ',' ','General','\n",
      "',' ',' ','Technical','\n",
      "',' ',' ','Academic','\n",
      "',' ',' ','News','\n",
      "',' ',' ','Legal','\n",
      "',' ',' ','Geography','\n",
      "','\n",
      "','\n",
      "',' ',' ','Entity',' ','Diversity','\n",
      "',' ',' ','Dates',':',' ','We',' ','needed',' ','multiple',' ','patterns','—','DD','/','MM','/','YY','YY',',','MM','/','DD','/','YY','YY',',',' ','DD','-','MM','-','YY',',',' ','2nd',' ','Jan',',',' ','<|unknown_token|>','March',' ','15',',',' ','1990','<|unknown_token|>',',',' ','etc','.',' ','This',' ','ensures',' ','the',' ','final',' ','model',' ','or',' ','algorithm',' ','can',' ','adapt',' ','to',' ','different',' ','real','-','world',' ','date',' ','nota','tions','.','\n",
      "',' ',' ','Cur','ren','cies',':',' ','$','120',',',' ','₹','500',',',' ','INR',' ','700',',',' ','Rs','.',' ','250',',',' ','€','300',',',' ','¥','1500',',',' ','etc','.',' ','The',' ','top',' ','10',' ','global',' ','currencies',',',' ','each',' ','possibly',' ','spelled',' ','out',' ','differently',' ','in',' ','each',' ','language',' ','script','.','\n",
      "',' ',' ','Scientific',' ','Units',':',' ','From',' ','simple',' ','(','10k','g',')',' ','to',' ','compound',' ','(','10',' ','km','/','h','),',' ','plus',' ','temperature',' ','(','20','°','C','),',' ','volume',' ','(','2',' ','litre','),',' ','weight',' ','(','lbs',',',' ','to','nne','),',' ','and',' ','more','.','\n",
      "',' ',' ','2','.','2',' ','Prompt',' ','Engineering',' ','for',' ','Data',' ','Generation','\n",
      "',' ',' ','The',' ','data',' ','was',' ','synthe','tically',' ','created',' ','using',' ','Google',' ','AI',' ','Studio',' ','(','Gemini','),',' ','with',' ','carefully',' ','structured',' ','prompts','.',' ','A',' ','typical',' ','master',' ','prompt',' ','looked',' ','something',' ','like',':','\n",
      "',' ',' ','The',' ','prompts',' ','can',' ','be',' ','found',' ','in',' ','the',' ','data',' ','generation',' ','prompts',' ','t','ab','.',' ','An',' ','iterative',' ','approach',' ','was',' ','used',' ','to',' ','create',' ','the',' ','data',' ','in',' ','the',' ','chat','.',' ','Model',' ','used',' ','for',' ','synthetic',' ','data',' ','generation',' ','is',' ','`','ge','mini','-','2','.','0','-','flash','-','thinking','-','0','1','-','21','`',' ','as',' ','it',' ','had',' ','long',' ','context',' ','support',',',' ','is',' ','free',' ','of',' ','cost',' ','and',' ','has',' ','a',' ','hu','uge',' ','output',' ','window',' ','(~','6','5K',' ','tokens',').','\n",
      "',' ',' ','This',' ','iterative',' ','approach',' ','let',' ','us',' ','produce',' ','~','1',',','600',' ','examples',',',' ','ensuring',' ','distribution',' ','across',':','\n",
      "',' ',' ','Dom','ains',':',' ','10',' ','domains',' ','in',' ','total','.','\n",
      "',' ',' ','Languages',':',' ','10',' ','languages',' ','in',' ','total','.','\n",
      "',' ',' ','Temperature',':',' ','0','.','1',',',' ','0','.','4',',',' ','0','.','7',',',' ','1','.','0',' ','(','Some',' ','examples',' ','are',' ','short',' ','and',' ','direct',',',' ','while',' ','others',' ','are',' ','long',' ','or',' ','more',' ','creative',').','\n",
      "',' ',' ','2','.','3',' ','Data',' ','Inspection',' ','&',' ','Quality',' ','Checks','\n",
      "',' ',' ','Format',' ','Validation',':',' ','Each',' ','synthetic',' ','record',' ','was',' ','verified',' ','to',' ','ensure',' ','it',' ','had',' ','the',' ','keys',' ','[\"','sl','.',' ','no','.\",',' ','\"','language','\",',' ','\"','input','\",',' ','\"','output','\",',' ','\"','domain','\"].',' ','We',' ','also',' ','introduced',' ','a',' ','gen_','temperature',' ','field',' ','to',' ','track',' ','which',' ','temperature',' ','setting',' ','originated',' ','in',' ','the',' ','sample','.','\n",
      "','\n",
      "','\n",
      "',' ',' ','Edge',' ','Cases',':','\n",
      "',' ',' ','Some',' ','lines',' ','had',' ','no',' ','entity','.',' ','(','E','.','g','.,',' ','<|unknown_token|>','य','ह',' ','ए','क',' ','स','ा','म','ा','न','्','य',' ','व','ा','क','्','य',' ','ह','ै','।','<|unknown_token|>',' ','in',' ','Hindi',',',' ','with',' ','no',' ','numer','ic',' ','content','.)','\n",
      "',' ',' ','Some',' ','lines',' ','had',' ','multiple',' ','(','3','–','4',')',' ','entities','.','\n",
      "','\n",
      "','\n",
      "',' ',' ','Hall','uci','nations',':',' ','Occasional','ly',',',' ','Gemini',' ','would',' ','produce',' ','incomplete',' ','JSON',' ','or',' ','trun','cated',' ','text','.',' ','We',' ','filtered',' ','or',' ','corrected',' ','these',' ','by',' ','re','-','prompting',' ','or',' ','manually',' ','disc','arding',' ','them',' ','if',' ','they',' ','were',' ','too',' ','broken','.','\n",
      "','\n",
      "','\n",
      "',' ',' ','Red','und','ancy',':',' ','Red','und','ancy',' ','was',' ','low',',',' ','it',' ','was',' ','ensured',' ','that',' ','the',' ','whole',' ','model',' ','had',' ','the',' ','context',' ','of',' ','the',' ','entire',' ','chat','.',' ','Certain',' ','words',' ','were',' ','found',' ','to',' ','be',' ','repeated',' ','in',' ','a',' ','few',' ','sentences',';',' ','however',',',' ','either',' ','the',' ','application',' ','of',' ','words',' ','was',' ','in',' ','a',' ','different',' ','context',',',' ','or',' ','there',' ','was',' ','enough',' ','diversity',' ','between',' ','the',' ','sentences','.',' ','All',' ','sentences',' ','could',' ','not',' ','be',' ','manually',' ','verified',',',' ','but',' ','a',' ','sample',' ','portion',' ','was',' ','randomly',' ','checked',' ','(','about',' ','100',').','\n",
      "',' ',' ','2','.','4',' ','Split','ting',' ','into',' ','Train',' ','&',' ','Eva','l','\n",
      "',' ',' ','2','.','4','.','1',' ','State',' ','Definition','\n",
      "',' ',' ','Each',' ','sample',' ','had',' ','a',' ','(','language',',',' ','domain',',',' ','gen_','temperature',')',' ','tuple','.',' ','We',' ','call',' ','this',' ','a',' ','<|unknown_token|>','state','<|unknown_token|>','.',' ','We',' ','aim','ed',' ','for',' ','each',' ','state',' ','to',' ','appear',' ','in',' ','both',' ','train',' ','and',' ','eval',' ','in',' ','a',' ','3',':','1',' ','ra','ti','o','.',' ','However',',',' ','we',' ','also',' ','had',' ','exceptions',':','\n",
      "',' ',' ','If',' ','a',' ','state',' ','had',' ','only',' ','1',' ','sample',',',' ','we',' ','randomly',' ','assigned',' ','it',' ','with',' ','50','%',' ','probability',' ','to',' ','train',' ','or',' ','eval','.','\n",
      "',' ',' ','If',' ','a',' ','state',' ','had',' ','2',' ','or',' ','more',' ','samples',',',' ','we',' ','tried',' ','a',' ','3',':','1',' ','ra','ti','o','.',' ','But',' ','if',' ','that',' ','left',' ','a',' ','state',' ','with',' ','0',' ','in',' ','eval',',',' ','we',' ','forced',' ','at',' ','least',' ','one',' ','sample',' ','into',' ','eval','.','\n",
      "',' ',' ','Hence',',',' ','we',' ','ended',' ','up',' ','with',' ','~','1',',','185',' ','train',' ','samples',' ','and',' ','~','415',' ','eval',' ','samples','.','\n",
      "',' ',' ','Note',':',' ','A',' ','potential',' ','shortcoming',' ','of',' ','the',' ','data',' ','could',' ','be',' ','the',' ','lack',' ','of',' ','multiple',' ','elements',' ','present',' ','in',' ','each',' ','state','.',' ','Thus',' ','requiring',' ','a',' ','bit',' ','more',' ','diversity',',',' ','class',' ','balancing',' ','and',' ','a',' ','larger',' ','dataset','.','\n",
      "',' ',' ','2','.','4','.','2',' ','Statistical',' ','Distribution','\n",
      "',' ',' ','We',' ','carefully',' ','examined',' ','these',' ','to',' ','ensure',' ','no',' ','major',' ','domain','-','language',' ','cluster',' ','was',' ','starved','.',' ','The',' ','distribution',' ','of',' ','the',' ','total',' ','data',' ','can',' ','be',' ','found',' ','in',' ','the',' ','data',' ','distribution',' ','t','ab','.','\n",
      "',' ',' ','2','.','5',' ','Final',' ','Dataset',' ','&',' ','Access','\n",
      "',' ',' ','Size',':',' ','1',',','600',' ','total','.','\n",
      "',' ',' ','Train',':',' ','~','1',',','185','.','\n",
      "',' ',' ','Eva','l',':',' ','~','415','.','\n",
      "',' ',' ','Format',':',' ','Provided',' ','as',' ','CSV',' ','with',' ','columns',' ','sl_','no',',',' ','language',',',' ','input',',',' ','output',',',' ','domain',',',' ','gen_','temperature','.',' ','Also',' ','shared',' ','on',' ','Hug','ging',' ','Fa','ce',' ','at',' ','Tas','may','-','Ti','b','/','sar','vam','-','entity','-','recognition','-','ge','mini','-','2','.','0','-','flash','-','thinking','-','0','1','-','21','-','distill','-','1600','.','\n",
      "',' ',' ','Note',':',' ','Being',' ','synthetic',',',' ','it',' ','might',' ','not',' ','perfectly',' ','reflect',' ','real',' ','usage',' ','in',' ','morphological',' ','or',' ','domain','-','specific',' ','complexities','.',' ','However',',',' ','it','<|unknown_token|>','s',' ','still',' ','valuable',' ','for',' ','a',' ','first',' ','pass',' ','at',' ','the',' ','normalization',' ','task','.','\n",
      "','\n",
      "','\n",
      "','\n",
      "',' ',' ','2','.','6',' ','Issues',' ','Found',' ','in',' ','the',' ','Dataset',' ','(','Impact','ing',' ','Model',' ','Performance',')','\n",
      "',' ',' ','Limited',' ','Very',' ','Long',' ','Sent','ences','\n",
      "',' ',' ','The',' ','dataset',' ','mostly',' ','contains',' ','short',' ','to',' ','medium','-','length',' ','sentences',' ','(','under',' ','~','100',' ','tokens',').',' ','Consequently',',',' ','extremely',' ','long',' ','sentences',' ','(','150','–','200','+',' ','tokens',')',' ','are',' ','underre','presented','.','\n",
      "','\n",
      "','\n",
      "',' ',' ','Restricted',' ','Decimal',' ','Number',' ','Variety','\n",
      "',' ',' ','Although',' ','decimal',' ','numbers',' ','appear',' ','in',' ','the',' ','dataset',',',' ','they',' ','are',' ','not',' ','comprehensive','ly',' ','represented',' ','(','e','.','g','.,',' ','2','.','5',' ','but',' ','rarely',' ','2','.','75',',',' ','3','.','14','159',',',' ','etc','.).',' ','This',' ','relative',' ','sparse','ness',' ','leads',' ','to',' ','the',' ','model',' ','mis','handling',' ','more',' ','complex',' ','decimal',' ','expansions','.','\n",
      "','\n",
      "','\n",
      "',' ',' ','Rare',' ','Date',' ','Formats','\n",
      "',' ',' ','Formats',' ','like',' ','<|unknown_token|>','2',' ','taa','rik',' ','March',' ','2016',' ','ko','…','<|unknown_token|>',' ','are',' ','infrequent','.',' ','Most',' ','examples',' ','stick',' ','to',' ','more',' ','standardized',' ','forms',' ','(','DD','/','MM','/','YY','YY',',',' ','DD','-','MM','-','YY','YY',',',' ','textual',' ','month',' ','names',' ','in',' ','scripts',',',' ','etc','.).',' ','Hence',',',' ','the',' ','model',' ','might',' ','fail',' ','to',' ','parse',' ','or',' ','transform',' ','dates',' ','expressed',' ','in',' ','col','loqu','ial',' ','or',' ','semi','-','trans','lite','rated',' ','styles','.','\n",
      "','\n",
      "','\n",
      "',' ',' ','Complex',' ','or',' ','Uncom','mon',' ','Uni','t',' ','Handling','\n",
      "',' ',' ','Rare',' ','or',' ','domain','-','specific',' ','units',' ','(','e','.','g','.,',' ','mm','Hg',',',' ','mE','q','/','L',')',' ','are',' ','not',' ','well','-','represented','.',' ','The',' ','dataset',' ','focuses',' ','on',' ','more',' ','common',' ','units',' ','(','k','g',',',' ','mg',',',' ','km','/','h',',',' ','etc','.),',' ','so',' ','the',' ','model',' ','may',' ','hallucin','ate',' ','or',' ','omit',' ','expansions',' ','for',' ','those',' ','complex',',',' ','less',' ','frequent',' ','units','.','\n",
      "','\n",
      "','\n",
      "',' ',' ','In','sufficient',' ','Numeric',' ','Range','\n",
      "',' ',' ','Synthetic',' ','examples',' ','typically',' ','use',' ','smaller',' ','or',' ','moderately',' ','sized',' ','numbers','.',' ','Very',' ','large',' ','numbers',' ','or',' ','close',' ','numer','ic',' ','values',' ','(','e','.','g','.,',' ','74',' ','vs','.',' ','75',')',' ','appear',' ','only',' ','occasionally','.',' ','This',' ','can',' ','lead',' ','the',' ','model',' ','to',' ','con','fuse',' ','near','-','similar',' ','values',',',' ','revealing',' ','a',' ','gap',' ','in',' ','numer','ic',' ','variety',' ','within',' ','the',' ','training',' ','data','.','\n",
      "','\n",
      "','\n",
      "',' ',' ','In','sufficient',' ','number',' ','of',' ','examples','\n",
      "',' ',' ','Although',' ','the',' ','dataset',' ','was',' ','relatively',' ','diverse',',',' ','still',' ','a',' ','larger',' ','dataset',' ','covering',' ','all',' ','different',' ','sentence',' ','types',',',' ','like',' ','uncommon',' ','date',' ','types',',',' ','multiple',' ','examples',' ','for',' ','each',' ','state','.',' ','More',' ','diversity',' ','per',' ','language',',',' ','larger',' ','sentences',',',' ','sentences',' ','with',' ','decimal','s',' ','and',' ','larger',' ','numer','ic',' ','ranges',',',' ','etc','.',' ','Would',' ','have',' ','provided',' ','a',' ','good',' ','base',' ','for',' ','training','.',' ','Something',' ','like',' ','~','10K',' ','queries',' ','would',' ','push',' ','the',' ','model',' ','to',' ','its',' ','maximum','.','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "',' ',' ','3','.',' ','Three',' ','Appro','aches',' ','to',' ','Entity',' ','Normal','ization','\n",
      "',' ',' ','We',' ','tried',' ','three',' ','approaches',' ','to',' ','map',' ','input',' ','->',' ','normalized',' ','output',' ','for',' ','the',' ','same',' ','dataset','.',' ','Below',',',' ','each',' ','is',' ','explained',' ','in',' ','detail','.','\n",
      "',' ',' ','3','.','1',' ','Agen','tic',' ','(','Prompt','-','Based',')',' ','Approach','\n",
      "',' ',' ','3','.','1','.','1',' ','Conceptual',' ','Overview','\n",
      "',' ',' ','This',' ','method',' ','uses',' ','a',' ','single',' ','large',' ','language',' ','model',',',' ','e','.','g','.,',' ','uns','lo','th','/','Meta','-','Lla','ma','-','3','.','1','-','8B',',',' ','sar','vam','ai','/','sar','vam','-','1',',',' ','and',' ','Qw','en','/','Qw','en2','.','5','-','3B','.',' ','We',' ','set',' ','up',' ','a',' ','chat','-','like',' ','scenario',':','\n",
      "',' ',' ','We',' ','prompted',' ','the',' ','model',' ','in',' ','a',' ','non','-','instruct',' ','manner',' ','since',' ','we',' ','were',' ','using',' ','the',' ','base',' ','model',' ','versions','.',' ','We',' ','created',' ','a',' ','movie','-','script','-','like',' ','prompt','.',' ','Conta','ining',' ','the',' ','back','-','and','-','forth',' ','between',' ','the',' ','two',' ','main',' ','characters','.',' ','This',' ','included',':','\n",
      "',' ',' ','The',' ','main',' ','character',' ','describing',' ','the',' ','task','\n",
      "',' ',' ','The',' ','main',' ','character',' ','giving',' ','an',' ','example','\n",
      "',' ',' ','The',' ','helper',' ','character',' ','giving',' ','a',' ','feedback',' ','of',' ','what',' ','they',' ','understand','\n",
      "',' ',' ','The',' ','main',' ','character',' ','asking',' ','to',' ','transform',' ','a',' ','sentence','\n",
      "',' ',' ','The',' ','helper',' ','character',' ','giving',' ','a',' ','response',' ','of',' ','the',' ','sentence','\n",
      "',' ',' ','This',' ','cycle',' ','getting',' ','repeated',' ','(','multi','-','shot',' ','prompting',')','\n",
      "',' ',' ','We',' ','take',' ','input',' ','the',' ','sentence','\n",
      "',' ',' ','We',' ','analyze',' ','the',' ','language',' ','of',' ','the',' ','sentence',' ','(','using',' ','an',' ','algorithmic',' ','language',' ','detector',' ','-',' ','in',' ','algorithmic',' ','approach',')',' ','and',' ','use',' ','the',' ','language','<|unknown_token|>','s',' ','specific',' ','prompt',' ','to',' ','feed',' ','the',' ','sentence',' ','for',' ','transformation','.','\n",
      "',' ',' ','The',' ','model',' ','attempts',' ','to',' ','produce',' ','an',' ','immediate',',',' ','direct',' ','transformation','.','\n",
      "',' ',' ','Note',':',' ','The',' ','language',' ','detector',' ','was',' ','used',' ','to',' ','use',' ','the',' ','stored',' ','pre','-','translated',' ','prompt',' ','in',' ','the',' ','same',' ','language',' ','of',' ','the',' ','input',' ','text',',',' ','since',' ','the',' ','model',' ','was',' ','facing',' ','a',' ','lot',' ','of',' ','issues',' ','with',' ','translation',' ','if',' ','the',' ','prompt',' ','included',' ','multiple',' ','languages',' ','(','i','.','e','.',' ','instruction',' ','in',' ','one',' ','language',',',' ','examples',' ','in',' ','other',' ','and',' ','final',' ','sentence',' ','in',' ','a',' ','third',' ','one',').',' ','Also',',',' ','since',' ','this',' ','method',' ','was',' ','only',' ','experimental',' ','and',' ','performed',' ','poorly',',',' ','only',' ','Hindi',' ','examples',' ','were',' ','tested',' ','(','since',' ','we',' ','had',' ','only',' ','crafted',' ','the',' ','Hindi',' ','prompt',').',' ','Still',',',' ','if',' ','the',' ','prompt',' ','is',' ','translated',' ','to',' ','other',' ','languages',' ','and',' ','stored',',',' ','the',' ','system',' ','can',' ','also',' ','be',' ','tested',' ','on',' ','other',' ','language',' ','samples','.',' ','Hindi',' ','was',' ','chosen',' ','just',' ','for',' ','testing',' ','and',' ','demonstration',' ','purposes',',',' ','and',' ','since',' ','this',' ','method',' ','was',' ','not',' ','followed',',',' ','other',' ','language',' ','samples',' ','were',' ','not',' ','added','.','\n",
      "',' ',' ','3','.','1','.','2',' ','Limit','ations',' ','&',' ','Observations','\n",
      "',' ',' ','Off','-','Topic',' ','or',' ','Extra',' ','Text',':','\n",
      "',' ',' ','The',' ','model',' ','sometimes',' ','appe','nded',' ','partial',' ','dialogues',' ','or',' ','new',' ','sentences',',',' ','where',' ','it',' ','continued',' ','the',' ','text',' ','of',' ','the',' ','script','.','\n",
      "',' ',' ','Or',' ','it',' ','repeated',' ','the',' ','user','<|unknown_token|>','s',' ','instructions',' ','instead',' ','of',' ','producing',' ','a',' ','direct',' ','final',' ','result','.','\n",
      "','\n",
      "',' ',' ','Poor',' ','Numeric',' ','Accu','racy',':','\n",
      "',' ',' ','E','.','g','.,',' ','¥','120',' ','→',' ','120',' ','dollars',' ','or',' ','mes','sing',' ','up',' ','the',' ','spelled','-','out',' ','date',' ','for',' ','certain',' ','languages','.','\n",
      "',' ',' ','No',' ','specialized',' ','training',' ','was',' ','done',',',' ','so',' ','it','<|unknown_token|>','s',' ','guess','work','.','\n",
      "','\n",
      "','\n",
      "',' ',' ','In','consistent',' ','Across',' ','Languages',':','\n",
      "',' ',' ','In',' ','Tamil',' ','or',' ','Od','ia',',',' ','the',' ','model',' ','might',' ','default',' ','to',' ','trans','litera','tion',' ','or',' ','partial',' ','expansions',' ','in',' ','English','.','\n",
      "',' ',' ','Variation',' ','in',' ','performance',' ','was',' ','high',' ','from',' ','one',' ','prompt',' ','to',' ','another','.','\n",
      "',' ',' ','3','.','1','.','3',' ','Model',' ','performance',' ','and',' ','age','ntic',' ','use','\n",
      "',' ',' ','Lla','ma',' ','performed',' ','quite',' ','well',' ','compared',' ','to',' ','Qw','en',' ','and',' ','Sar','vam','.',' ','For',' ','the',' ','others',',',' ','it',' ','often',' ','converted',' ','the',' ','entities',' ','to',' ','words',' ','in',' ','English',' ','and',' ','often',' ','forgot',' ','to',' ','convert',' ','numbers',' ','to',' ','words','.',' ','Thus',',',' ','an',' ','iterative',' ','preset',' ','set',' ','of',' ','prompts',' ','was',' ','stored',',',' ','which',' ','incorporated',' ','the',' ','model','<|unknown_token|>','s',' ','new',' ','responses',' ','and',' ','at',' ','each',' ','step',',',' ','into',' ','updated',' ','prompts',' ','based',' ','on',' ','the',' ','movie','-','script',' ','structure',',',' ','instru','cting',' ','on',' ','the',' ','removal',' ','of',' ','English',' ','words',' ','and',' ','conversion',' ','of',' ','numbers',' ','to',' ','words','.',' ','This',' ','improved',' ','Qw','en',' ','and',' ','Sar','vam',''','s',' ','performance',' ','relatively',' ','well','.','\n",
      "','\n",
      "',' ',' ','All',' ','the',' ','prompts',' ','for',' ','this',' ','section',' ','can',' ','be',' ','found',' ','in',' ','the',' ','Agen','tic',' ','prompt',' ','t','ab','.','\n",
      "',' ',' ','Conclusion',':',' ','The',' ','age','ntic',' ','approach',' ','is',' ','the',' ','easiest',' ','to',' ','set',' ','up',' ','but',' ','gave',' ','the',' ','weakest',' ','results',' ','in',' ','systematic','ally',' ','normal','izing',' ','multi','-','ling','ual',' ','numer','ic',' ','data','.',' ','It','<|unknown_token|>','s',' ','a',' ','fall','back',' ','method',' ','if',' ','one',' ','can','<|unknown_token|>','t',' ','or',' ','won','<|unknown_token|>','t',' ','do',' ','fine','-','tuning',' ','or',' ','code',' ','a',' ','rule','-','based',' ','pipeline',',',' ','but',' ','it',' ','is',' ','not',' ','recommended',' ','for',' ','serious',' ','usage','.','\n",
      "',' ',' ','3','.','2',' ','Algorithm','ic',' ','(','Rule','-','Based',')',' ','Approach',' ','\n",
      "',' ',' ','3','.','2','.','1',' ','High','-','Level',' ','Flow','\n",
      "',' ',' ','Script',' ','&',' ','Language',' ','Detection','\n",
      "',' ',' ','We',' ','first',' ','identify',' ','which',' ','Ind','ic',' ','language',' ','(','Hindi',',',' ','Tamil',',',' ','Telugu',',',' ','etc','.)',' ','we','<|unknown_token|>','re',' ','dealing',' ','with',',',' ','typically',' ','via',' ','script',' ','range',' ','checks',' ','or',' ','secondary',' ','heur','istics','.','\n",
      "','\n",
      "','\n",
      "',' ',' ','Date',' ','conversion','\n",
      "',' ',' ','Dates',' ','are',' ','recognised',' ','by',' ','pattern',' ','matching',' ','in',' ','the',' ','sentence','\n",
      "',' ',' ','Then',' ','the',' ','forms',' ','of',' ','these',' ','dates',' ','are',' ','recognised',',',' ','and',' ','a',' ','placeholder',' ','is',' ','attached',' ','indicating',' ','that',' ','it',' ','is',' ','a',' ','date',' ','token',',',' ','to',' ','be',' ','converted',' ','to',' ','words',' ','in',' ','a',' ','different',' ','manner',' ','(','later',')','\n",
      "','\n",
      "','\n",
      "',' ',' ','Regex',' ','&',' ','Pattern',' ','Sea','rches','\n",
      "',' ',' ','We',' ','define',' ','layered',',',' ','specialized',' ','reg','exes',' ','for',' ','dates',',',' ','currencies',',',' ','units',',',' ','and',' ','decimal','/','whole',' ','numbers','.','\n",
      "',' ',' ','Each',' ','date',' ','match',' ','is',' ','replaced',' ','or',' ','<|unknown_token|>','tagged','<|unknown_token|>',' ','with',' ','a',' ','placeholder',' ','(','e','.','g','.',' ',':$','date',',',' ','etc','.)',' ','while',' ','we',' ','store',' ','the',' ','structured',' ','parse',' ','data',' ','in',' ','some',' ','dictionary','.','\n",
      "','\n",
      "','\n",
      "',' ',' ','Language','-','Specific',' ','Spe','lled','-','Out',' ','Expan','sions','\n",
      "',' ',' ','For',' ','each',' ','placeholder',',',' ','we',' ','look',' ','up',' ','the',' ','expansions',' ','in',' ','the',' ','appropriate',' ','script',':',' ','e','.','x','.',' ','for',' ','Hindi',':',' ','\"$','\"',' ','->',' ','\"','ड','ॉ','ल','र','\",',' ','mg',' ','->',' ','\"','म','ि','ल','ी','ग','्','र','ा','म','\",',' ','etc','.','\n",
      "',' ',' ','For',' ','the',' ','numbers',',',' ','we',' ','modify',' ','a',' ','nu','m2','words',' ','function',' ','from',' ','a',' ','pre','-','built',' ','library',' ','to',' ','get',' ','the',' ','word',' ','for',' ','the',' ','number',' ','after',' ','the',' ','conversion',' ','process','.','\n",
      "','\n",
      "','\n",
      "',' ',' ','Re','-','Assembly','\n",
      "',' ',' ','After',' ','all',' ','place','holders',' ','are',' ','recognized',',',' ','we',' ','replace',' ','them',' ','one',' ','by',' ','one',' ','in',' ','the',' ','text','.',' ','We',' ','ensure',' ','that',' ','spacing',',',' ','punctu','ation',',',' ','or',' ','original',' ','text',' ','ordering',' ','is',' ','preserved','.','\n",
      "','\n",
      "','\n",
      "',' ',' ','Output','\n",
      "',' ',' ','We',' ','finalize',' ','the',' ','text',' ','with',' ','all',' ','expansions',',',' ','checking',' ','for',' ','leftover',' ','un','recognized',' ','patterns',' ','or',' ','possible',' ','partial',' ','collisions','.','\n",
      "',' ',' ','Below',',',' ','we',' ','break',' ','down',' ','each',' ','stage',' ','in',' ','more',' ','granular',' ','detail','.','\n",
      "',' ',' ','3','.','2','.','2',' ','Language',' ','&',' ','Script',' ','Identification','\n",
      "',' ',' ','Most',' ','of',' ','the',' ','algorithm','<|unknown_token|>','s',' ','expansions',' ','are',' ','language','/','script','-','depend','ent','—','for',' ','instance',',',' ','<|unknown_token|>','dollar','<|unknown_token|>',' ','in',' ','Hindi',' ','is',' ','<|unknown_token|>','ड','ॉ','ल','र',',','<|unknown_token|>',' ','but',' ','in',' ','Tamil',' ','it','<|unknown_token|>','s',' ','<|unknown_token|>','ட','ா','ல','ர','்','.','<|unknown_token|>',' ','Therefore',',',' ','we',' ','must',' ','detect',' ','which',' ','language',' ','we','<|unknown_token|>','re',' ','dealing',' ','with','.','\n",
      "',' ',' ','Script',' ','Range','\n",
      "',' ',' ','For',' ','each',' ','character',',',' ','check',' ','if',' ','it',' ','falls',' ','in',' ','a',' ','known',' ','block',' ','(','e','.','g','.',' ','Dev','ana','gari',':',' ','U','+','09','00','–','U','+','09','7F',').',' ','If',' ','80','%','+',' ','of',' ','the',' ','text','<|unknown_token|>','s',' ','characters',' ','are',' ','in',' ','the',' ','Dev','ana','gari',' ','range',',',' ','we',' ','guess',' ','it','<|unknown_token|>','s',' ','Hindi',' ','or',' ','Mara','thi','.','\n",
      "',' ',' ','Gur','muk','hi',' ','block',':',' ','likely',' ','Punjabi',',',' ','etc','.','\n",
      "',' ',' ','A',' ','script',' ','confidence',' ','score',' ','is',' ','calculated',' ','based',' ','on',' ','total',' ','words',' ','found',' ','in',' ','script',' ','X',' ','vs',' ','script',' ','Y','.','\n",
      "','\n",
      "','\n",
      "',' ',' ','Dis','ambigu','ation',' ','(','if',' ','multiple',' ','languages',' ','share',' ','the',' ','same',' ','script',',',' ','e','.','g','.',' ','Hindi',' ','vs','.',' ','Mara','thi',')','\n",
      "',' ',' ','Mara','thi',' ','tends',' ','to',' ','use',' ','a',' ','given',' ','set',' ','of',' ','characters',' ','or',' ','alpha','bets',' ','(','ex',':',' ','े',' ','or',' ','vir','ama','/','hal','ant',')',' ','\n",
      "',' ',' ','Similarly',' ','Hindi',' ','has',' ','some',' ','common',' ','set',' ','of',' ','alpha','bets',' ','(','ex',':',' ','ा',' ','or',' ','nasal','ization',' ','marks',')','\n",
      "',' ',' ','Similarly',',',' ','vowel',' ','marks',' ','have',' ','different',' ','frequencies',' ','in',' ','each',' ','language','.','\n",
      "','\n",
      "','\n",
      "',' ',' ','Prob','ability',' ','Calculation',' ','\n",
      "',' ',' ','Language','<|unknown_token|>','s',' ','script','-','based',' ','probabilities',' ','are',' ','directly',' ','calculated',' ','based',' ','on',' ','script',' ','distribution',' ','and',' ','language','-','specific',' ','properties',' ','\n",
      "',' ',' ','Each',' ','script',' ','is',' ','previously',' ','mapped',' ','to',' ','a',' ','language',';',' ','thus',',',' ','the',' ','language','<|unknown_token|>','s',' ','score',' ','is',' ','directly',' ','affected','.','\n",
      "',' ',' ','If',' ','the',' ','number',' ','of',' ','languages',' ','corresponding',' ','to',' ','the',' ','script',' ','is',' ','one',',',' ','then',' ','a',' ','direct',' ','score',' ','is',' ','directly',' ','given',' ','for',' ','the',' ','percentage',' ','of',' ','the',' ','characters',' ','of',' ','the',' ','script',' ','out',' ','of',' ','total',' ','chars','\n",
      "',' ',' ','For',' ','Hindi',' ','and',' ','mara','thi',',',' ','we',' ','start',' ','with',' ','a',' ','base',' ','score',' ','of',' ','50',' ','and',' ','a',' ','max',' ','adjustment',' ','per',' ','feature',' ','score',' ','of',' ','50',' ','(','we',' ','check',' ','for',' ','3',' ','features',',',' ','two',' ','features',' ','favor','ing',' ','each',' ','language',' ','(','one',' ','is',' ','common',' ','for',' ','both','),',' ','and',' ','for',' ','the',' ','occurrence',' ','of',' ','each',' ','feature',',',' ','we',' ','find',' ','its',' ','percentage',' ','occurrence',',',' ','and',' ','adjust',' ','the',' ','score',' ','count',' ','accordingly','\n",
      "',' ',' ','Maximum',' ','score',' ','adjustment',' ','could',' ','be',' ','~','100',' ','(','for',' ','complete',' ','confidence',' ','in',' ','two',' ','favor','ing',' ','and',' ','zero',' ','confidence',' ','in',' ','opposing',' ','features',',',' ','which',' ','could',' ','take',' ','up',' ','the',' ','maximum',' ','language',' ','score',' ','to',' ','be',' ','150',').',' ','But',' ','practically',',',' ','that',' ','is',' ','not',' ','likely',' ','because',' ','of',' ','con','son','ants',' ','and',' ','other',' ','language','-','based',' ','characters',' ','present',' ','in',' ','the',' ','sentence',',',' ','taking',' ','the',' ','individual',' ','scores',' ','down',' ','(','since',' ','they',' ','are',' ','based',' ','on',' ','character',' ','probability',')',' ','or',' ','increasing',' ','the',' ','opposing',' ','scores','.','\n",
      "',' ',' ','Still',',',' ','a',' ','maximum',' ','threshold',' ','of',' ','90',' ','and',' ','a',' ','minimum',' ','threshold',' ','of',' ','10',' ','is',' ','kept',' ','to',' ','keep',' ','the',' ','probabilities',' ','from',' ','getting',' ','skewed','.','\n",
      "','\n",
      "','\n",
      "',' ',' ','Further',' ','Robust','ness',' ','and',' ','Dis','ambigu','ation','\n",
      "',' ',' ','Despite',' ','language','-','based',' ','features',',',' ','we',' ','look',' ','for',' ','a',' ','more',' ','robust',' ','method',' ','of',' ','language',' ','detection','\n",
      "',' ',' ','This',' ','includes',' ','n','-','gr','am',' ','probability',' ','extraction',',',' ','common',' ','word',' ','markers',' ','count',' ','per',' ','language',',',' ','and',' ','dictionary','-','based',' ','stop','-','word',' ','matching','.','\n",
      "',' ',' ','Dictionary','-','based',' ','matching','\n",
      "',' ',' ','A',' ','dictionary',' ','of',' ','each',' ','language',' ','is',' ','created',' ','based',' ','on',' ','content',' ','available',' ','on',' ','GitHub',',',' ','in',' ','NLP',' ','libraries',',',' ','or',' ','it',' ','is',' ','just',' ','generated',' ','using',' ','Gemini',' ','(','for',' ','at',' ','least',' ','100','-','250',' ','most',' ','common',' ','words',')','\n",
      "',' ',' ','Based',' ','on',' ','that',',',' ','simple',' ','word',' ','matching',' ','is',' ','performed',' ','and',' ','checked',' ','for',' ','the',' ','presence',' ','of',' ','dictionary',' ','stop',' ','words',' ','in',' ','the',' ','sentence',',',' ','and',' ','a',' ','score',' ','is',' ','calculated',' ','based',' ','on',' ','the',' ','percentage',' ','of',' ','common',' ','words',' ','in',' ','each',' ','language','.','\n",
      "',' ',' ','After',' ','the',' ','stop','words',' ','data',' ','was',' ','created',',',' ','it',' ','was',' ','stored',' ','in',' ','.','txt',' ','files',' ','with',' ','name',' ','for','mat',' ','(','language','_st','op_','words','.','txt','),',' ','and',' ','then',' ','all',' ','of',' ','this',' ','was',' ','stored',' ','in',' ','a',' ','zip',' ','called',' ',''','stop','_word','s_a','rchive','.','zip',''.','\n",
      "','\n",
      "','\n",
      "',' ',' ','Common',' ','marker','-','based',' ','matching','\n",
      "',' ',' ','We',' ','can',' ','look',' ','for',' ','common',' ','words',' ','or',' ','suff','ixes',' ','(','<|unknown_token|>','ह','ै','<|unknown_token|>',',',' ','<|unknown_token|>','क','ी','<|unknown_token|>',',',' ','<|unknown_token|>','म','ें','<|unknown_token|>',' ','for',' ','Hindi',' ','vs','.',' ','<|unknown_token|>','आ','ह','े','<|unknown_token|>',',',' ','<|unknown_token|>','अ','स','त','ो','<|unknown_token|>',' ','in',' ','Mara','thi',',',' ','<|unknown_token|>','ந','ா','ன','்','<|unknown_token|>',',',' ','<|unknown_token|>','ந','ீ','<|unknown_token|>',',',' ','<|unknown_token|>','அ','வ','ள','்','<|unknown_token|>',' ','in',' ','Tamil',',',' ','etc','.).','\n",
      "',' ',' ','These',' ','are',' ','different',' ','from',' ','dictionary',' ','words',',',' ','since',' ','they',' ','are',' ','looking',' ','at',' ','which',' ','words',' ','were',' ','present',' ','from',' ','which',' ','language',' ','and',' ','trying',' ','to',' ','get',' ','a',' ','score',' ','on',' ','the',' ','number',' ','of',' ','unique',' ','words',' ','per',' ','language',' ','/',' ','total',' ','words',' ','based',' ','on',' ','100',' ','or',' ','200',' ','most',' ','common',' ','words','.','\n",
      "',' ',' ','This',' ','is',' ','just',' ','using',' ','the',' ','markers',',',' ','which',' ','are',' ','to',' ','be',' ','commonly',' ','found',' ','across',',',' ','and',' ','trying',' ','to',' ','measure',' ','an',' ','occurrence',' ','score',' ','(','how',' ','frequently',' ','markers',' ','occur',')',' ','per',' ','language',' ','after',' ','dividing',' ','it',' ','by',' ','total',' ','marker',' ','frequency','.','\n",
      "','\n",
      "','\n",
      "',' ',' ','Ng','ram',' ','based',' ','matching','\n",
      "',' ',' ','Ng','ram',' ','is',' ','the',' ','concept',' ','of',' ','using',' ','n','-','length',' ','character',' ','combinations',' ','and',' ','creating',' ','a',' ','probabilist','ic',' ','match',' ','on',' ','top',' ','of',' ','it','.','\n",
      "',' ',' ','This',' ','requires',' ','the',' ','total',' ','unique',' ','set',' ','of',' ','common',' ','markers',' ','and',' ','unique',' ','stop',' ','words',' ','(','from',' ','the',' ','dictionary',')',' ','that',' ','are',' ','joined',' ','together',' ','with',' ','spaces','.','\n",
      "',' ',' ','Then',',',' ','the',' ','spaces',' ','are',' ','replaced',' ','by',' ','(','n','-','1',')',' ','<|unknown_token|>','_','<|unknown_token|>',' ','signs','.',' ','For',' ','example',':',' ','<|unknown_token|>','Hi',' ','I',' ','am',' ','Tas','may','<|unknown_token|>',' ','for',' ','n','=','3',' ','is',' ','<|unknown_token|>','Hi','__','I_','_am','__','Tas','may','<|unknown_token|>',' ','where',' ','each',' ','space',' ','was',' ','replaced',' ','with',' ','two',' ','consecutive',' ','under','scores',' ','<|unknown_token|>','__','<|unknown_token|>','.',' ','\n",
      "',' ',' ','This',' ','now',' ','creates',' ','a',' ','text',' ','on',' ','which',' ','we',' ','can',' ','iterate',' ','over',' ','a',' ','window',' ','of',' ','length',',',' ','and',' ','for',' ','each',' ','unique',' ','window',',',' ','we',' ','can',' ','tally',' ','the',' ','counts','.','\n",
      "',' ',' ','After',' ','we',' ','have',' ','the',' ','total',' ','window',' ','counts',' ','and',' ','the',' ','individual',' ','count',' ','of',' ','the',' ','unique',' ','windows',',',' ','we',' ','have',' ','built',' ','an',' ','n','-','gr','am',' ','model','.','\n",
      "',' ',' ','Now',',',' ','we',' ','proceed',' ','to',' ','do',' ','the',' ','same',' ','thing',' ','with',' ','the',' ','input',' ','text',' ','and',' ','match',' ','with',' ','the',' ','available',' ','n','-','gr','ams','.',' ','If',' ','a',' ','match',' ','is',' ','present',',',' ','we',' ','tally',' ','the',' ','log',' ','probability',' ','of',' ','the',' ','match',' ','in',' ','our',' ','score','.',' ','Else',',',' ','we',' ','add',' ','a',' ','log',' ','of',' ','1e','-','20',' ','(','a',' ','very',' ','small',' ','number',' ','instead',' ','of',' ','0',' ','to',' ','smo','othen',' ','the',' ','probability',')','\n",
      "',' ',' ','We',' ','divide',' ','the',' ','score',' ','by',' ','the',' ','total',' ','number',' ','of',' ','n','-','gr','ams',' ','found',' ','in',' ','the',' ','language',' ','to',' ','get',' ','an',' ','average',' ','score',' ','for',' ','each',' ','language',' ','(','average',' ','score',' ','instead',' ','of',' ','total',' ','score',' ','to',' ','measure',' ','the',' ','relative',' ','quality',' ','of',' ','each',' ','language',' ','and',' ','not',' ','just',' ','the',' ','quantity',' ','part','.',' ','It',' ','may',' ','also',' ','be',' ','that',' ','the',' ','dictionary','/','marker',' ','size',' ','of',' ','a',' ','language',' ','was',' ','larger',',',' ','thus',' ','creating',' ','a',' ','higher',' ','absolute',' ','match',' ','of',' ','the',' ','n','-','gr','ams',' ','for',' ','the',' ','language','.','\n",
      "',' ',' ','Then',',',' ','we',' ','normalize',' ','the',' ','scores',' ','between',' ','0',' ','and',' ','1',' ','by',' ','the',' ','maximum',' ','language',' ','score',' ','by',' ','an',' ','exponential',' ','normalization',' ','technique',' ','(','esc','ore','-','max_','score',').','\n",
      "',' ',' ','After',' ','this',',',' ','a',' ','final',' ','probability',' ','score',' ','is',' ','calculated',' ','by',' ','dividing',' ','language',' ','scores',' ','by',' ','the',' ','total',' ','scores','.','\n",
      "',' ',' ','We',' ','also',' ','expe','rimented',' ','with',' ','various',' ','other',' ','smooth','ening',' ','techniques',' ','because',' ','of',' ','the',' ','low',' ','probabilities',' ','and',' ','the',' ','difficult','-','to','-','handle',' ','nature',' ','of',' ','n','-','gr','am',' ','matches',' ','(','but',' ','eventually',' ','settled',' ','with',' ','this',')','\n",
      "',' ',' ','For',' ','the',' ','value',' ','of',' ','n',',',' ','we',' ','started',' ','with',' ','n','=','3',' ','and',' ','expe','rimented',' ','with',' ','n','=','2',' ','and',' ','n','=','4',' ','and',' ','found',' ','n','=','4',' ','to',' ','be',' ','the',' ','best',' ','performer','.',' ','Higher',' ','values',' ','were',' ','not',' ','considered',' ','since',' ','they',' ','have',' ','quite',' ','a',' ','high',' ','probability',' ','of',' ','shooting',' ','into',' ','next',' ','words',' ','(','which',' ','may',' ','make',' ','the',' ','use',' ','case',' ','not',' ','ideal',').','\n",
      "',' ',' ','Note',':',' ','If',' ','we',' ','guess',' ','incorrectly',',',' ','expansions',' ','might',' ','reference',' ','the',' ','wrong',' ','dictionary',' ','(','e','.','g','.,',' ','<|unknown_token|>','ड','ॉ','ल','र','<|unknown_token|>',' ','vs','.',' ','<|unknown_token|>','ड','ा','ल','र','<|unknown_token|>','),',' ','but',' ','typically',' ','the',' ','matching',' ','system',' ','was',' ','made',' ','distinct',' ','enough',' ','for',' ','recognizing',' ','major',' ','languages',' ','accurately',' ','(','Tamil',',',' ','Telugu',',',' ','Kann','ada',',',' ','etc','.).','\n",
      "',' ',' ','3','.','2','.','3',' ','Number',' ','conversion','\n",
      "',' ',' ','For',' ','the',' ','number','-','to','-','text',' ','conversion',' ','we',' ','did',':','\n",
      "',' ',' ','We',' ','use',' ','a',' ','pre','-','built',' ','library',' ','(','in','dic','-','nu','m2','words',')',' ','to',' ','convert',' ','numbers',' ','into',' ','Indian',' ','languages','.','\n",
      "',' ',' ','We',' ','even',' ','built',' ','an',' ','improved',' ','nu','m2','words',' ','function',' ','incorporating',' ','the',' ','previous',' ','one',',',' ','to',' ','adjust',' ','for',' ','decimal',' ','numbers',' ','and',' ','date','-','based',' ','number','ing',' ','as',' ','well','.','\n",
      "',' ',' ','Decimal',' ','numbers',' ','just',' ','included',' ','identifying',' ','the',' ','dots',' ','attached',' ','to',' ','the',' ','numbers',' ','and',' ','then',' ','itera','ting',' ','towards',' ','the',' ','next',' ','break',',',' ','converting',' ','them',' ','into',' ','numbers',' ','dig','it',' ','by',' ','dig','it',' ','instead',' ','of',' ','the',' ','whole',' ','number',' ','at',' ','once','.','\n",
      "',' ',' ','For',' ','date','-','based',' ','number','ing',' ','it',' ','involved',' ','dealing',' ','with',' ','languages',' ','that',' ','do',' ','include',' ','them',' ','(','Hindi',',',' ','Gujarat','i',',',' ','Mara','thi',',',' ','Bengali',',',' ','Pa','nja','bi',',',' ','Ori','ya',')',' ','and',' ','that',' ','do',' ','not',' ','(','Tamil',',',' ','Telugu',',',' ','Kann','ada',',',' ','Mala','yalam',')',' ','differently','.',' ','\n",
      "',' ',' ','There',' ','was',' ','no',' ','difference',' ','in',' ','the',' ','number','-','to','-','text',' ','conversion',' ','on',' ','the',' ','second',' ','set','.','\n",
      "',' ',' ','For',' ','the',' ','first',' ','set',',',' ','it',' ','required',' ','separating',' ','the',' ','number',' ','in',' ','hundreds',' ','instead',' ','of',' ','thousands',' ','(','if',' ','the',' ','hundreds','<|unknown_token|>',' ','value',' ','was',' ','greater',' ','than',' ','0',').',' ','Ex',':',' ','1920',' ','is',' ','not',' ','<|unknown_token|>','e','k',' ','haz','aar',' ','nau',' ','so',' ','bees','<|unknown_token|>',' ','(','one',' ','thousand',' ','nine',' ','hundred',' ','and',' ','twenty',')',' ','in',' ','hindi',',',' ','but',' ','<|unknown_token|>','une','es',' ','sau',' ','bees','<|unknown_token|>',' ','(','nineteen',' ','hundred',' ','and',' ','twenty',').',' ','On',' ','the',' ','contrary',',',' ','if',' ','the',' ','hundred',' ','position','<|unknown_token|>','s',' ','value',' ','is',' ','0',',',' ','say',' ','for',' ','2020',',',' ','then',' ','the',' ','text',' ','is',' ','<|unknown_token|>','do',' ','haz','aar',' ','bees','<|unknown_token|>',' ','(','two',' ','thousand',' ','and',' ','twenty',')',' ','instead',' ','of',' ','<|unknown_token|>','bees',' ','sau',' ','bees','<|unknown_token|>','(','twenty',' ','hundred',' ','and',' ','twenty',').',' ','\n",
      "','\n",
      "',' ',' ','3','.','2','.','4',' ','Date',' ','Matching',' ','&',' ','Recognition','\n",
      "',' ',' ','In',' ','the',' ','algorithmic',' ','approach',',',' ','dates',' ','are',' ','processed',' ','slightly',' ','differently',' ','from',' ','other',' ','numer','ic',' ','or',' ','unit','-','based',' ','patterns','.',' ','Instead',' ','of',' ','combining',' ','them',' ','with',' ','ordinary',' ','numer','ic',' ','tokens',' ','directly',',',' ','we',' ','use',' ','a',' ','specialized',' ','pipeline',' ','to',' ','recognize',',',' ','interpret',',',' ','and',' ','transform',' ','date',' ','strings',' ','into',' ','a',' ','standardized',' ','textual',' ','for','mat',',',' ','appe','nding',' ','the',' ','placeholder',':$','date',' ','at',' ','the',' ','end','.',' ','Below',' ','is',' ','a',' ','detailed',' ','look',' ','at',' ','how',' ','it',' ','works',',',' ','refer','encing',' ','the',' ','relevant',' ','Python',' ','code','.','\n",
      "',' ',' ','3','.','2','.','4','.','1',' ','Core',' ','Ideas',' ','and',' ','Flow','\n",
      "',' ',' ','Regex',' ','Identification','\n",
      "',' ',' ','The',' ','code',' ','uses',' ','a',' ','regular',' ','expression',' ','to',' ','loc','ate',' ','any',' ','subst','ring',' ','that',' ','might',' ','be',' ','a',' ','date','.',' ','A',' ','typical',' ','pattern',' ','is',' ','something',' ','like',':','\n",
      "',' ',' ','This',' ','ai','ms',' ','to',' ','find',' ','up',' ','to',' ','three',' ','numer','ic',' ','parts',',',' ','possibly',' ','separated',' ','by',' ','<|unknown_token|>','-','<|unknown_token|>',',',' ','<|unknown_token|>','/','<|unknown_token|>',',',' ','<|unknown_token|>','.','<|unknown_token|>',',',' ','or',' ','<|unknown_token|>',',','<|unknown_token|>',',',' ','with',' ','optional',' ','white','space',' ','in',' ','between','.',' ','These',' ','parts',' ','can',' ','represent',':','\n",
      "',' ',' ','Two','-','part',' ','dates',' ','(','e','.','g','.,',' ','MM','/','YY',',',' ','DD','/','MM',',',' ','MM','/','DD',',',' ','etc','.).','\n",
      "',' ',' ','Three','-','part',' ','dates',' ','(','e','.','g','.,',' ','DD','/','MM','/','YY','(','YY','),',' ','YY','YY','/','MM','/','DD',',',' ','etc','.).','\n",
      "','\n",
      "','\n",
      "',' ',' ','Parser',' ','Functions','\n",
      "',' ',' ','After',' ','extracting',' ','a',' ','candidate',' ','date',' ','subst','ring',',',' ','we',' ','run',' ','it',' ','through',' ','parsing',' ','log','ic',':','\n",
      "',' ',' ','parse_','date_','parts','(','parts',',',' ','original','_','candidate',')','\n",
      "',' ',' ','Spl','its',' ','the',' ','numer','ic',' ','chunks',' ','into',' ','2',' ','or',' ','3',' ','elements',' ','and',' ','dispa','tches',' ','to',' ','either',' ','parse_','three','_part','_date',' ','or',' ','parse_','two','_part','_date','.','\n",
      "',' ',' ','parse_','three','_part','_date','(','parts',')','\n",
      "',' ',' ','Tries',' ','perm','utations',' ','like',' ','DD','/','MM','/','YY',',',' ','MM','/','DD','/','YY',',',' ','YY','YY','/','MM','/','DD',',',' ','or',' ','a',' ','fall','back',' ','of',' ','YY','/','MM','/','DD','.',' ','It',' ','ensures',' ','that',' ','each',' ','day',',',' ','month',',',' ','or',' ','year',' ','is',' ','valid',' ','(','e','.','g','.,',' ','month',' ','in',' ','1','..','12',',',' ','day',' ','≤',' ','days',' ','in',' ','that',' ','month',').','\n",
      "',' ',' ','parse_','two','_part','_date','(','parts',',',' ','original','_','candidate',')','\n",
      "',' ',' ','All','ows',' ','<|unknown_token|>','half','-','written','<|unknown_token|>',' ','forms',' ','(','e','.','g','.,',' ','05','/','2005',' ','or',' ','15','/','04',')',' ','only',' ','if',' ','the',' ','subst','ring',' ','uses',' ','/',' ','(','and',' ','not',' ','-',' ','or',' ','.',' ','since',' ','1','.','2',' ','may',' ','mean',' ','the',' ','number',' ','and',' ','1','-','2',' ','may',' ','mean',' ','something',' ','like',' ','1',' ','to',' ','2',').',' ','This',' ','function',' ','tries',' ','DD','/','MM',',',' ','then',' ','MM','/','DD',',',' ','then',' ','MM','/','YY','.',' ','The',' ','year',' ','is',' ','guessed',' ','with',' ','a',' ','short','-','year',' ','rule',' ','(<',' ','25',' ','=>',' ','2000','+','yy',',',' ','else',' ','1900','+','yy',').','\n",
      "','\n",
      "','\n",
      "',' ',' ','Validation',' ','&',' ','Format',' ','Tag','\n",
      "',' ',' ','The',' ','parser',' ','checks',' ','each',' ','combination',' ','to',' ','ensure',' ','that',' ','the',' ','day',' ','is',' ','within',' ','the',' ','maximum',' ','allowed',' ','for',' ','that',' ','month',' ','(','accounting',' ','for',' ','leap',' ','years',' ','if',' ','the',' ','year',' ','is',' ','fully',' ','known',').',' ','If',' ','no',' ','valid',' ','date',' ','structure',' ','is',' ','found',',',' ','we',' ','discard',' ','the',' ','candidate',' ','and',' ','leave',' ','it',' ','as','-','is','.','\n",
      "','\n",
      "','\n",
      "',' ',' ','Conversion',' ','to',' ','Text','ual',' ','Format','\n",
      "',' ',' ','If',' ','a',' ','valid',' ','date',' ','is',' ','recognized',',',' ','we',' ','then',':','\n",
      "',' ',' ','Possibly',' ','omit',' ','the',' ','day',' ','or',' ','year',' ','if',' ','they','<|unknown_token|>','re',' ','not',' ','present',' ','(','for',' ','half','–','written',' ','forms',').','\n",
      "',' ',' ','Use',' ','mapping',' ','from',' ','month',' ','number',' ','(','1','–','12',')',' ','to',' ','a',' ','month',' ','name',' ','in',' ','the',' ','language',' ','of',' ','choice',' ','(','e','.','g','.,',' ','in',' ','Hindi',',',' ','3',' ','->',' ','<|unknown_token|>','म','ा','र','्','च','<|unknown_token|>',').','\n",
      "',' ',' ','Attach',' ','the',' ','placeholder',':$','date',' ','at',' ','the',' ','end','.',' ','E','.','g','.,',' ','if',' ','we',' ','parse',' ','15','/','03','/','1990',',',' ','and',' ','the',' ','language',' ','is',' ','set',' ','to',' ','<|unknown_token|>','Hindi',',','<|unknown_token|>',' ','the',' ','code',' ','might',' ','transform',' ','it',' ','to',' ','<|unknown_token|>','15',' ','म','ा','र','्','च',' ','1990',':$','date','<|unknown_token|>','.','\n",
      "',' ',' ','This',' ','ensures',' ','that',' ','subsequent',' ','numer','ic',' ','expansions',' ','can',' ','handle',' ','the',' ','<|unknown_token|>','year','<|unknown_token|>',' ','portion',' ','differently',' ','(','e','.','g','.,',' ','special',' ','expansions',' ','like',' ','<|unknown_token|>','उ','न','्','न','ी','स',' ','स','ौ',' ','न','ब','्','ब','े','<|unknown_token|>',' ','instead',' ','of',' ','<|unknown_token|>','ए','क',' ','ह','ज','ा','र',' ','न','ौ',' ','स','ौ',' ','न','ब','्','ब','े','<|unknown_token|>',').','\n",
      "',' ',' ','Regex',' ','Replacement','\n",
      "',' ',' ','We',' ','use',' ','a',' ','function',':',' ','replace','_date','s_in','_text','()',' ','for',' ','conversion',' ','of',' ','the',' ','dates',' ','in',' ','the',' ','text',' ','into',' ','date',' ','tagged',' ','entities',' ','(','with',' ','recognised',' ','formats',' ','and',' ','converted',' ','month',' ','words',')','\n",
      "','\n",
      "',' ',' ','It',' ','uses',' ','a',' ','repl','acer',' ','callback',' ','that',' ','calls',' ','conver','t_dat','e_str','(...)',' ','on',' ','each',' ','matched',' ','subst','ring','.',' ','If',' ','conver','t_dat','e_str',' ','returns',' ','None',' ','(','invalid',' ','date','),',' ','we',' ','revert',' ','to',' ','the',' ','original',' ','candidate','.',' ','Otherwise',',',' ','we',' ','em','bed',' ','the',' ','textual',' ','date',' ','with',':$','date','.','\n",
      "',' ',' ','3','.','2','.','4','.','2',' ','Step','-','by','-','Step',' ','with',' ','Key',' ','Functions','\n",
      "',' ',' ','replace','_date','s_in','_text',':','\n",
      "',' ',' ','Finds',' ','potential',' ','2','-',' ','or',' ','3','-','part',' ','numer','ic',' ','com','bos','.','\n",
      "',' ',' ','For',' ','each',' ','match',':','\n",
      "',' ',' ','candidate',' ','=',' ','match','.','group','(','1',')','\n",
      "',' ',' ','converted',' ','=',' ','conver','t_dat','e_str','(','candidate',',',' ','lang',')','\n",
      "',' ',' ','If',' ','converted',' ','is',' ','not',' ','None',',',' ','we',' ','replace',' ','the',' ','original',' ','subst','ring',' ','with',' ','that',' ','textual',' ','form',' ','(','including',' ','$','date',').','\n",
      "','\n",
      "','\n",
      "',' ',' ','conver','t_dat','e_str','(','date','_str',',',' ','lang','=\"','hindi','\"):','\n",
      "',' ',' ','Cleans',' ','the',' ','string',' ','(','removing',' ','extra',' ','spaces',' ','around',' ','separator','s',').','\n",
      "',' ',' ','Spl','its',' ','on',' ','[-','/','.,','].','\n",
      "',' ',' ','Calls',' ','parse_','date_','parts','(','parts',',',' ','date','_str',').','\n",
      "',' ',' ','If',' ','parse',' ','is',' ','successful',',',' ','it',' ','yields',' ','(','day',',',' ','month',',',' ','year',',',' ','format_','tag',').','\n",
      "',' ',' ','We',' ','retrieve',' ','the',' ','month',' ','name',' ','from',' ','month','_names','[','lang','.','lower','()]',' ','or',' ','default',' ','English',' ','names','.','\n",
      "',' ',' ','Con','struct',' ','the',' ','output',' ','string',' ','as',':','\n",
      "',' ',' ','\"<','day','>',' ','<','month','_text','>',' ','<','year','>:','$','date','\",',' ','omit','ting',' ','day',' ','or',' ','year',' ','if',' ','they','<|unknown_token|>','re',' ','None','.','\n",
      "','\n",
      "','\n",
      "',' ',' ','parse_','three','_part','_date','(','parts','):','\n",
      "',' ',' ','Attempt',' ','each',' ','pattern',' ','in',' ','turn',':','\n",
      "',' ',' ','DD','/','MM','/','YY',',',' ','MM','/','DD','/','YY',',',' ','YY','YY','/','MM','/','DD',',',' ','fall','back',' ','YY','/','MM','/','DD','.','\n",
      "',' ',' ','Validate',' ','the',' ','day',',',' ','month',',',' ','year','.',' ','If',' ','found',' ','valid',',',' ','return',' ','(','day',',',' ','month',',',' ','year',',',' ',''.','..','').','\n",
      "','\n",
      "','\n",
      "',' ',' ','parse_','two','_part','_date','(','parts',',',' ','original','_','candidate','):','\n",
      "',' ',' ','Only',' ','triggers',' ','if',' ','we',' ','see',' ','/',' ','exclusively',' ','(','no',' ','-',' ','or',' ','.).','\n",
      "',' ',' ','Try',' ','DD','/','MM',' ','if',' ','the',' ','second',' ','part',' ','≤',' ','12','.','\n",
      "',' ',' ','Then',' ','MM','/','DD',' ','if',' ','the',' ','first',' ','part',' ','≤',' ','12','.','\n",
      "',' ',' ','Finally',',',' ','if',' ','those',' ','fail',',',' ','interpret',' ','it',' ','as',' ','MM','/','YY','.','\n",
      "',' ',' ','Return',' ','partial',' ','(','day',',',' ','month',',',' ','year',',',' ','...','),',' ','where',' ','some',' ','might',' ','be',' ','None','.','\n",
      "','\n",
      "','\n",
      "',' ',' ','Date',' ','Validation',':','\n",
      "',' ',' ','Helper',' ','functions',':','\n",
      "',' ',' ','is_','leap','_ye','ar','(','year',')',' ','→',' ','check',' ','if',' ','year',' ','is',' ','leap','.','\n",
      "',' ',' ','max_','day','_for_','month','(','month',',',' ','year',')',' ','→',' ','get',' ','the',' ','day','-','limit',' ','for',' ','that',' ','month',' ','(','handles',' ','February',').','\n",
      "',' ',' ','valid','_day','_for_','month','(','day',',',' ','month',',',' ','year',')',' ','→',' ','ensures',' ','day',' ','≤',' ','max',' ','allowed','.','\n",
      "',' ',' ','conver','t_y','ear_','generic','(','year','_str',')',' ','→',' ','short','-','year',' ','log','ic',' ','(','if',' ','<',' ','25',' ','=>',' ','2000',' ','+',' ','y',',',' ','else',' ','1900',' ','+',' ','y',').','\n",
      "',' ',' ','3','.','2','.','4','.','3',' ','Example',' ','Partial',' ','Conver','sions','\n",
      "',' ',' ','\"','15','/','03','/','1990','\"',' ','(','Hindi',')','\n",
      "',' ',' ','Spl','its',' ','→',' ','[\"','15','\",\"','03','\",\"','1990','\"].','\n",
      "',' ',' ','parse_','three','_part','_date',' ','tries',' ','dd','/','mm','/','yy','(','yy',')',' ','→',' ','day','=','15',',',' ','month','=','3',',',' ','year','=','1990','.',' ','Valid','.','\n",
      "',' ',' ','Becomes',' ','\"','15',' ','म','ा','र','्','च',' ','1990',':$','date','\".','\n",
      "','\n",
      "','\n",
      "',' ',' ','\"','05','/','2023','\"',' ','(','Hindi',',',' ','half','–','written',')','\n",
      "',' ',' ','Spl','its',' ','→',' ','[\"','05','\",\"','2023','\"].','\n",
      "',' ',' ','Only',' ','uses',' ','/,',' ','so',' ','it',' ','tries',' ','dd','/','mm',',',' ','mm','/','dd',',',' ','or',' ','mm','/','yy','.','\n",
      "',' ',' ','If',' ','2023',' ','is',' ','interpreted',' ','as',' ','a',' ','year',' ','via',' ','mm','/','yy',' ','log','ic',',',' ','it','<|unknown_token|>','s',' ','<|unknown_token|>','mm','=','05',',',' ','year','=','2023',',','<|unknown_token|>',' ','so',' ','day',' ','is',' ','None','.','\n",
      "',' ',' ','Final',':',' ','\"','म','ई',' ','2023',':$','date','\".','\n",
      "','\n",
      "','\n",
      "',' ',' ','\"','12','.','25','.','2003','\"','\n",
      "',' ',' ','Because',' ','.',' ','is',' ','present',',',' ','we',' ','treat',' ','it',' ','as',' ','<|unknown_token|>','three','-','part',' ','numer','ic',',','<|unknown_token|>',' ','so',' ','either',' ','DD','.','MM','.','YY','YY',' ','or',' ','MM','.','DD','.','YY','YY',' ','or',' ','fall','back','.','\n",
      "',' ',' ','If',' ','it','<|unknown_token|>','s',' ','recognized',' ','valid',',',' ','we',' ','convert',' ','month','=','25',' ','=>',' ','invalid','.',' ','This',' ','returns',' ','None',',',' ','so',' ','we',' ','revert',' ','to',' ','the',' ','original',' ','subst','ring','.','\n",
      "',' ',' ','Thus',',',' ','the',' ','date',' ','subst','ring',' ','is',' ','replaced',' ','in',' ','the',' ','text',' ','with',' ','a',' ','placeholder','-','labeled',' ','expansion',',',' ','e','.','g','.,',' ','\"','15',' ','म','ा','र','्','च',' ','1990',':$','date','\".',' ','Later',',',' ','the',' ','numer','ic',' ','expansions',' ','can',' ','see',' ','the',' ','suffix',':$','date',' ','and',' ','apply',' ','specialized',' ','year',' ','expansions',' ','(','like',' ','<|unknown_token|>','उ','न','्','न','ी','स',' ','स','ौ',' ','न','ब','्','ब','े','<|unknown_token|>',' ','vs','.',' ','<|unknown_token|>','ए','क',' ','ह','ज','ा','र',' ','न','ौ',' ','स','ौ','...','<|unknown_token|>',')',' ','in',' ','the',' ','final',' ','pass','.','\n",
      "','\n",
      "','\n",
      "',' ',' ','3','.','2','.','5',' ','Regex',' ','&',' ','Pattern',' ','Matching','\n",
      "',' ',' ','After',' ','we','<|unknown_token|>','ve',' ','identified',' ','the',' ','language',' ','(','or',' ','at',' ','least',' ','the',' ','script',' ','range',')',' ','in',' ','Section',' ','3','.','2','.','2',',',' ','the',' ','next',' ','step',' ','is',' ','to',' ','scan',' ','the',' ','text',' ','for',' ','dates',',',' ','recognize',' ','the',' ','date',' ','for','mat',',',' ','convert',' ','the',' ','month',' ','into',' ','a',' ','word',',',' ','and',' ','tag',' ','the',' ','date',' ','with',' ','<|unknown_token|>',':$','date','<|unknown_token|>',' ','placeholder','.',' ','This',' ','is',' ','then',' ','followed',' ','by',' ','the',' ','regex',' ','and',' ','pattern',' ','matching',' ','part',',',' ','where',' ','we',' ','scan',' ','recognized',' ','patterns','—','currencies',',',' ','units',',',' ','numbers',',',' ','etc','.—','and',' ','convert',' ','them',' ','into',' ','words','.',' ','This',' ','is',' ','typically',' ','accomplished',' ','by',':','\n",
      "',' ',' ','Token','izing',' ','the',' ','text',' ','into',' ','chunks',' ','(','words',',',' ','punctu','ation',',',' ','white','space',').','\n",
      "',' ',' ','Combining',' ','certain',' ','tokens',' ','that',' ','log','ically',' ','belong',' ','together',' ','(','e','.','g','.,',' ','co','mma','-','separated',' ','numbers',' ','like',' ','2',',','000',',',' ','decimal','-','separated',' ','numbers',' ','like',' ','3','.','14',',',' ','date',' ','patterns',' ','like',' ','25','/','12','/','2022',',',' ','currency',' ','tokens',' ','like',' ','₹','500',',',' ','etc','.).','\n",
      "',' ',' ','Conve','rting',' ','the',' ','units',',',' ','numbers',',',' ','dates',' ','and',' ','currency',' ','tokens',' ','into',' ','words',' ','(','numbers',' ','with',' ','the',' ','help',' ','of',' ','nu','m2','words',' ','part',')',' ','and',' ','rest',' ','with',' ','an',' ','extensive',' ','set',' ','of',' ','dictionary',' ','containing',' ','currency',' ','and',' ','unit',' ','symbols','/','short','hands',' ','mapped',' ','to',' ','words',' ','in',' ','each',' ','language','.','\n",
      "',' ',' ','Token',' ','Split','ting',' ','(','split','_string',')','\n",
      "',' ',' ','The',' ','script','<|unknown_token|>','s',' ','function',':','\n",
      "',' ',' ','Uses',' ','re','.','find','all',' ','to',' ','extract',':','\n",
      "',' ',' ','sequen','ces',' ','of',' ','word',' ','characters',',','\n",
      "',' ',' ','single',' ','non','-','word',',',' ','non','-','white','space',' ','characters',' ','(','punctu','ation',')','\n",
      "',' ',' ','spaces','.','\n",
      "',' ',' ','This',' ','ensures',' ','the',' ','text',' ','is',' ','broken',' ','into',' ','tokens',':',' ','words',',',' ','punctu','ation',',',' ','and',' ','white','space',',',' ','each',' ','captured',' ','separately','.',' ','It','<|unknown_token|>','s',' ','important',' ','because',' ','we',' ','often',' ','want',' ','to',' ','preserve',' ','exact',' ','spacing',' ','and',' ','punctu','ation',' ','when',' ','reasse','mbling',' ','the',' ','final',' ','string','.','\n",
      "',' ',' ','Combining',' ','Date',' ','Tokens',' ','(','combine','_dat','e_to','kens',')','\n",
      "',' ',' ','This',' ','function',' ','specifically',' ','looks',' ','for',' ','a',' ','pattern',' ','like',' ','[',' ',''','1995','',',' ','':','',',' ',''$','',',' ',''','date',''',' ',']',' ','and',' ','mer','ges',' ','them',' ','into',' ','a',' ','single',' ','token',':',' ','\"','1995',':$','date','\".','\n",
      "',' ',' ','Ratio','nale',':',' ','We',' ','interpret',' ',':$','date',' ','as',' ','a',' ','marker',' ','that',' ','signals',' ','a',' ','year',' ','or',' ','partial',' ','date',' ','string',' ','is',' ','truly',' ','meant',' ','to',' ','be',' ','a',' ','date',' ','that',' ','should',' ','be',' ','spelled',' ','out',' ','differently',' ','(','like',' ','for',' ','<|unknown_token|>','day',' ','month',' ','year','<|unknown_token|>',' ','expansions',' ','in',' ','our',' ','improved',' ','nu','m2','words',' ','function',').','\n",
      "',' ',' ','It',' ','iter','ates',' ','through',' ','tokens',',',' ','skip','ping',' ','white','space',',',' ','and',' ','tries',' ','to',' ','detect',' ','the',' ','three',' ','consecutive',' ','tokens',' ','\":','\",',' ','\"$','\",',' ','\"','date','\"',' ','in',' ','order','.',' ','If',' ','found',',',' ','it',' ','mer','ges',' ','them',' ','with',' ','the',' ','preceding',' ','numer','ic',' ','token','.','\n",
      "',' ',' ','This',' ','keeps',' ','the',' ','date',' ','references',' ','compact',',',' ','e','.','g','.,',' ','\"','1995',':$','date','\"',' ','or',' ','\"','02','/','05','/','1998',':$','date','\",',' ','so',' ','we',' ','can',' ','do',' ','specialized',' ','expansions',' ','later',' ','(','like',' ','<|unknown_token|>','उ','न','्','न','ी','स',' ','स','ौ',' ','प','ं','च','ा','न','व','े','<|unknown_token|>',' ','or',' ','<|unknown_token|>','द','ो',' ','म','ई',' ','उ','न','्','न','ी','स',' ','स','ौ',' ','अ','ट','्','ठ','ा','न','व','े',',','<|unknown_token|>',' ','etc','.).','\n",
      "','\n",
      "','\n",
      "',' ',' ','Combining',' ','Com','ma','-','Separat','ed',' ','Numbers',' ','(','combine','_comm','a_s','epar','ated_','numbers',')','\n",
      "',' ',' ','Often',' ','in',' ','Indian',' ','numer','ic',' ','formatting',',',' ','you',' ','see',' ','1',',','000',',',' ','2',',','50',',','000',',',' ','etc','.',' ','The',' ','code',' ','mer','ges',' ','[',' ',''','2','',',' ','',','',',' ',''','000',''',' ',']',' ','into',' ','[',' ',''','2000',''',' ','].','\n",
      "',' ',' ','This',' ','is',' ','a',' ','simple',' ','pass',' ','that',' ','checks',' ','if',' ','a',' ','token',' ','is',' ','a',' ','dig','it',',',' ','and',' ','if',' ','the',' ','subsequent',' ','tokens',' ','are',' ','<|unknown_token|>',',',' ','plus',' ','more',' ','digits','`,','<|unknown_token|>',' ','mer','ges',' ','them',' ','all',' ','into',' ','a',' ','single',' ','numer','ic',' ','token','.','\n",
      "',' ',' ','E','.','g','.:',' ','['','2','',',' ','',','',',' ',''','000','']',' ','→',' ','['','2000',''].','\n",
      "',' ',' ','Combining',' ','Dot','-','Separat','ed',' ','Numbers',' ','(','combin','e_do','t_se','para','ted_','numbers',')','\n",
      "',' ',' ','Similar',' ','to',' ','com','mas',',',' ','it',' ','mer','ges',' ','[',' ',''','3','',',' ',''.','',',' ',''','14',''',' ',']',' ','into',' ','['','3','.','14',''].','\n",
      "',' ',' ','Specifically',',',' ','it',' ','checks',' ','if',' ','a',' ','token',' ','is',' ','purely',' ','digits',' ','and',' ','if',' ','the',' ','next',' ','tokens',' ','form',' ','a',' ','<|unknown_token|>','.',' ','+',' ','digits','<|unknown_token|>',' ','pattern','.',' ','If',' ','so',',',' ','it',' ','con','caten','ates',' ','them',' ','into',' ','e','.','g','.',' ','\"','3','.','14','\"`','.','\n",
      "',' ',' ','This',' ','step',' ','helps',' ','us',' ','handle',' ','decimal','s',' ','in',' ','an',' ','earlier',' ','pass','.','\n",
      "',' ',' ','Currency',' ','Combination',' ','(','combine','_curr','ency','_to','kens',')','\n",
      "',' ',' ','Pattern',' ','A',':',' ','<','number','>',' ','<','currency',' ','token','>','\n",
      "',' ',' ','Pattern',' ','B',':',' ','<','currency',' ','token','>',' ','<','number','>','\n",
      "',' ',' ','The',' ','code',' ','checks',':','\n",
      "',' ',' ','If',' ','a',' ','token',' ','is',' ','a',' ','recognized',' ','currency',' ','symbol',' ','or',' ','ab','breviation',' ','($',',',' ','₹',',',' ','usd',',',' ','inr',',',' ','etc','.),',' ','it',' ','checks',' ','whether',' ','it','<|unknown_token|>','s',' ','followed',' ','or',' ','preceded',' ','by',' ','a',' ','dig','it',' ','token','.','\n",
      "',' ',' ','We',' ','keep',' ','a',' ','global',' ','dictionary',',',' ','e','.','x','.:','\n",
      "',' ',' ',' ',' ','1',')',' ','currency','_norm','alization','['','₹','']',' ','=',' ',''','inr',''',' ','or',' ','currency','_norm','alization','['','rs','']',' ','=',' ',''','inr',''',' ','or',' ',' ','currency','_norm','alization','['','inr','']',' ','=',' ',''','inr','',',' ','…',' ','etc','.','\n",
      "',' ',' ','2',')',' ','currency','_la','ngu','age_','mapping','['','usd','']['','hi','']',' ','=',' ','\"','ड','ॉ','ल','र','\"','\n",
      "',' ',' ','This',' ','helps',' ','us',' ','track',' ','all',' ','different',' ','currency',' ','symbols',' ','and',' ','short','hands',',',' ','from',' ','which',' ','we',' ','can',' ','convert',' ','them',' ','to',' ','words',' ','in',' ','the',' ','respective',' ','language',' ','already',' ','identified',' ','earlier','.','\n",
      "',' ',' ','The',' ','function',' ','mer','ges',' ','them',' ','into',' ','a',' ','single',' ','token',' ','with',' ','the',' ','numer','ic',' ','part',' ','plus',' ','the',' ','spelled','-','out',' ','currency',' ','(','like',' ','\"','500',' ','ड','ॉ','ल','र','\").','\n",
      "',' ',' ','If',' ','a',' ','currency',' ','token',' ','is',' ','standalone',',',' ','it',' ','might',' ','simply',' ','convert',' ','$',' ','→',' ','<|unknown_token|>','ड','ॉ','ल','र','<|unknown_token|>',' ','in',' ','the',' ','chosen',' ','language','.','\n",
      "',' ',' ','One',' ','nuance',':',' ','trailing',' ','dots',' ','are',' ','removed',' ','(','e','.','g','.,',' ','<|unknown_token|>','Rs','.','<|unknown_token|>',' ','is',' ','recognized',' ','as',' ','<|unknown_token|>','rs',',','<|unknown_token|>',' ','then',' ','mapped',' ','to',' ','<|unknown_token|>','inr',',','<|unknown_token|>',' ','etc','.).','\n",
      "',' ',' ','Because',' ','the',' ','user',' ','might',' ','place',' ','the',' ','currency',' ','either',' ','before',' ','or',' ','after',' ','the',' ','numer','ic',' ','part',',',' ','the',' ','function',' ','checks',' ','for',' ','both',' ','patterns','.','\n",
      "',' ',' ','All',' ','edge',' ','cases',' ','involving',' ','spaces',' ','and',' ','dots',' ','are',' ','checked',',',' ','for',' ','more',' ','robust','ness','.','\n",
      "',' ',' ','Uni','t',' ','Combination',' ','(','combine','_uni','t_to','kens',')','\n",
      "',' ',' ','Pattern',' ','A',':',' ','<','number','>',' ','<','unit',' ','token','>','\n",
      "',' ',' ','Pattern',' ','B',':',' ','<','unit',' ','token','>',' ','<','number','>','\n",
      "',' ',' ','The',' ','script',' ','has',' ','a',' ','big',' ','unit_','normalization',',',' ','unit_','language','_map','ping',' ','and',' ','a',' ','unit_','variants',' ','dict',',',' ','e','.','g','.:','\n",
      "',' ',' ',' ',' ','1',')',' ','unit_','variants','[\"','meter','\"]',' ','=',' ','['','m','',',' ',''','mtr','',',' ',''','mt','rs','',',' ',''','met','re','',',' ',''','meter','',',' ',''','metres','',',' ',''','meters',''],',' ','this',' ','contains',' ','short','hands',' ','and',' ','symbols',' ','(','in',' ','thousands',')',' ','for',' ','a',' ','range',' ','of',' ','different',' ','units',' ','(','more',' ','than',' ','100',').','\n",
      "',' ',' ',' ',' ',' ',' ','2',')',' ','unit_','normalization','[\"','met','re','\"]',' ','=',' ','\"','meter','\",',' ','this',' ','contains',' ','the',' ','units',' ','listed',' ','out',' ','in',' ','an',' ','exhaustive',' ','manner',',',' ','with',' ','some',' ','common',' ','alternate',' ','versions','.',' ','\n",
      "',' ',' ',' ',' ',' ',' ','3',')',' ','unit_','language','_map','ping',' ','consists',' ','of',' ','all',' ','of',' ','these',' ','units',' ','in',' ','unit_','normali','sation',' ','and',' ','maps',' ','them',' ','to',' ','different',' ','languages',' ','based',' ','on',' ','their',' ','language',' ','code','.','\n",
      "',' ',' ',' ',' ','e','.','x','.:',' ','unit_','language','_map','ping','[','<|unknown_token|>','meter','<|unknown_token|>','][','<|unknown_token|>','hi','<|unknown_token|>',']',' ','=',' ','\"','म','ी','ट','र','\"','\n",
      "',' ',' ','If',' ','we',' ','find',' ','<|unknown_token|>','50','0mg',',','<|unknown_token|>',' ','we',' ','first',' ','see',' ','the',' ','number',' ','<|unknown_token|>','500','<|unknown_token|>',' ','and',' ','the',' ','unit',' ','<|unknown_token|>','mg','.','<|unknown_token|>',' ','Once',' ','recognized',',',' ','we',' ','produce',' ','something',' ','like',' ','<|unknown_token|>','प','ा','ँ','च',' ','स','ौ',' ','म','ि','ल','ी','ग','्','र','ा','म','<|unknown_token|>',' ','in',' ','Hindi','.','\n",
      "',' ',' ','The',' ','function',' ','tries',' ','to',' ','accumulate',' ','multi','-','character',' ','units',' ','(','like',' ','<|unknown_token|>','km','/','h',',','<|unknown_token|>',' ','<|unknown_token|>','k','g',',','<|unknown_token|>',' ','<|unknown_token|>','tonnes','<|unknown_token|>',').',' ','It',' ','also',' ','accounts',' ','for',' ','possible',' ','spacing',' ','or',' ','punctu','ation',' ','(.',',',' ','etc','.)',' ','in',' ','between','.','\n",
      "',' ',' ','Because',' ','the',' ','user',' ','might',' ','place',' ','the',' ','unit',' ','either',' ','before',' ','or',' ','after',' ','the',' ','numer','ic',' ','part',',',' ','the',' ','function',' ','checks',' ','for',' ','both',' ','patterns','.',' ','If',' ','only',' ','the',' ','unit',' ','is',' ','found',',',' ','only',' ','the',' ','unit',' ','is',' ','converted',' ','to',' ','word',' ','form','.','\n",
      "',' ',' ','All',' ','edge',' ','cases',' ','involving',' ','spaces',' ','and',' ','dots',' ','are',' ','checked',' ','for',' ','more',' ','robust','ness','.','\n",
      "',' ',' ','Checking',' ','if',' ','Token',' ','is',' ','Numeric',' ','(','is_','number',')','\n",
      "',' ',' ','This',' ','is',' ','a',' ','helper',' ','that',' ','checks',' ','if',' ','the',' ','token',' ','is',' ','a',' ','number',',',' ','ensuring',' ','that',' ','if',' ','we',' ','have',' ','one',' ','(','or',' ','zero',')',' ','decimal',' ','point',' ','in',' ','the',' ','token',',',' ','it','<|unknown_token|>','s',' ','considered',' ','numer','ic','.','\n",
      "',' ',' ','If',' ','it','<|unknown_token|>','s',' ','purely',' ','digits',' ','or',' ','digits',' ','with',' ','one',' ','dot',',',' ','we',' ','treat',' ','it',' ','as',' ','a',' ','candidate',' ','number','.','\n",
      "',' ',' ','If',' ','there',' ','are',' ','any',' ','spaces',',',' ','it',' ','is',' ','not',' ','considered',' ','a',' ','number',' ','since',' ','spaces',' ','could',' ','mean',' ','that',' ','the',' ','dot',' ','is',' ','actually',' ','a',' ','full',' ','stop',',',' ','and',' ','by',' ','default',',',' ','spaces',' ','would',' ','not',' ','have',' ','been',' ','incorporated',' ','into',' ','the',' ','numerical',' ','token','.','\n",
      "',' ',' ','Main',' ','Pipeline',' ','to',' ','Convert',' ','Tokens',' ','(','convert','_number','s_to_','words',')','\n",
      "',' ',' ','Finally',' ','(','this',' ','is',' ','the',' ','function',' ','to',' ','convert',' ','the',' ','normalized',' ','sentence',' ','after',' ','date',' ','replacement',' ','(','shown',' ','later',')',' ','into',' ','words',',',' ','mostly',' ','involving',' ','numbers',',',' ','currencies',',',' ','place','holders',',',' ','and',' ','units',' ','along',' ','with',' ','simple',' ','text','):','\n",
      "',' ',' ','def',' ','convert','_number','s_to_','words','(','text',',',' ','lang','='','hi',''):','\n",
      "',' ',' ',' ',' ',' ',' ','tokens',' ','=',' ','split','_string','(','text',')','\n",
      "',' ',' ',' ',' ',' ',' ','tokens',' ','=',' ','combine','_dat','e_to','kens','(','tokens',')','\n",
      "',' ',' ',' ',' ',' ',' ','tokens',' ','=',' ','combine','_comm','a_s','epar','ated_','numbers','(','tokens',')','\n",
      "',' ',' ',' ',' ',' ',' ','tokens',' ','=',' ','combin','e_do','t_se','para','ted_','numbers','(','tokens',')','\n",
      "',' ',' ',' ',' ',' ',' ','tokens',' ','=',' ','combine','_curr','ency','_to','kens','(','tokens',',',' ','lang','=','lang',')','\n",
      "',' ',' ',' ',' ',' ',' ','tokens',' ','=',' ','combine','_uni','t_to','kens','(','tokens',',',' ','lang','=','lang',')','\n",
      "',' ',' ',' ',' ',' ',' ','...','\n",
      "',' ',' ',' ',' ',' ',' ','#',' ','second',' ','pass',' ','to',' ','expand',' ','leftover',' ','numer','ic',' ','tokens','\n",
      "','\n",
      "',' ',' ','Token','ize','.','\n",
      "',' ',' ','Combine',' ','date',' ','place','holders',',',' ','co','mma',' ','numbers',',',' ','decimal',' ','numbers',',',' ','currency',' ','tokens',',',' ','and',' ','unit',' ','tokens','.','\n",
      "',' ',' ','Expand',' ','any',' ','pure',' ','numer','ic',' ','tokens',' ','using',' ','the',' ','function',' ','improved','_num','_to_','word','(..','.).','\n",
      "',' ',' ','Re','assemble',' ','them',' ','while',' ','preserving',' ','spacing',' ','(','by',' ','rejoi','ning',' ','them',' ','carefully','—','note',' ','it',' ','uses',' ','''.','join','(','new_','tokens',')',' ','but',' ','makes',' ','sure',' ','white','space',' ','tokens',' ','remain',' ','intact',').','\n",
      "',' ',' ','This',' ','chain',' ','of',' ','transformations',' ','effectively',' ','mer','ges',' ','partial',' ','tokens',' ','into',' ','single',' ','tokens',' ','representing',' ','recognized',' ','entities',' ','(','dates',',',' ','decimal','s',',',' ','currency',' ','references',',',' ','unit',' ','references',')',' ','so',' ','they',' ','can',' ','be',' ','spelled',' ','out',' ','or',' ','processed',' ','further','.',' ','This',' ','is',' ','a',' ','post','-','processing',' ','step',' ','to',' ','the',' ','date','-','to','-','word',' ','conversion',' ','step',',',' ','which',' ','involves',' ','recognizing',' ','the',' ','date',' ','pattern',',',' ','understanding',' ','the',' ','date',' ','for','mat',',',' ','replacing',' ','the',' ','month',' ','with',' ','its',' ','word',',',' ','and',' ','attach','ing',' ','a',' ','date',' ','placeholder','.','\n",
      "',' ',' ','3','.','2','.','6',' ','Place','holder',' ','Replacement','\n",
      "',' ',' ','We',' ','store',' ','the',' ','date',' ','placeholder',':$','date',',',' ','in',' ','the',' ','date',' ','recognition',' ','part',' ','which',' ','is',' ','removed',' ','from',' ','the',' ','final',' ','converted',' ','sentence','.',' ','The',' ','concept',' ','is',' ','as',' ','follows',':','\n",
      "',' ',' ','Dates',' ','can',' ','become',',',' ','e','.','x','.:',' ','<|unknown_token|>','12',' ','May',' ','1998',':$','date',',','<|unknown_token|>',' ','then',' ','we',' ','handle',' ','them',' ','in',' ','a',' ','separate',' ','pass','.','\n",
      "',' ',' ','Alternatively',',',' ','we',' ','merge',' ','numer','ic',' ','+',' ','currency',' ','into',' ','a',' ','single',' ','token',',',' ','e','.','x','.:',' ','\"','500',' ','ड','ॉ','ल','र','\"',' ','which',' ','is',' ','effectively',' ','a',' ','<|unknown_token|>','placeholder','<|unknown_token|>',' ','for',' ','the',' ','currency',' ','expansion','.','\n",
      "',' ',' ','Where',' ','place','holders',' ','appear',':','\n",
      "',' ',' ','Specifically',' ','for',' ','dates',',',' ','we',' ','use',' ','patterns',' ','like',':$','date',' ','appe','nded',' ','to',' ','the',' ','final',' ','token',' ','in',' ','combine','_dat','e_to','kens','.',' ','Then',',',' ','in',' ','improved','_num','_to_','word','(..','.),',' ','if',' ','it',' ','sees',':$','date',',',' ','it',' ','triggers',' ','date','-','specific',' ','expansions',' ','(','like',' ','<|unknown_token|>','स','न','्',' ','1947',' ','ई','स','्','व','ी','<|unknown_token|>',' ','or',' ','<|unknown_token|>','उ','न','्','न','ी','स',' ','स','ौ',' ','स','ै','ं','त','ा','ल','ी','स','<|unknown_token|>',').','\n",
      "',' ',' ','Hence',',',' ','the',' ','placeholder',' ','mechanism',' ','is',' ','integrated',' ','into',' ','the',' ','numer','ic',' ','expansions',',',' ','ensuring',' ','a',' ','date','-','year',' ','is',' ','spelled',' ','out',' ','according',' ','to',' ','each',' ','language','<|unknown_token|>','s',' ','century',' ','rules','.',' ','If',' ','no',' ','placeholder',' ','is',' ','found',',',' ','it','<|unknown_token|>','s',' ','treated',' ','as',' ','a',' ','normal',' ','numer','ic',' ','token','.','\n",
      "',' ',' ','3','.','2','.','7',' ','Detailed',' ','Example','\n",
      "',' ',' ','Now',' ','we','<|unknown_token|>','ll',' ','walk',' ','through',' ','a',' ','complete',' ','pipeline',' ','example',' ','that',' ','incorporates',' ','date',' ','recognition',',',' ','numer','ic',' ','expansions',',',' ','currency','/','unit',' ','detection',',',' ','and',' ','rea','ssembly','.',' ','Consider',' ','the',' ','input',':','\n",
      "',' ',' ','\"','अ','न','ि','ल',' ','क','ा',' ','ज','न','्','म',' ','15','/','03','/','1990',' ','क','ो',' ','ह','ु','आ','।',' ','उ','स','न','े',' ','$','2',',','000',' ','ब','च','ा','ए',' ','थ','े',',',' ','औ','र',' ','द','ो',' ','द','ि','न',' ','ब','ा','द',' ','18','/','03','/','1990',' ','क','ो',' ','(','म','ि','त','्','र','ों',' ','स','े',' ','10','0lb',' ','उ','ध','ा','र',' ','ल','े','कर',')',' ','2','.','5km',' ','च','ल','ा','।','\"','\n",
      "','\n",
      "',' ',' ','Step',' ','1',':',' ','Date',' ','Recognition','\n",
      "',' ',' ','The',' ','function',' ','replace','_date','s_in','_text',' ','with',' ','the',' ','regex',':','\n",
      "',' ',' ','pattern',' ','=',' ','r',''(\\','d','{','1',',','4','}\\','s','*[','-/','.,',']\\','s','*\\','d','{','1',',','4','}(','?:','\\','s','*[','-/','.,',']\\','s','*\\','d','{','1',',','4','})','?)',''','\n",
      "',' ',' ','finds',':','\n",
      "',' ',' ','\"','15','/','03','/','1990','\"','\n",
      "',' ',' ','\"','18','/','03','/','1990','\"','\n",
      "','\n",
      "','\n",
      "',' ',' ','For',' ','each',' ','match',',',' ','we',' ','call',' ','conver','t_dat','e_str','.','\n",
      "',' ',' ','\"','15','/','03','/','1990','\"',' ','→',' ','parse',' ','as',' ','DD','/','MM','/','YY','YY','.',' ','day','=','15',',',' ','month','=','3',',',' ','year','=','1990',' ','→',' ','\"','15',' ','म','ा','र','्','च',' ','1990',':$','date','\".','\n",
      "',' ',' ','\"','18','/','03','/','1990','\"',' ','→',' ','day','=','18',',',' ','month','=','3',',',' ','year','=','1990',' ','→',' ','\"','18',' ','म','ा','र','्','च',' ','1990',':$','date','\".','\n",
      "',' ',' ','The',' ','text',' ','becomes',':','\n",
      "',' ',' ','\"','अ','न','ि','ल',' ','क','ा',' ','ज','न','्','म',' ','15',' ','म','ा','र','्','च',' ','1990',':$','date',' ','क','ो',' ','ह','ु','आ','।',' ','उ','स','न','े',' ','$','2',',','000',' ','ब','च','ा','ए',' ','थ','े',',',' ','औ','र',' ','द','ो',' ','द','ि','न',' ','ब','ा','द',' ','18',' ','म','ा','र','्','च',' ','1990',':$','date',' ','क','ो',' ','(','म','ि','त','्','र','ों',' ','स','े',' ','10','0lb',' ','उ','ध','ा','र',' ','ल','े','कर',')',' ','2','.','5km',' ','च','ल','ा','।','\"','\n",
      "',' ',' ','Step',' ','2',':',' ','Conve','rting',' ','Numbers',' ','and',' ','Entities','\n",
      "',' ',' ','We',' ','pass',' ','the',' ','above',' ','string',' ','into',' ','convert','_number','s_to_','words','(','text',',',' ','lang','='','hi',''),',' ','which',':','\n",
      "',' ',' ','Token','ize',':','\n",
      "',' ',' ','Spl','its',' ','on',' ','words',',',' ','punctu','ation',',',' ','white','space',' ','(','like',' ','\"','अ','न','ि','ल','\",',' ','\"',' ','\",',' ','\"','क','ा','\",',' ','\"',' ','\",',' ','etc','.).','\n",
      "','\n",
      "','\n",
      "',' ',' ','combine','_dat','e_to','kens',':','\n",
      "',' ',' ','Looks',' ','for',' ','patterns',' ','[',' ',''','1990','',',' ','':','',',' ',''$','',',' ',''','date',''',' ',']',' ','→',' ','mer','ges',' ','→',' ','[\"','1990',':$','date','\"].','\n",
      "',' ',' ','E','.','g','.,',' ','the',' ','subst','ring',' ','[',' ','\"','15','\",',' ','\"',' ','\",',' ','\"','म','ा','र','्','च','\",',' ','\"',' ','\",',' ','\"','1990','\",',' ','\":','\",',' ','\"$','\",',' ','\"','date','\"',' ',']',' ','becomes',' ','[',' ','\"','15','\",',' ','\"',' ','\",',' ','\"','म','ा','र','्','च','\",',' ','\"',' ','\",',' ','\"','1990',':$','date','\"',' ','].','\n",
      "',' ',' ','Now',' ','we',' ','have',' ','tokens',' ','like',' ','\"','15','\",',' ','\"','म','ा','र','्','च','\",',' ','\"','1990',':$','date','\",',' ','etc','.','\n",
      "','\n",
      "','\n",
      "',' ',' ','combine','_comm','a_s','epar','ated_','numbers',' ','and',' ','combin','e_do','t_se','para','ted_','numbers',':','\n",
      "',' ',' ','Mer','ges',' ','[',' ',''','2','',',' ','',','',',' ',''','000',''',' ',']',' ','into',' ','[',' ',''','2000',''',' ',']',' ','if',' ','any','.','\n",
      "',' ',' ','Mer','ges',' ','[',' ',''','2','',',' ',''.','',',' ',''','5',''',' ',']',' ','into',' ','[',' ',''','2','.','5',''',' ','].','\n",
      "',' ',' ','In',' ','our',' ','example',',',' ','$','2',',','000',' ','→',' ','tokens',' ','might',' ','be',' ','[\"','$\"',',',' ','\"','2','\",',' ','\",','\",',' ','\"','000','\"]',' ','→',' ','eventually',' ','[\"','$\"',',',' ','\"','2000','\"]','and',' ',' ','2','.','5km',' ','→',' ','[\"','2','\",',' ','\".','\",',' ','\"','5','\",',' ','\"','km','\"]',' ','→',' ','eventually',' ','[\"','2','.','5','\",',' ','\"','km','\"].','\n",
      "','\n",
      "','\n",
      "',' ',' ','combine','_curr','ency','_to','kens',':','\n",
      "',' ',' ','Se','es',' ','a',' ','pattern',' ','like',' ','[\"','$\"',',',' ','\"','2000','\"].',' ','$',' ','->',' ','\"','usd','\",',' ','but',' ','with',' ','currency','_la','ngu','age_','mapping','[\"','usd','\"]','[\"','hi','\"]',' ','=',' ','\"','ड','ॉ','ल','र','\".','\n",
      "',' ',' ','Eventually',' ','mer','ges',' ','into',' ','a',' ','single',' ','token',' ','\"','2000',' ','ड','ॉ','ल','र','\"',' ','or',' ','sets',' ','up',' ','for',' ','numer','ic',' ','expansion','.','\n",
      "','\n",
      "','\n",
      "',' ',' ','combine','_uni','t_to','kens',':','\n",
      "',' ',' ','For',' ','[',' ','\"','2','.','5','\",',' ','\"','km','\"',' ','],',' ','we',' ','check',' ','if',' ','<|unknown_token|>','km','<|unknown_token|>',' ','is',' ','recognized','.',' ','Then',' ','we',' ','produce',' ','something',' ','like',' ','\"','2','.','5',' ','क','ि','ल','ो','म','ी','ट','र','\".','\n",
      "',' ',' ','If',' ','<|unknown_token|>','lb','<|unknown_token|>',' ','is',' ','recognized',' ','→',' ',''','lb',''',' ','->',' ',''','प','ा','उ','ं','ड',''',' ','in',' ','Hindi',' ','or',' ',''','प','ा','उ','ण','्','ड',''',' ','depending',' ','on',' ','the',' ','dictionary','.','\n",
      "',' ',' ','So',' ','[\"','100','\",',' ','\"','lb','\"]',' ','might',' ','become',' ','\"','100',' ','प','ा','उ','ं','ड','\".','\n",
      "','\n",
      "','\n",
      "',' ',' ','Final',' ','Numeric',' ','Expansion',':','\n",
      "',' ',' ','If',' ','a',' ','token',' ','is',' ','\"','2','.','5','\",',' ','we',' ','call',' ','improved','_num','_to_','word','(\"','2','.','5','\",',' ','\"','hi','\")',' ','→',' ','<|unknown_token|>','द','ो',' ','द','श','म','ल','व',' ','प','ा','ँ','च','.','<|unknown_token|>','\n",
      "',' ',' ','If',' ','a',' ','token',' ','is',' ','\"','2000','\"',' ','→',' ','<|unknown_token|>','द','ो',' ','ह','ज','़','ा','र','.','<|unknown_token|>','\n",
      "',' ',' ','If',' ','a',' ','token',' ','is',' ','\"','1990',':$','date','\",',' ','improved','_num','_to_','word',' ','sees',' ',':$','date',' ','and',' ','does',' ','special',' ','date','-','year',' ','expansions',' ','(','like',' ','<|unknown_token|>','उ','न','्','न','ी','स',' ','स','ौ',' ','न','ब','्','ब','े','<|unknown_token|>',' ','in',' ','Hindi',').',' ','The',' ','presence',' ','of',' ',':$','date',' ','triggers',' ','a',' ','different',' ','rule',' ','that',' ','might',' ','handle',' ','century',' ','log','ic',' ','(','for',' ','Indo','-','Aryan',' ','languages',' ','we',' ','might',' ','do',' ','<|unknown_token|>','स','न','्',' ','...',' ','ई','स','्','व','ी','<|unknown_token|>',').','\n",
      "',' ',' ','Hence',' ','we',' ','get',' ','a',' ','final',' ','text',' ','something',' ','like',':','\n",
      "',' ',' ','\"','अ','न','ि','ल',' ','क','ा',' ','ज','न','्','म',' ','प','ं','द','्','र','ह',' ','म','ा','र','्','च',' ','उ','न','्','न','ी','स',' ','स','ौ',' ','न','ब','्','ब','े',' ','क','ो',' ','ह','ु','आ','।',' ','उ','स','न','े',' ','द','ो',' ','ह','ज','़','ा','र',' ','ड','ॉ','ल','र',' ','ब','च','ा','ए',' ','थ','े',',',' ','औ','र',' ','द','ो',' ','द','ि','न',' ','ब','ा','द',' ','अ','ठ','ा','र','ह',' ','म','ा','र','्','च',' ','उ','न','्','न','ी','स',' ','स','ौ',' ','न','ब','्','ब','े',' ','क','ो',' ','(','म','ि','त','्','र','ों',' ','स','े',' ','ए','क',' ','स','ौ',' ','प','ा','उ','ं','ड',' ','उ','ध','ा','र',' ','ल','े','कर',')',' ','द','ो',' ','द','श','म','ल','व',' ','प','ा','ँ','च',' ','क','ि','ल','ो','म','ी','ट','र',' ','च','ल','ा','।','\"','\n",
      "',' ',' ','Notice',' ','each',' ','numer','ic',' ','piece',' ','(','15',',',' ','1990',':$','date',',',' ','2000',',',' ','100',',',' ','2','.','5',')',' ','is',' ','spelled',' ','out',' ','in',' ','Hindi','.','\n",
      "',' ',' ','The',' ','month',' ','expansions',' ','(','<|unknown_token|>','म','ा','र','्','च','<|unknown_token|>',')',' ','came',' ','from',' ','date',' ','recognition',',',' ','and',' ','the',' ','year',' ','expansions',' ','used',' ','the',' ','date','-','based',' ','numer','ic',' ','log','ic',' ','with',' ',':$','date',' ','place','holders','.','\n",
      "',' ',' ','Currency',' ','$',' ','turned',' ','into',' ','<|unknown_token|>','ड','ॉ','ल','र',',','<|unknown_token|>',' ','and',' ','lb',' ','turned',' ','into',' ','<|unknown_token|>','प','ा','उ','ं','ड','.','<|unknown_token|>','\n",
      "',' ',' ','In',' ','summary',',',' ','the',' ','final',' ','pipeline',' ','for',' ','a',' ','complex',' ','input',' ','with',' ','multiple',' ','date',' ','references',' ','is',':','\n",
      "',' ',' ','replace','_date','s_in','_text',' ','to',' ','parse',',',' ','interpret',',',' ','and',' ','transform',' ','date',' ','patterns',' ','into',' ','a',' ','textual',' ','<','day','>',' ','<','month','>',' ','<','year','>:','$','date',' ','form','.','\n",
      "',' ',' ','convert','_number','s_to_','words',' ','to',' ','handle',' ','date',' ','place','holders',',',' ','numer','ic',' ','expansions',',',' ','currency',' ','expansions',',',' ','and',' ','unit',' ','expansions','.','\n",
      "',' ',' ','Re','assemble',' ','tokens',' ','carefully',' ','to',' ','preserve',' ','the',' ','original',' ','spacing',' ','and',' ','punctu','ation','.','\n",
      "',' ',' ','This',' ','fully',' ','algorithmic',' ','approach',' ','ensures',' ','no',' ','hallucin','ation',' ','while',' ','guarantee','ing',' ','correct',' ','expansions',' ','for',' ','recognized',' ','patterns','—','dates',' ','especially',',',' ','thanks',' ','to',' ','a',' ','specialized',' ','date',' ','parser',' ','and',' ','textual',' ','month','-','label','ling','.','\n",
      "',' ',' ','3','.','2','.','8',' ','Error',' ','Handling',' ','&',' ','Fall','backs','\n",
      "',' ',' ','Unknown',' ','Patterns',':',' ','If',' ','we',' ','see',' ','something',' ','like',' ','<|unknown_token|>','4','.','2c','/','s','<|unknown_token|>',' ','but',' ','<|unknown_token|>','c','/','s','<|unknown_token|>',' ','isn','<|unknown_token|>','t',' ','in',' ','the',' ','unit',' ','dictionary',',',' ','we',' ','skip',' ','or',' ','partially',' ','convert',' ','only',' ','the',' ','number','.','\n",
      "','\n",
      "','\n",
      "',' ',' ','Partial',' ','Overl','aps',':',' ','If',' ','a',' ','date',' ','also',' ','includes',' ','a',' ','currency',' ','symbol',' ','(','rare',' ','but',' ','possible',',',' ','e','.','g','.,',' ','<|unknown_token|>','12','/','03','-','1990','$','<|unknown_token|>','),',' ','the',' ','system',' ','might',' ','incorrectly',' ','parse','.',' ','Typically',',',' ','we',' ','do',' ','multiple',' ','passes',' ','or',' ','design',' ','one',' ','big',' ','combined',' ','regex',' ','to',' ','avoid',' ','collisions','.',' ','Though',' ','a',' ','very',' ','thorough',' ','and',' ','detailed',' ','approach',' ','is',' ','kept',' ','to',' ','tackle',' ','special',' ','symbols',',',' ','<|unknown_token|>','-','<|unknown_token|>',',',' ','<|unknown_token|>','$','<|unknown_token|>',',',' ','spaces',',',' ','numbers',',',' ','etc','.',' ','And',' ','a',' ','very',' ','detailed',' ','method',' ','is',' ','laid',' ','out',' ','for',' ','the',' ','procedure',' ','to',' ','go',' ','about',' ','for',' ','parsing','.',' ','Still',',',' ','in',' ','certain',' ','cases',',',' ','a',' ','rule','-','based',' ','approach',' ','may',' ','not',' ','be',' ','enough',' ','(','either',' ','due',' ','to',' ','the',' ','absence',' ','of',' ','certain',' ','rules',',',' ','not',' ','thought',' ','for',' ','(','which',' ','can',' ','be',' ','manually',' ','added',' ','for',' ','a',' ','more',' ','robust',' ','system','),',' ','or',' ','due',' ','to',' ','the',' ','semantic',' ','nature',' ','of',' ','parsing',' ','the',' ','sentence',' ','for',' ','which',' ','rules',' ','may',' ','not',' ','suffice',')','\n",
      "','\n",
      "','\n",
      "',' ',' ','Language',' ','Mi','sma','tch',':',' ','If',' ','the',' ','text',' ','is',' ','partially',' ','English',' ','or',' ','code','-','mixed',',',' ','expansions',' ','might',' ','still',' ','appear',' ','in',' ','the',' ','guessed',' ','language',' ','script','.',' ','We',' ','can',' ','either',' ','do',' ','partial',' ','expansions',' ','or',' ','fall','back','.','\n",
      "','\n",
      "','\n",
      "',' ',' ','Spa','cing',':',' ','We',' ','must',' ','ensure',' ','that',' ','after',' ','removing',' ','the',' ','pattern',' ','from',' ','the',' ','text',',',' ','we',' ','place',' ','the',' ','expanded',' ','output',' ','with',' ','correct',' ','spacing','.',' ','Some',' ','code',' ','uses',' ','sub',' ','with',' ','capturing',' ','groups',' ','to',' ','handle',' ','the',' ','spacing',' ','el','egantly',' ','(','e','.','g','.,',' ','a',' ','capturing',' ','group',' ','for',' ','leading','/','trailing',' ','spaces',').','\n",
      "',' ',' ','3','.','2','.','9',' ','Str','engths',' ','&',' ','Draw','backs',' ','Revis','ited','\n",
      "',' ',' ','Str','engths',':','\n",
      "',' ',' ','No',' ','Hall','uci','nation',':',' ','We',' ','only',' ','output',' ','expansions',' ','for',' ','recognized',' ','patterns','.','\n",
      "',' ',' ','Absolute',' ','Accu','racy',' ','for',' ','known',' ','forms',':',' ','If',' ','<|unknown_token|>','$','120','<|unknown_token|>',' ','is',' ','in',' ','the',' ','dictionary',',',' ','we',' ','do',' ','it',' ','right',' ','100','%',' ','of',' ','the',' ','time','.','\n",
      "',' ',' ','Light','weight',':',' ','No',' ','GPU',' ','or',' ','large',' ','model',' ','needed','.',' ','Typically',' ','runs',' ','in',' ','near',' ','real','-','time','.','\n",
      "','\n",
      "','\n",
      "',' ',' ','Draw','backs',':','\n",
      "',' ',' ','Coverage',':',' ','Any',' ','new',' ','for','mat',' ','or',' ','rare',' ','domain',' ','(','like',' ','<|unknown_token|>','c','/','s','<|unknown_token|>',',',' ','<|unknown_token|>','ZAR',' ','500','<|unknown_token|>',')',' ','must',' ','be',' ','manually',' ','added','.','\n",
      "',' ',' ','Maintenance',':',' ','For',' ','10',' ','languages',',',' ','each',' ','new',' ','date',' ','or',' ','currency',' ','style',' ','becomes',' ','a',' ','chunk',' ','of',' ','new',' ','code',' ','or',' ','dictionary',' ','expansions','.','\n",
      "',' ',' ','Code',' ','Swit','ching',' ','or',' ','multi','-','ling','ual',' ','sentences',' ','are',' ','not',' ','easily',' ','handled',',',' ','as',' ','each',' ','script','<|unknown_token|>','s',' ','expansions',' ','might',' ','conflict',' ','or',' ','overlap','.','\n",
      "',' ',' ','Despite',' ','these',' ','drawbacks',',',' ','many',' ','real','-','world',' ','systems',' ','rely',' ','on',' ','rule','-','based',' ','expansions',' ','where',' ','stable',',',' ','guaranteed',' ','corre','ctness',' ','is',' ','paramount','.',' ','For',' ','new',' ','or',' ','un','structured',' ','data',',',' ','an',' ','LL','M',' ','can',' ','fill',' ','coverage',' ','gaps',',',' ','but',' ','the',' ','rule','-','based',' ','method',' ','remains',' ','a',' ','strong',' ','fall','back',' ','or',' ','post','-','processor','.','\n",
      "',' ',' ','3','.','2','.','10',' ','Potential',' ','Extensions',' ','&',' ','Hybrid',' ','Appro','aches','\n",
      "',' ',' ','Hybrid',' ','Pipeline',':','\n",
      "',' ',' ','Let',' ','the',' ','LL','M',' ','generate',' ','expansions',',',' ','then',' ','parse',' ','that',' ','output',' ','with',' ','a',' ','<|unknown_token|>','checker',' ','mode','<|unknown_token|>',' ','of',' ','this',' ','rule','-','based',' ','system','.',' ','If',' ','expansions',' ','deviate',' ','from',' ','recognized',' ','patterns',',',' ','correct',' ','them','.','\n",
      "','\n",
      "','\n",
      "',' ',' ','Auto','-','Generation',' ','of',' ','Regex',':','\n",
      "',' ',' ','Some',' ','advanced',' ','systems',' ','attempt',' ','to',' ','parse',' ','user',' ','logs',' ','to',' ','auto','-','update',' ','the',' ','dictionary',' ','or',' ','patterns',' ','for',' ','new',' ','currencies','/','units','.',' ','This',' ','reduces',' ','maintenance','.','\n",
      "','\n",
      "','\n",
      "',' ',' ','Language',' ','ID',':','\n",
      "',' ',' ','For',' ','code','-','mixed',' ','text',' ','(','<|unknown_token|>','He',' ','spent',' ','$','120',' ','र','ू','प','य','े','<|unknown_token|>','),',' ','we',' ','can',' ','attempt',' ','segment','-','level',' ','detection','.',' ','e','.','g','.,',' ','if',' ','a',' ','chunk',' ','is',' ','in',' ','Dev','ana','gari',',',' ','we',' ','treat',' ','expansions',' ','in',' ','Hindi',',',' ','else',' ','in',' ','English','.',' ','This',' ','can',' ','become',' ','complex',' ','in',' ','practice','.','\n",
      "',' ',' ','3','.','2','.','11',' ','Key',' ','Take','aways','\n",
      "',' ',' ','The',' ','algorithmic',' ','system',' ','is',' ','extremely',' ','precise',' ','for',' ','in','-','distribution',',',' ','recognized',' ','patterns','.','\n",
      "',' ',' ','Multiple',' ','passes',' ','or',' ','a',' ','single',' ','mega','-','regex',' ','can',' ','capture',' ','dates',',',' ','currency',',',' ','units',',',' ','decimal',' ','expansions',',',' ','numer','ic',' ','expansions',',',' ','etc','.','\n",
      "',' ',' ','The',' ','approach',' ','requires',' ','carefully',' ','curated',' ','diction','aries',' ','(','month',' ','names',',',' ','currency',' ','expansions',',',' ','unit',' ','expansions',')',' ','for',' ','each',' ','Ind','ic',' ','language','.','\n",
      "',' ',' ','For',' ','real',' ','production',' ','usage',',',' ','pairing',' ','a',' ','rule','-','based',' ','system',' ','with',' ','an',' ','LL','M','<|unknown_token|>','s',' ','more',' ','generalized',' ','coverage',' ','can',' ','yield',' ','near','-','total',' ','reliability','.','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "',' ',' ','3','.','3',' ','Fine','-','Tun','ed',' ','LL','M',' ','Approach','\n",
      "',' ',' ','Goal',':',' ','Teach',' ','an',' ','LL','M',' ','the',' ','transformation',' ','from',' ','numer','ic',' ','forms',' ','to',' ','spelled','-','out',' ','forms',' ','by',' ','exposing',' ','it',' ','to',' ','hundreds',' ','of',' ','examples',' ','from',' ','the',' ','synthetic',' ','dataset','.','\n",
      "',' ',' ','3','.','3','.','1',' ','Base',' ','Model','\n",
      "',' ',' ','We',' ','used',' ','<|unknown_token|>','uns','lo','th','/','Meta','-','Lla','ma','-','3','.','1','-','8B','<|unknown_token|>',' ','in',' ','4','-','bit',' ','quantiz','ation','.',' ','It','<|unknown_token|>','s',' ','large',' ','enough',' ','to',' ','handle',' ','multi','-','ling','ual',' ','tasks',' ','dec','ently',' ','but',' ','still',' ','possible',' ','to',' ','fine','-','tune',' ','in',' ','~','21','GB',' ','VR','AM',' ','(','with',' ','24',' ','batch',' ','size',').',' ','The',' ','quantiz','ation',' ','(','bits','andb','ytes',' ','4','-','bit',')',' ','ensures',' ','minimal',' ','memory',' ','usage','.',' ','Memory',' ','required',' ','for',' ','model',' ','inference',' ','is',' ','<','6GB',' ','on',' ','GPU','.','\n",
      "',' ',' ','3','.','3','.','2',' ','Data',' ','&',' ','Prompt',' ','Format','\n",
      "',' ',' ','We',' ','fe','d',' ','each',' ','training',' ','example',' ','in',' ','the',' ','prompt','.',' ','So',' ','that',' ','the',' ','model',' ','can',' ','easily',' ','understand',' ','what',' ','is',' ','to',' ','be',' ','done',' ','and',' ','can',' ','train',' ','understanding',' ','the',' ','information','.',' ','The',' ','sample',' ','prompt',' ','can',' ','be',' ','found',' ','in',' ','the',' ','fine','tuning',' ','prompt',' ','t','ab','.','\n",
      "',' ',' ','We',' ','included',' ','a',' ','single',' ','example',' ','in',' ','the',' ','prompt',' ','heading',' ','as',' ','well',' ','(','<|unknown_token|>','15','/','03','/','1990',' ','क','ो','…','<|unknown_token|>',').',' ','The',' ','training',' ','loop',' ','sees',' ','1',',','185',' ','of',' ','these',' ','examples',',',' ','each',' ','in',' ','multiple',' ','epochs','.',' ','The',' ','eval',' ','set',' ','is',' ','415',' ','examples','.','\n",
      "',' ',' ','Here','<|unknown_token|>','s',' ','an',' ','updated',' ','version',' ','of',' ','the',' ','Hyper','parameters',' ','&',' ','Training',' ','Cost',' ','section',',',' ','clarif','ying',' ','that',' ','the',' ','so','-','called',' ','<|unknown_token|>','crashed','<|unknown_token|>',' ','run',' ','wasn','<|unknown_token|>','t',' ','forcibly',' ','stopped',' ','but',' ','did',',',' ','in',' ','fact',',',' ','yield',' ','surprisingly',' ','good',' ','(','though',' ','unstable',')',' ','results',':','\n",
      "',' ',' ','3','.','3','.','3',' ','Hyper','parameters',' ','&',' ','Tun','ing',' ','Runs','\n",
      "',' ',' ','We',' ','conducted',' ','multiple',' ','major',' ','training',' ','runs',',',' ','tuning',' ','key',' ','hyper','parameters',' ','such',' ','as',' ','learning',' ','rate',',',' ','weight',' ','decay',',',' ','war','mup',' ','steps',',',' ','and',' ','LR',' ','schedule','rs','.',' ','Below',' ','is',' ','a',' ','breakdown',' ','of',' ','how',' ','those',' ','runs',' ','evolved',' ','and',' ','why','.','\n",
      "',' ',' ','Common',' ','Settings',' ','Across',' ','Runs','\n",
      "',' ',' ','Comp','ute',' ','Metrics',':',' ','We',' ','monitored',' ','Training',' ','Loss',',',' ','Evaluation',' ','Loss',' ','(','primary','),',' ','plus',' ','CH','RF',',',' ','CH','RF','++',',',' ','BL','EU',',',' ','WER',',',' ','CER',' ','on',' ','the',' ','eval',' ','set','.','\n",
      "',' ',' ','Optim','izer',':',' ','Ada','mW',' ','(','8','-','bit',')',' ','for',' ','reduced',' ','memory',' ','usage','.','\n",
      "',' ',' ','Max',' ','Sequence',' ','Length',':',' ','2048',' ','(','though',' ','typical',' ','input',' ','lengths',' ','rarely',' ','exceed','ed',' ','~','594',' ','tokens',').','\n",
      "',' ',' ','Packing',':',' ','False',' ','(','no',' ','multi','-','sample',' ','packing',').','\n",
      "',' ',' ','dataset','_num','_pro','cess',':',' ','2',' ','(','minor',' ','parallel','ism',').','\n",
      "',' ',' ','Per','-','Device',' ','Train',' ','Batch',' ','Size',':',' ','Usually',' ','16',' ','early',' ','on',',',' ','and',' ','then',' ','increased',' ','to',' ','24',' ','in',' ','later',' ','runs',' ','when',' ','we',' ','realized',' ','we',' ','had',' ','leftover',' ','VR','AM','.','\n",
      "',' ',' ','Gradient',' ','Accu','mulation',':',' ','With',' ','gradient',' ','accumulation',' ','of',' ','4',',',' ','the',' ','effective',' ','batch',' ','size',' ','is',' ','16',' ','*',' ','4',' ','=',' ','64',' ','or',' ','24',' ','*',' ','4',' ','=',' ','96','.','\n",
      "',' ',' ','Epo','chs',':',' ','Initially',' ','up',' ','to',' ','10',',',' ','but',' ','often',' ','set',' ','to',' ','7',' ','or',' ','8',' ','in',' ','later',' ','runs',' ','to',' ','save',' ','time',' ','and',' ','refine','.','\n",
      "',' ',' ','Precision',':',' ','bf','16',' ','=',' ','True',' ','(','with',' ','fp','16',' ','=',' ','False',')',' ','for',' ','improved',' ','numerical',' ','stability','.','\n",
      "',' ',' ','Seed',':',' ','34','07',' ','for',' ','reproduci','bility','.','\n",
      "',' ',' ','Logging',' ','Steps',':',' ','1',' ','(','frequent',' ','logging',').','\n",
      "',' ',' ','Eva','l',' ','Steps',':',' ','Typically',' ','2',',',' ','changed',' ','to',' ','4',' ','in',' ','some',' ','runs',' ','for',' ','time',' ','savings','.','\n",
      "',' ',' ','Load',' ','Best',' ','Model',' ','at',' ','End',':',' ','True',',',' ','ensuring',' ','the',' ','best',' ','eva','l_l','oss',' ','checkpoint',' ','is',' ','relo','aded','.','\n",
      "',' ',' ','Save',' ','Strategy',':',' ','By',' ','steps',',',' ','typically',' ','save_','steps',' ','=',' ','4','.','\n",
      "',' ',' ','Metric',' ','for',' ','Best',' ','Model',':',' ','eva','l_l','oss',' ','(','lower',' ','is',' ','better',').','\n",
      "',' ',' ','Note',':',' ','Another',' ','custom',' ','metric',' ','was',' ','used',' ','to',' ','check',' ','the',' ','model',' ','performance',' ','(','based',' ','on',' ','eval',' ','and',' ','train',' ','loss','):',' ','cut','om_','metric',' ','(','squa','red_','eval','_to_','train_','loss','_ra','ti','o',' ','=',' ','eva','l_l','os','s2','/','train_','loss','):',' ','0','.','13','12','.',' ','It',' ','minim','izes',' ','both',' ','the',' ','eva','l_l','oss',' ','and',' ','the',' ','ra','ti','o',' ','of',' ','eval',' ','to',' ','train',' ','loss',' ','(','sign','ifying',' ','ove','rfit',').',' ','this',' ','mostly',' ','matches',' ','the',' ','best',' ','performance',' ','across',' ','metrics',' ','(','thus',',',' ','this',' ','metric',',',' ','when',' ','good',',',' ','is',' ','often',' ','when',' ','all',' ','the',' ','other',' ','given',' ','metrics',' ','are',' ','in',' ','their',' ','best',' ','spots',').',' ','found',' ','to',' ','be',' ','consistent',' ','from',' ','experiment','ation',' ','across',' ','46',' ','training',' ','runs','.','\n",
      "',' ',' ','One',' ','major',' ','drawback',' ','of',' ','this',' ','is',' ','that',' ','it',' ','often',' ','goes',' ','wrong',' ','on',' ','sudden',' ','peaks',' ','in',' ','train',' ','loss','.','\n",
      "',' ',' ','an',' ','improvement',' ','is',' ','to',' ','use',' ','(','eva','l_l','os','s2',')/(','min','(','train_','los','sj',' ','))',' ','for',' ','j',' ','ranging',' ','from',' ','1',' ','to',' ','i','.',' ','This',' ','is',' ','often',' ','a',' ','better',' ','estimate','.',' ','here',' ','eva','l_l','os','sx',' ','and',' ','train_','los','sx',' ','sign','ifies',' ','the',' ','respective',' ','losses',' ','at',' ','step',' ','=',' ','x','.','\n",
      "',' ',' ','An',' ','even',' ','better',' ','estimate',' ','ranges',' ','from',' ','max','(','0',',',' ','i','-','k',')',' ','to',' ','i','.',' ','where',' ','k',' ','is',' ','a',' ','hyper','-','parameter',' ','decided',' ','by',' ','the',' ','user',' ','based',' ','on',' ','the',' ','volatility',' ','of',' ','the',' ','training',' ','run',' ','and',' ','the',' ','number',' ','of',' ','total',' ','steps','.','\n",
      "',' ',' ','Individual',' ','Runs','\n",
      "',' ',' ','Orange',' ','is',' ','<|unknown_token|>','sar','vam','_trai','ning','_run','_ma','in2','<|unknown_token|>',' ','and',' ','Yellow',' ','is',' ','<|unknown_token|>','sar','vam','_trai','ning','_run','_ma','in1','<|unknown_token|>','.','\n",
      "',' ',' ','Below',' ','is',' ','an',' ','overview',' ','of',' ','the',' ','major',' ','runs',' ','(','numbered',' ','1',' ','through',' ','5','),',' ','plus',' ','notes',' ','on',' ','a',' ','<|unknown_token|>','crashed','<|unknown_token|>',' ','run',' ','that',' ','ended',' ','unexpectedly',' ','but',' ','still',' ','showed',' ','strong',' ','partial',' ','performance','.','\n",
      "',' ',' ','Run',' ','1','\n",
      "',' ',' ','Learning',' ','Rate',':',' ','2e','-','4','\n",
      "',' ',' ','War','mup',' ','Steps',':',' ','20','\n",
      "',' ',' ','Weight',' ','Dec','ay',':',' ','0','.','0','1','\n",
      "',' ',' ','LR',' ','Scheduler',' ','Type',':',' ','Linear','\n",
      "',' ',' ','Ratio','nale',':','\n",
      "',' ',' ','A',' ','moderate',' ','LR',' ','(','2e','-','4',')',' ','with',' ','amp','le',' ','war','mup',' ','(','20',' ','steps',')',' ','and',' ','linear',' ','decay','—','intended',' ','to',' ','prevent',' ','early',' ','instability','.',' ','This',' ','was',' ','the',' ','baseline',' ','for',' ','comparison','.','\n",
      "',' ',' ','Result',':','\n",
      "',' ',' ','Training',' ','was',' ','stable',' ','but',' ','somewhat',' ','slow',' ','to',' ','converge','.','\n",
      "',' ',' ','Ind','icated',' ','that',' ','we',' ','could',' ','push',' ','the',' ','LR',' ','higher',' ','to',' ','speed',' ','up',' ','improvement','.','\n",
      "',' ',' ','Run',' ','2','\n",
      "',' ',' ','Learning',' ','Rate',':',' ','4e','-','4',' ','(','doubling',' ','from',' ','ru','n1',')','\n",
      "',' ',' ','War','mup',' ','Steps',':',' ','20','\n",
      "',' ',' ','Weight',' ','Dec','ay',':',' ','0','.','0','1','\n",
      "',' ',' ','LR',' ','Scheduler',' ','Type',':',' ','Linear','\n",
      "',' ',' ','max_','grad','_norm',':',' ','1','.','0',' ','(','to',' ','prevent',' ','exploding',' ','gradients',')','\n",
      "',' ',' ','data','load','er_p','in_','memory',':',' ','True',' ','(','faster',' ','GPU',' ','transfer',')','\n",
      "',' ',' ','Ratio','nale',':','\n",
      "',' ',' ','Since',' ','ru','n1',' ','was',' ','stable',' ','but',' ','slow',',',' ','we',' ','doubled',' ','LR',' ','to',' ','4e','-','4','.',' ','We',' ','kept',' ','war','mup',' ','at',' ','20',' ','steps',' ','for',' ','a',' ','controlled',' ','slope',' ','and',' ','introduced',' ','grad',' ','no','rm',' ','clipping',' ','to',' ','safeguard',' ','training','.','\n",
      "','\n",
      "',' ',' ','Result',':','\n",
      "',' ',' ','Qui','cker',' ','convergence',' ','than',' ','ru','n1',' ','without',' ','major',' ','insta','bilities','.','\n",
      "',' ',' ','Reason','able',' ','final',' ','losses',',',' ','but',' ','we',' ','still',' ','suspected',' ','we',' ','could',' ','push',' ','LR',' ','even',' ','more','.','\n",
      "',' ',' ','Run',' ','3',' ','(','Crash','ed',',',' ','but',' ','Produced',' ','Good',' ','Results',')','\n",
      "',' ',' ','Learning',' ','Rate',':',' ','4e','-','4','\n",
      "',' ',' ','War','mup',' ','Steps',':',' ','5',' ','(','much',' ','lower',')','\n",
      "',' ',' ','Weight',' ','Dec','ay',':',' ','0','.','0','1','\n",
      "',' ',' ','LR',' ','Scheduler',':',' ','Cos','ine','\n",
      "',' ',' ','Ratio','nale',':','\n",
      "',' ',' ','Here',',',' ','we',' ','tested',' ','a',' ','short',' ','war','mup',' ','to',' ','ramp',' ','LR',' ','up',' ','very',' ','quickly',',',' ','plus',' ','a',' ','cosi','ne',' ','schedule',' ','for',' ','a',' ','smoother',' ','later','-','phase',' ','decay','.',' ','The',' ','plan',' ','was',' ','to',' ','accelerate',' ','early',' ','learning','.','\n",
      "',' ',' ','Result',':','\n",
      "',' ',' ','The',' ','training',' ','ended',' ','unexpectedly',' ','early',' ','(','<|unknown_token|>','crashed','<|unknown_token|>','),',' ','presumably',' ','due',' ','to',' ','an',' ','abrupt',' ','LR',' ','ramp',' ','plus',' ','not',' ','enough',' ','war','mup',' ','steps','.','\n",
      "',' ',' ','Partial',' ','logs',' ','showed',' ','performance','/','metrics',' ','that',' ','were',' ','too',' ','good',' ','before',' ','it',' ','crashed',',',' ','suggesting',' ','the',' ','model',' ','was',' ','learning',' ','quickly',' ','but',' ','on',' ','the',' ','edge',' ','of',' ','stability','.','\n",
      "',' ',' ','On',' ','retr','ying',' ','the',' ','run',',',' ','results',' ','were',' ','produced',' ','that',' ','were',' ','good',' ','but',' ','not',' ','as',' ','good',' ','as',' ','the',' ','crashed',' ','version','.','\n",
      "',' ',' ','Run',' ','3',' ','(','New',' ','Variation',' ','with',' ','sl','ight',' ','changes',' ','from',' ','previous',')','\n",
      "',' ',' ','Learning',' ','Rate',':',' ','1','.','5e','-','3',' ','(','much',' ','higher',' ','than',' ','4e','-','4',')','\n",
      "',' ',' ','War','mup',' ','Steps',':',' ','7',' ','(','a',' ','bit',' ','more',' ','than',' ','5',',',' ','but',' ','still',' ','short',')','\n",
      "',' ',' ','Weight',' ','Dec','ay',':',' ','0','.','03','\n",
      "',' ',' ','LR',' ','Scheduler',':',' ','Cos','ine','\n",
      "',' ',' ','Ratio','nale',':','\n",
      "',' ',' ','Post','-','<|unknown_token|>','crash',',','<|unknown_token|>',' ','we',' ','tried',' ','an',' ','even',' ','higher',' ','LR',' ','but',' ','compensated',' ','with',' ','more',' ','weight',' ','decay',' ','(','0','.','03',')',' ','to',' ','rein',' ','in',' ','potential',' ','overfi','tting',' ','and',' ','partial',' ','gradient',' ','explosion','.',' ','We',' ','also',' ','used',' ','7',' ','war','mup',' ','steps',',',' ','since',' ','the',' ','l','r',' ','would',' ','increase',' ','quite',' ','fast',',',' ','and',' ','to',' ','compensate',' ','for',' ','it','.','\n",
      "',' ',' ','Result',':','\n",
      "',' ',' ','Very',' ','fast',' ','convergence',' ','and',' ','decent',' ','final',' ','results',',',' ','but',' ','with',' ','spikes',' ','in',' ','the',' ','loss',' ','curve','.','\n",
      "',' ',' ','Still',' ','could',' ','not',' ','match',' ','the',' ','results',' ','from',' ','the',' ','<|unknown_token|>','lucky','<|unknown_token|>',' ','crash',' ','run','.','\n",
      "',' ',' ','Run',' ','4','\n",
      "',' ',' ','Learning',' ','Rate',':',' ','2e','-','3',' ','(','increased',' ','further',')','\n",
      "',' ',' ','Weight',' ','Dec','ay',':',' ','0','.','001',' ','(','significantly',' ','lower',')','\n",
      "',' ',' ','War','mup',' ','Steps',':',' ','10','\n",
      "',' ',' ','LR',' ','Scheduler',':',' ','Cos','ine',' ','with',' ','Rest','arts','\n",
      "',' ',' ','Epoch',':',' ','8','\n",
      "',' ',' ','Ratio','nale',':','\n",
      "',' ',' ','From',' ','experience',',',' ','a',' ','high',' ','LR',' ','can',' ','accelerate',' ','initial',' ','learning',',',' ','but',' ','you',' ','risk',' ','over','shoot','.',' ','Cos','ine',' ','with',' ','rest','arts',' ','rebo','ots',' ','the',' ','LR',' ','periodically',',',' ','allowing',' ','re','-','convergence',' ','if',' ','the',' ','model',' ','starts',' ','overfi','tting',' ','or',' ','flatte','ning',' ','out','.',' ','The',' ','lower',' ','weight',' ','decay',' ','(','0','.','001',')',' ','counters',' ','the',' ','prior',' ','run','<|unknown_token|>','s',' ','heavy',' ','penalty',' ','(','0','.','03','),',' ','which',' ','showed',' ','to',' ','be',' ','experiment','ally',' ','worse','.','\n",
      "',' ',' ','Result',':','\n",
      "',' ',' ','Rapid',' ','initial',' ','improvement','.','\n",
      "',' ',' ','8',' ','epochs',' ','gave',' ','the',' ','system',',',' ','a',' ','shorter',' ','train',' ','time',',',' ','and',' ','a',' ','faster',' ','learning',' ','rate',' ','degradation',' ','by',' ','cosi','ne',' ','curve','.','\n",
      "',' ',' ','Some',' ','over','shoot',' ','at',' ','earlier',' ','epochs',',',' ','but',' ','it',' ','typically',' ','settled',' ','near',' ','the',' ','end','.','\n",
      "',' ',' ','Results',' ','similar',' ','to',' ','that',' ','of',' ','the',' ','crashed',' ','run','\n",
      "',' ',' ','More',' ','unstable',' ','training',' ','run',' ','with',' ','more',' ','peaks',',',' ','and',' ','a',' ','lower',' ','eval',' ','loss',' ','to',' ','due',' ','overfi','tting',' ','later',' ','on',' ','in',' ','the',' ','run','.','\n",
      "',' ',' ','Run',' ','5','\n",
      "',' ',' ','Learning',' ','Rate',':',' ','1','.','6e','-','3','\n",
      "',' ',' ','Weight',' ','Dec','ay',':',' ','0','.','0','05','\n",
      "',' ',' ','War','mup',' ','Steps',':',' ','8','\n",
      "',' ',' ','LR',' ','Scheduler',':',' ','Cos','ine',' ','with',' ','Rest','arts','\n",
      "',' ',' ','Epoch',':',' ','7','\n",
      "',' ',' ','Ratio','nale',':','\n",
      "',' ',' ','Refin','ing',' ','from',' ','ru','n4',':','\n",
      "',' ',' ','Slight','ly',' ','lower',' ','LR',' ','(','1','.','6e','-','3',')',' ','for',' ','more',' ','stability','.','\n",
      "',' ',' ','Weight',' ','decay',' ','at',' ','0','.','0','05',',',' ','balancing',' ','the',' ','extr','emes',' ','of',' ','0','.','03',' ','vs','.',' ','0','.','001',',',' ','preventing',' ','overfi','tting','.','\n",
      "',' ',' ','Lower',' ','number',' ','of',' ','war','mup',' ','steps',',',' ','for',' ','a',' ','faster',' ','rise',' ','(','required',' ','early',')',' ','and',' ','earlier',' ','degradation',' ','start',' ','off',' ','(','for',' ','later',' ','stability',')',' ','of',' ','the',' ','learning',' ','rate','.','\n",
      "',' ',' ','7',' ','epochs',' ','to',' ','keep',' ','the',' ','total',' ','time',' ','short',' ','and',' ','further',' ','the',' ','effect',' ','of',' ','a',' ','faster',' ','declining',' ','learning',' ','rate',' ','on',' ','model',' ','ove','rfit',' ','and',' ','training',' ','stability','.','\n",
      "',' ',' ','Result',':','\n",
      "',' ',' ','One',' ','of',' ','our',' ','best',' ','overall',' ','runs',',',' ','with',' ','high',' ','performance',' ','across',' ','all',' ','metrics',',',' ','better',' ','stability',' ','and',' ','a',' ','lower',' ','ove','rfit','.','\n",
      "',' ',' ','Had',' ','a',' ','better',' ','performance',' ','(','arguably',')',' ','than',' ','the',' ','crashed',' ','3rd',' ','run','.','\n",
      "',' ',' ','Found',' ','to',' ','be',' ','reproduc','ible',' ','in',' ','the',' ','reproduci','bility',' ','runs','\n",
      "',' ',' ','Chosen',' ','as',' ','our',' ','main',' ','final',' ','model',' ','for',' ','subsequent',' ','usage',' ','(','GG','UF',' ','conversion',',',' ','inference',' ','tests',',',' ','predictions',',',' ','etc',').','\n",
      "','\n",
      "',' ',' ','Potential',' ','Future',' ','Tun','ing','\n",
      "',' ',' ','Sho','rter',' ','Runs',':',' ','Running',' ','only',' ','2','–','3',' ','epochs',' ','if',' ','we',' ','want',' ','a',' ','quick',' ','improvement',' ','over',' ','the',' ','base',',',' ','and',' ','a',' ','way',' ','faster',' ','l','r',' ','decline',' ','for',' ','a',' ','way',' ','more',' ','stable',' ','run','.','\n",
      "',' ',' ','Ric','her',' ','Data',':',' ','If',' ','the',' ','dataset',' ','grows',' ','to',' ','~','10k',' ','examples',',',' ','we',' ','might',' ','need',' ','even',' ','more',' ','epochs',' ','or',' ','refined',' ','schedules','.','\n",
      "',' ',' ','3','.','3','.','4',' ','Training',' ','Time',' ','&',' ','Cost','\n",
      "',' ',' ','All',' ','these',' ','experiments',' ','were',' ','done',' ','on',' ','an',' ','Nvidia',' ','L4',' ','GPU','.',' ','Notable',' ','points',':','\n",
      "',' ',' ','Steps','/','Epoch',':',' ','~','84',',',' ','given',' ','an',' ','effective',' ','batch',' ','size',' ','of',' ','96',' ','(','24',' ','per',' ','device',' ','×',' ','4',' ','gradient',' ','accumulation',').','\n",
      "',' ',' ','Time','/','Epoch',':',' ','~','2',' ','hr',' ','20',' ','min',' ','(','140',' ','minutes','),',' ','factoring',' ','in',' ','both',' ','forward','/','backward',' ','passes',' ','and',' ','partial',' ','evaluation','.','\n",
      "',' ',' ','Eva','l',' ','Steps',':',' ','~','42',' ','each',' ','epoch',',',' ','taking',' ','~','89',' ','seconds',' ','each',' ','→',' ','~','62',' ','min',' ','total',' ','for',' ','eval','.','\n",
      "',' ',' ','Total',' ','for',' ','7',' ','Epo','chs',':',' ','~','202',' ','minutes',' ','(~','3',' ','hr',' ','22',' ','min',')','\n",
      "',' ',' ','For',' ','20',' ','step',' ','(','best',' ','step',' ','before',' ','ove','rfit',' ','begins',',',' ','by',' ','custom',' ','metric','):',' ','~','20',' ','*',' ','140',' ','/',' ','84',' ','→',' ','~',' ','33',' ','minutes',' ','and',' ','20','/','2',' ','=',' ','10',' ','eval',' ','steps',' ','~',' ','10',' ','*',' ','89',' ','→',' ','~','15',' ','min',' ','for',' ','eval','.',' ','Thus',' ','a',' ','total',' ','runtime',' ','of',' ','~',' ','48',' ','minutes','.','\n",
      "',' ',' ','Detailed',' ','Cost',' ','Calculation','\n",
      "',' ',' ','Assuming',' ','the',' ','platform','<|unknown_token|>','s',' ','billing',' ','model',' ','uses',' ','<|unknown_token|>','units','<|unknown_token|>',':','\n",
      "',' ',' ','Cost',' ','of',' ','L4',':',' ','2','.','4',' ','units','/','hour',' ','=',' ','0','.','04',' ','unit','/','minute','.','\n",
      "',' ',' ','1',' ','Uni','t',' ','=',' ','$','10','/','100',' ','=',' ','10',' ','cents','.','\n",
      "',' ',' ','7','-','Epoch',' ','Training',' ','(~','202',' ','minutes','):','\n",
      "',' ',' ','202',' ','min',' ','×',' ','0','.','04',' ','unit','/','min',' ','×',' ','10',' ','cent','/','unit',' ','=',' ','$','0','.','808','\n",
      "',' ',' ','~',' ','Rs','.',' ','70',' ','(','if',' ','$','1',' ','~',' ','Rs','.',' ','86','.','5',').','\n",
      "',' ',' ','Model',' ','Check','point',' ','(~','48',' ','min',' ','to',' ','get',' ','best',' ','partial',' ','step','):','\n",
      "',' ',' ','48',' ','min',' ','×',' ','0','.','04',' ','×',' ','0','.','1',' ','=',' ','$','0','.','192',' ','(~','Rs','.',' ','16','.','6',').','\n",
      "',' ',' ','Total',' ','for',' ','multiple',' ','runs',' ','or',' ','additional',' ','trials',' ','(','total',' ','5',' ','major',' ','runs',' ','+',' ','1',' ','reproduci','bility',' ','run',' ','+',' ','1',' ','crashed',' ','run',' ','+',' ','39',' ','other',' ','runs',' ','=',' ','46',' ','runs','):',' ','~$','9','.','6',' ','(~','Rs','.',' ','830','.','4',')',' ','if',' ','we',' ','sum',' ','up',' ','extended',' ','experiments','.','\n",
      "',' ',' ','Resource',' ','Stand','point',':','\n",
      "',' ',' ','The',' ','model',' ','loads',' ','in',' ','~','5','–','6GB',' ','of',' ','RAM',' ','and',' ','handles',' ','short',' ','inference',' ','requests',' ','at',' ','minimal',' ','overhead','.','\n",
      "',' ',' ','After',' ','training',',',' ','we',' ','merged',' ','the',' ','Lo','RA',' ','adapter',' ','into',' ','the',' ','base',' ','model',' ','and',' ','quant','ized',' ','to',' ','Q4','_K','_M',' ','(','GG','UF',').','\n",
      "',' ',' ','We',' ','can',' ','deploy',' ','on',' ','CPU',' ','via',' ','ll','ama','.','cpp',',',' ','incur','ring',' ','no',' ','monthly',' ','GPU',' ','hosting',' ','cost',' ','on',' ','a',' ','free',' ','Hug','ging',' ','Fa','ce',' ','Space','.',' ','The',' ','model',' ','loads',' ','in',' ','~','2GB',' ','of',' ','RAM',' ','(','without',' ','–','mlo','ck',',',' ','which',' ','forces',' ','off','loading',' ','to',' ','stop',',',' ','increasing',' ','inference',' ','speeds',')',' ','due',' ','to',' ','storage',' ','off','loading',' ','and',' ','memory',' ','mapping','.',' ','This',' ','approach',' ','mar','ries',' ','cheap',' ','GPU','-','based',' ','training',' ','with',' ','free',' ','CPU','-','based',' ','inference','.','\n",
      "',' ',' ','System',' ','Setup',' ','time',' ','-',' ','initially',',',' ','model',' ','download',',',' ','running',' ','.','sh',' ','files',',',' ','etc','.,',' ','~','6','-','7',' ','minutes',' ','of',' ','initial',' ','setup',' ','time',' ','with',' ','<',' ','15',' ','seconds',' ','of',' ','model',' ','load',' ','time',' ','on',' ','refresh',' ','(','model',' ','load',' ','time',' ','and',' ','running',' ','the',' ','first',' ','prompt',' ','for',' ','caching',' ','in',' ','RAM',').',' ','Hug','ging',' ','face',' ','stops',' ','the',' ','face',' ','on',' ','48',' ','hours',' ','of',' ','no',' ','usage',',',' ','then',' ','the',' ','full',' ','setup',' ','is',' ','again',' ','required',' ','on',' ','loading','.',' ','Response',' ','time',' ','during',' ','inference',' ','is',' ','drastically',' ','reduced',' ','due',' ','to',' ','prompt',' ','caching',' ','and',' ','multi','-','threading','.',' ','(','However',' ','in',' ','the',' ','spaces',' ','only',' ','two',' ','cor','es',' ','are',' ','present',' ','and',' ','thus',' ','it',' ','is',' ','recommended',' ','to',' ','host',' ','on',' ','higher',' ','core',' ','systems',',',' ','modern',' ','systems',' ','with',' ','8',' ','or',' ','16',' ','cor','es',',',' ','would',' ','give',' ','much',' ','faster',' ','results',')','\n",
      "',' ',' ','3','.','3','.','5',' ','Results',' ','&',' ','Potential',' ','Issues','\n",
      "',' ',' ','Accu','racy',':','\n",
      "',' ',' ','For',' ','typical',' ','patterns',' ','(','DD','/','MM','/','YY','YY',',',' ','$','120',',',' ','50','0mg','),',' ','near',' ','absolute',' ','corre','ctness','.','\n",
      "',' ',' ','Model',' ','Metrics',' ','at',' ','checkpoint',':','\n",
      "',' ',' ','train_','loss',':',' ','0','.','101','\n",
      "',' ',' ','eva','l_l','oss',':',' ','0','.','11','551','\n",
      "',' ',' ','cer',':',' ','0','.','12','292','\n",
      "',' ',' ','wer',':',' ','0','.','09','581','\n",
      "',' ',' ','bleu',':',' ','0','.','873','92','\n",
      "',' ',' ','chr','f',':',' ','94','.','0','154','\n",
      "',' ',' ','chr','f','++',':',' ','93','.','787','56','\n",
      "',' ',' ','cut','om_','metric',' ','(','squa','red_','eval','_to_','train_','loss','_ra','ti','o',' ','=',' ','eva','l_l','os','s2','/','train_','loss','):',' ','0','.','13','12','\n",
      "',' ',' ','Fum','bles',' ','on',':','\n",
      "',' ',' ','Rare',' ','or',' ','strange',' ','date',' ','formats',' ','or',' ','complex',' ','scientific',' ','units','.','\n",
      "',' ',' ','Very',' ','large',' ','sentences',' ','(~','200','+',' ','tokens',')','\n",
      "',' ',' ','Conf','uses',' ','between',' ','similar',' ','numbers',' ','and',' ','large',' ','decimal',' ','expansions','\n",
      "',' ',' ','Over','-','generation',' ','(','the',' ','model',' ','might',' ','keep',' ','generating',' ','instructions',' ','after',' ','finishing',')','\n",
      "',' ',' ','In',' ','short',',',' ','the',' ','fine','-','tuned',' ','approach',' ','is',' ','flexible',' ','and',' ','covers',' ','more',' ','variety',' ','than',' ','the',' ','rule','-','based',' ','approach',' ','(','if',' ','it',' ','was',' ','in',' ','the',' ','training',' ','data',').',' ','However',',',' ','if',' ','it',' ','sees',' ','something',' ','truly',' ','outside',' ','its',' ','training',' ','distribution',',',' ','it',' ','might',' ','guess',' ','or',' ','hallucin','ate','.','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "',' ',' ','Re','produc','ibi','lity',' ','run',' ','plots',':','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "',' ',' ','4','.',' ','Training',' ','&',' ','Evaluation',' ','Metrics','\n",
      "',' ',' ','4','.','1',' ','Metrics',' ','Chosen','\n",
      "',' ',' ','We',' ','meticulously',' ','tracked',':','\n",
      "',' ',' ','Training',' ','Loss',' ','&',' ','Eva','l',' ','Loss',':',' ','Classic',' ','measure',' ','of',' ','how',' ','well',' ','the',' ','model',' ','fits',' ','the',' ','data','.','\n",
      "',' ',' ','BL','EU',':',' ','Word',' ','n','-','gr','am',' ','precision','.',' ','Good',' ','for',' ','measuring',' ','overall',' ','textual',' ','overlap','.','\n",
      "',' ',' ','CH','RF','/','CH','RF','++',':',' ','Character','-','based',' ','F','-','scores',',',' ','essential',' ','in',' ','highly',' ','infl','ected',' ','or',' ','morphological',' ','languages','.','\n",
      "',' ',' ','CER',':',' ','Character',' ','Error',' ','Rate',',',' ','i','.','e','.,',' ','edit',' ','distance',' ','at',' ','the',' ','character',' ','level','.',' ','If',' ','CER',' ','is',' ','0','.','12',',',' ','it',' ','means',' ','12','%',' ','of',' ','the',' ','characters',' ','di','ffer',' ','from',' ','the',' ','reference','.','\n",
      "',' ',' ','WER',':',' ','Word',' ','Error',' ','Rate','.',' ','Possibly',' ','~','0','.','08','–','0','.','1',',',' ','meaning',' ','8','–','10','%',' ','of',' ','the',' ','words',' ','di','ffer',' ','from',' ','reference','.','\n",
      "',' ',' ','Why',' ','so',' ','many','?',' ','Because',' ','partial',' ','expansions',' ','might',' ','get',' ','a',' ','high',' ','BL','EU',' ','but',' ','fail',' ','at',' ','the',' ','character',' ','level','.',' ','CH','RF',',',' ','CER',',',' ','WER',' ','each',' ','provide',' ','different',' ','vant','age',' ','points',' ','on',' ','alignment',' ','with',' ','the',' ','reference','.','\n",
      "',' ',' ','4','.','2',' ','Example',' ','Metric',' ','Snap','shots','\n",
      "',' ',' ','A',' ','<|unknown_token|>','best',' ','checkpoint','<|unknown_token|>',' ','from',' ','ru','n5',' ','might',' ','show',':','\n",
      "',' ',' ','train_','loss',' ','=',' ','0','.','101','\n",
      "',' ',' ','eva','l_l','oss',' ','=',' ','0','.','115','\n",
      "',' ',' ','bleu',' ','=',' ','0','.','874','\n",
      "',' ',' ','chr','f',' ','=',' ','94','.','0','15','\n",
      "',' ',' ','chr','f','++',' ','=',' ','93','.','788','\n",
      "',' ',' ','cer',' ','=',' ','0','.','123','\n",
      "',' ',' ','wer',' ','=',' ','0','.','096','\n",
      "',' ',' ','Interpretation',':',' ','~','12','.','3','%',' ','of',' ','characters',' ','di','ffer',' ','from',' ','reference',',',' ','which',' ','is',' ','quite',' ','decent',' ','for',' ','large',' ','multi','-','ling','ual',' ','expansions','.',' ','The',' ','model',' ','is',' ','basically',' ','correct',' ','~','88','–','90','%',' ','of',' ','the',' ','time',' ','for',' ','tricky',' ','expansions','.','\n",
      "',' ',' ','4','.','3',' ','Additional',' ','Logs','\n",
      "',' ',' ','We',' ','also',' ','tracked',':','\n",
      "',' ',' ','grad','_norm',':',' ','to',' ','see',' ','if',' ','the',' ','gradients',' ','were',' ','exploding',' ','or',' ','not','.','\n",
      "',' ',' ','learning','_rate',':',' ','verifying',' ','the',' ','scheduler','<|unknown_token|>','s',' ','shape','.','\n",
      "','\n",
      "',' ',' ','5','.',' ','Model',' ','Deployment',' ','&',' ','GG','UF',' ','Conversion','\n",
      "',' ',' ','5','.','1',' ','Why',' ','Convert',' ','to',' ','GG','UF','?','\n",
      "',' ',' ','Once',' ','the',' ','model',' ','is',' ','fine','-','tuned',',',' ','we',' ','want',' ','to',' ','deploy',' ','it',' ','for',' ','inference',' ','in',' ','a',' ','cost','-','free',' ','environment',' ','(','like',' ','CPU','-','only',' ','Hug','ging',' ','Fa','ce',' ','Spaces',').',' ','The',' ','<|unknown_token|>','gg','uf','<|unknown_token|>',' ','for','mat',',',' ','used',' ','by',' ','ll','ama','.','cpp',',',' ','is',' ','a',' ','CPU','-','friendly',' ','quant','ized',' ','for','mat',' ','that',':','\n",
      "',' ',' ','Reduc','es',' ','memory',' ','usage',' ','further',' ','than',' ','typical',' ','GPU',' ','quantiz','ation','.','\n",
      "',' ',' ','Perm','its',' ','near',' ','real','-','time',' ','inference',' ','for',' ','short',' ','sequen','ces',' ','on',' ','2',' ','CPU',' ','cor','es','.','\n",
      "',' ',' ','Minim','izes',' ','hosting',' ','cost','.',' ','(','We',' ','avoid',' ','renting',' ','GPUs',' ','in','definitely','.)','\n",
      "',' ',' ','Hence',' ','we',' ','used',' ','uns','lo','th','<|unknown_token|>','s',' ','built','-','in',' ','function',' ','push','_to_','hub','_g','guf','(...)',' ','with',' ','quantiz','ation_','method','=\"','q4','_k','_m','\".',' ','This',' ','yields',' ','a',' ','~','5','–','6GB',' ','model',' ','file','.','\n",
      "',' ',' ','5','.','2',' ','HF',' ','Spaces',' ','Setup',' ','in',' ','Depth','\n",
      "',' ',' ','5','.','2','.','1',' ','Repository',' ','Structure','\n",
      "',' ',' ','We',' ','have',' ','a',' ','space',' ','like',' ','Tas','may','-','Ti','b','/','sar','vam','-','ai','-','entity','-','normali','sation',' ','on',' ','Hug','ging',' ','Fa','ce','.',' ','Its',' ','files',' ','include',':','\n",
      "',' ',' ','app','.','py',' ','(','Stream','lit',' ','app','):','\n",
      "','\n",
      "','\n",
      "',' ',' ','A',' ','text',' ','input',' ','for',' ','the',' ','user','.','\n",
      "',' ',' ','A',' ','function',' ','infer','()',' ','that',' ','crafts',' ','the',' ','prompt',' ','(','the',' ','same',' ','<|unknown_token|>','instruction',' ','+',' ','input','<|unknown_token|>',' ','used',' ','in',' ','training',').','\n",
      "',' ',' ','Sub','mits',' ','this',' ','prompt',' ','to',' ','the',' ','local',' ','server',' ','via',' ','a',' ','curl',' ','POST',' ','request',' ','to',' ','http','://','localhost',':','80','81','/','completion','.','\n",
      "',' ',' ','Disp','lays',' ','the',' ','returned',' ','text',' ','to',' ','the',' ','user','.','\n",
      "',' ',' ','init','.','sh',':','\n",
      "','\n",
      "','\n",
      "',' ',' ','Clo','nes',' ','ll','ama','.','cpp','.','\n",
      "',' ',' ','Build','s',' ','ll','ama','-','server','.','\n",
      "',' ',' ','Downloads',' ','the',' ','.','gg','uf',' ','model',' ','from',' ','Hug','ging',' ','Fa','ce','.','\n",
      "',' ',' ','Launches',' ','ll','ama','-','server',' ','on',' ','port',' ','80','81','.','\n",
      "',' ',' ','in','it2','.','sh',':','\n",
      "','\n",
      "','\n",
      "',' ',' ','If',' ','the',' ','environment',' ','rest','arts',',',' ','re','-','check',' ','if',' ','ll','ama','.','cpp',' ','is',' ','present',',',' ','re','-','launch',' ','the',' ','server','.','\n",
      "',' ',' ','requirements','.','txt',':',' ','Minim','ally',' ','lists',' ','dependencies',' ','(','requests',' ','package',' ','is',' ','the',' ','only',' ','one',').','\n",
      "','\n",
      "','\n",
      "',' ',' ','5','.','2','.','2',' ','Free',' ','Tier',' ','Challenges','\n",
      "',' ',' ','On',' ','free',' ','Spaces',',',' ','the',' ','container',' ','stops',' ','after',' ','inactiv','ity','.',' ','We',' ','used',' ','an',' ','environment',' ','variable',' ','ran','_script','_once',' ','to',' ','skip',' ','re','-','downloading',' ','each',' ','time','.',' ','But',' ','if',' ','the',' ','container',' ','is',' ','truly',' ','re','started',',',' ','it',' ','has',' ','to',' ','recomp','ile','.',' ','Users',' ','might',' ','wait',' ','a',' ','few',' ','minutes',' ','for',' ','the',' ','server',' ','to',' ','initialize','.',' ','After',' ','that',',',' ','requests',' ','are',' ','fairly',' ','quick',' ','for',' ','short',' ','sentences','.','\n",
      "',' ',' ','The',' ','whole',' ','setup',' ','takes',' ','about',' ','6','-','7',' ','minutes',' ','to',' ','complete','.',' ','On',' ','loading',' ','the',' ','model',' ','once',',',' ','it',' ','stays',' ','for',' ','the',' ','next',' ','48',' ','hours',',',' ','else',' ','Hug','ging',' ','Fa','ce',' ','will',' ','turn',' ','it',' ','off','.',' ','Since',' ','no',' ','permanent',' ','storage',' ','is',' ','subscri','bed',' ','to',' ','it',' ','will',' ','have',' ','to',' ','reload',' ','if',' ','started',' ','after',' ','that','.',' ','Otherwise',',',' ','the',' ','model',' ','takes',' ','less',' ','than',' ','15',' ','seconds',' ','to',' ','load','.','\n",
      "',' ',' ','In',' ','the',' ','free',' ','tier',' ','a',' ','big',' ','challenge',' ','is',' ','inference',' ','speeds',',',' ','for',' ','which',' ','we',' ','have',' ','switched',' ','off',' ','the',' ','ram',' ','off','loading',' ','option',' ','(','enabling',' ','the',' ','whole',' ','model',' ','to',' ','be',' ','loaded',' ','on',' ','RAM','),',' ','enabled',' ','multi','-','threading',' ','and',' ','implemented',' ','prompt',' ','caching',' ','for',' ','the',' ','instruction',',',' ','example',' ','part',' ','of',' ','the',' ','prompt',' ','to',' ','be',' ','prelo','aded',',',' ','for',' ','quick',' ','loading',' ','of',' ','the',' ','model','.','\n",
      "',' ',' ','5','.','2','.','3',' ','End','-','User',' ','Experience','\n",
      "',' ',' ','Wait','-','time',':',' ','The',' ','server',' ','gets',' ','setup',' ','(','if',' ','not',' ','already','),',' ','or',' ','the',' ','model',' ','is',' ','spun',' ','up',' ','for',' ','serving',' ','and',' ','the',' ','prompt',' ','cached',' ','(','if',' ','it',' ','is',' ','already',' ','setup',').','\n",
      "',' ',' ','User',':',' ','<|unknown_token|>','I',' ','have',' ','a',' ','sentence',':',' ','20','/','02','/','2023',' ','क','ो',' ','म','ै','ं','न','े',' ','₹','500',' ','म','ें',' ','3l','bs',' ','च','ी','न','ी',' ','ख','र','ी','द','ी','।','<|unknown_token|>','\n",
      "',' ',' ','They',' ','press',' ','<|unknown_token|>','Submit','<|unknown_token|>',' ','or',' ','click',' ','enter',' ','on',' ','the',' ','input',' ','box','.','\n",
      "',' ',' ','The',' ','App',':',' ','constructs',' ','the',' ','big',' ','prompt',' ','and',' ','does',' ','curl',' ','--','data',' ',''.','..','json','..','.'',' ','http','://','localhost',':','80','81','/','completion','.','\n",
      "',' ',' ','ll','ama','-','server',':',' ','loads',' ','the',' ','model',' ','in',' ','CPU',' ','(','already',' ','loaded',' ','when',' ','shown',' ','to',' ','the',' ','user','),',' ','runs',' ','the',' ','inference',',',' ','returns',' ','the',' ','spelled','-','out',' ','text','.','\n",
      "',' ',' ','The',' ','App',':',' ','displays',' ','something',' ','like',' ','<|unknown_token|>','ब','ी','स',' ','फ','र','व','र','ी',' ','द','ो',' ','ह','ज','ा','र',' ','त','े','ई','स',' ','क','ो',' ','म','ै','ं','न','े',' ','प','ा','ँ','च',' ','स','ौ',' ','र','ु','प','य','े',' ','म','ें',' ','त','ी','न',' ','प','ा','उ','ं','ड',' ','च','ी','न','ी',' ','ख','र','ी','द','ी','।','<|unknown_token|>','\n",
      "',' ',' ','User',':',' ','<|unknown_token|>','Co','py',' ','Output','<|unknown_token|>',' ','button',' ','to',' ','get',' ','it',' ','in',' ','the',' ','clipboard','.','\n",
      "','\n",
      "','\n",
      "','\n",
      "',' ',' ','6','.',' ','Comparative',' ','Analysis',' ','&',' ','Discussion','\n",
      "',' ',' ','6','.','1',' ','Which',' ','Method',' ','Ex','cels',' ','Where','?','\n",
      "',' ',' ','Agen','tic',':','\n",
      "',' ',' ','Pros',':',' ','Zero',' ','additional',' ','code',' ','or',' ','training','.',' ','Just',' ','prompt',' ','engineering','.','\n",
      "',' ',' ','Cons',':',' ','In','consistent',',',' ','no',' ','guaranteed',' ','coverage',',',' ','messy',' ','for',' ','multi','-','ling','ual',' ','expansions','.','\n",
      "',' ',' ','Algorithm','ic',':','\n",
      "',' ',' ','Pros',':',' ','Determin','istic',',',' ','absolutely',' ','correct',' ','for',' ','known',' ','patterns',',',' ','easy',' ','to',' ','interpret','.','\n",
      "',' ',' ','Cons',':',' ','Requires',' ','large',',',' ','ever','-','expanding',' ','sets',' ','of',' ','rules','.',' ','Miss','es',' ','out','-','of','-','vocabulary',' ','patterns','.','\n",
      "',' ',' ','Fine','-','Tun','ed',' ','LL','M',':','\n",
      "',' ',' ','Pros',':',' ','Ada','pts',' ','to',' ','minor',' ','for','mat',' ','changes',' ','or',' ','partial',' ','trans','litera','tion',',',' ','more',' ','<|unknown_token|>','intelligent','<|unknown_token|>',' ','coverage',' ','if',' ','it','<|unknown_token|>','s',' ','in',' ','distribution','.','\n",
      "',' ',' ','Cons',':',' ','If',' ','it',' ','encounters',' ','something',' ','unseen',',',' ','it',' ','might',' ','guess',' ','or',' ','hallucin','ate','.',' ','Some',' ','minor',' ','numer','ic',' ','errors',',',' ','especially',' ','for',' ','larger',' ','numbers',' ','or',' ','decimal','s','.','\n",
      "',' ',' ','In',' ','Real',' ','Life',',',' ','A',' ','system',' ','might',' ','use',' ','a',' ','hybrid',' ','approach',':',' ','Let',' ','the',' ','LL','M',' ','produce',' ','expansions',',',' ','then',' ','pass',' ','it',' ','through',' ','the',' ','rule','-','based',' ','checker',',',' ','and',' ','for',' ','one',',',' ','just',' ','pass',' ','the',' ','input',' ','to',' ','a',' ','rule','-','based',' ','checker',' ','or',' ','corre','ctor','.',' ','Then',',',' ','have',' ','some',' ','sort',' ','of',' ','text',' ','classification',' ','system',',',' ','maybe',' ','BERT','-','based',' ','or',' ','LL','M','-','based',',',' ','to',' ','classify',' ','which',' ','is',' ','better','.','\n",
      "',' ',' ','6','.','2',' ','Synthesis',' ','of',' ','Costs',',',' ','Complex','ity',',',' ','&',' ','Relia','bility','\n",
      "',' ',' ','The',' ','rule','-','based',' ','approach',' ','is',' ','cheap',' ','to',' ','run',' ','but',' ','expensive',' ','to',' ','expand',' ','to',' ','new',' ','domains','.','\n",
      "',' ',' ','The',' ','fine','-','tuning',' ','approach',' ','costs',' ','a',' ','moderate',' ','amount',' ','(','~$','10',' ','for',' ','training','),',' ','but',' ','once',' ','done',',',' ','it',' ','is',' ','easy',' ','to',' ','deploy',' ','in',' ','CPU',' ','for','mat','.','\n",
      "',' ',' ','The',' ','age','ntic',' ','approach',' ','is',' ','easy',' ','to',' ','test',' ','but',' ','rarely',' ','meets',' ','production','-','level',' ','reliability','.','\n",
      "',' ',' ','A',' ','fourth',' ','method',' ','is',' ','using',' ','an',' ','instruct','-','trained',' ','model',' ','for',' ','deployment',',',' ','but',' ','that',' ','won','<|unknown_token|>','t',' ','require',' ','any',' ','inputs',' ','from',' ','this',' ','task','<|unknown_token|>','s',' ','perspective',' ','and',' ','would',' ','require',' ','a',' ','simple',' ','prompt','.',' ','Thus',',',' ','that',' ','is',' ','not',' ','demonstrated',' ','here','.','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "','\n",
      "',' ',' ','7','.',' ','Scope',' ','for',' ','Improvement','\n",
      "',' ',' ','We',' ','see',' ','multiple',' ','opportunities',' ','for',' ','future',' ','expansions',':','\n",
      "',' ',' ','Larger',' ','&',' ','More',' ','Var','ied',' ','Dataset','\n",
      "',' ',' ','Expand',' ','from',' ','1',',','600',' ','to',' ','10k',' ','or',' ','20k',' ','samples',',',' ','ensuring',' ','more',' ','unusual',' ','currencies',' ','(','SAR',',',' ','RUB',',',' ','BRL','?),',' ','more',' ','advanced',' ','scientific',' ','units',' ','(','kWh',',',' ','mm','Hg',',',' ','bar','),',' ','and',' ','more',' ','irregular',' ','date',' ','formats','.',' ','Possibly',' ','include',' ','code','-','switched',' ','data',' ','or',' ','incomplete',' ','trans','litera','tion','.','\n",
      "',' ',' ','Improve',' ','the',' ','dataset',' ','on',' ','aspects',' ','mentioned',' ','before',':','\n",
      "',' ',' ','Long',' ','sentences','\n",
      "',' ',' ','Complex',' ','Units','\n",
      "',' ',' ','Decimal',' ','based',' ','longer',' ','numbers','\n",
      "',' ',' ','Larger',' ','variety',' ','of',' ','numbers','\n",
      "',' ',' ','Including',' ','rar','er',' ','date',' ','formats','\n",
      "',' ',' ','Integ','rate',' ','real',' ','or',' ','partially',' ','real',' ','data',' ','(','like',' ','Indian',' ','news',' ','articles',' ','or',' ','official',' ','docs',')',' ','to',' ','reduce',' ','the',' ','synthetic',' ','bias',',',' ','or',' ','use',' ','data',' ','from',' ','multiple',' ','models','.','\n",
      "',' ',' ','Multi','-','Task',' ','or',' ','Instruction',' ','Tun','ing','\n",
      "',' ',' ','T','ying',' ','entity',' ','normalization',' ','with',' ','other',' ','tasks',' ','like',' ','translation',',',' ','summ','ariz','ation',',',' ','or',' ','classification',' ','can',' ','yield',' ','a',' ','more',' ','robust',' ','model','.','\n",
      "',' ',' ','Or',' ','incorporate',' ','a',' ','<|unknown_token|>','chain','-','of','-','thought','<|unknown_token|>',' ','approach',' ','with',' ','step','-','by','-','step',' ','expansions',' ','for',' ','each',' ','numer','ic',' ','entity','.','\n",
      "',' ',' ','A',' ','general',' ','instruct',' ','tuning',' ','approach',' ','may',' ','also',' ','be',' ','tried',',',' ','which',' ','can',' ','then',' ','be',' ','further',' ','fine','tuned',' ','for',' ','entity',' ','normali','sation',' ','task','.',' ','(','However',' ','this',' ','would',' ','cost',' ','much',' ','more',')','\n",
      "',' ',' ','Fine','-','Grai','ned',' ','Domain',' ','Coverage','\n",
      "',' ',' ','Cover',' ','a',' ','larger',' ','variety',' ','of',' ','data',' ','e','.','g','.,',' ','medical',' ','domain',' ','might',' ','have',' ','specific',' ','units',' ','like',' ','<|unknown_token|>','mg','/','dL',',','<|unknown_token|>',' ','<|unknown_token|>','mm','Hg',',','<|unknown_token|>',' ','<|unknown_token|>','IU',',','<|unknown_token|>',' ','<|unknown_token|>','mL','/','h',',','<|unknown_token|>',' ','which',' ','are',' ','rarely',' ','used',' ','in',' ','finance',' ','or',' ','general',' ','domains','.',' ','Each',' ','domain',' ','can',' ','get',' ','its',' ','own',' ','expansions','.','\n",
      "',' ',' ','Hybrid',' ','Pipeline','\n",
      "',' ',' ','Let',' ','the',' ','model',' ','do',' ','its',' ','best',',',' ','then',' ','verify',' ','or',' ','correct',' ','it',' ','with',' ','a',' ','rule','-','based',' ','approach','.',' ','If',' ','the',' ','model','<|unknown_token|>','s',' ','expansions',' ','deviate',' ','from',' ','recognized',' ','patterns',',',' ','the',' ','system',' ','can',' ','highlight',' ','or',' ','fix',' ','them',',',' ','thus',' ','combining',' ','the',' ','best',' ','of',' ','both',' ','worlds','.','\n",
      "',' ',' ','Better',' ','Language',' ','Detection','\n",
      "',' ',' ','Some',' ','user',' ','inputs',' ','might',' ','have',' ','code','-','mixed',' ','text',',',' ','e','.','g','.,',' ','<|unknown_token|>','He',' ','purchased',' ','$','120',' ','म','ें',' ','2k','g',' ','द','ू','ध','.','<|unknown_token|>',' ','The',' ','system',' ','must',' ','handle',' ','partial',' ','English',' ','and',' ','partial',' ','Hindi','.',' ','A',' ','robust',' ','code','-','mixing',' ','detection',' ','method',' ','might',' ','be',' ','required','.',' ','(','This',' ','is',' ','expected',' ','to',' ','move',' ','general','isation',' ','by',' ','a',' ','lot',')','\n",
      "','\n",
      "','\n",
      "',' ',' ','8','.',' ','Conclusion',' ','&',' ','Refr','ences','\n",
      "',' ',' ','8','.','1',' ','Conclusion','\n",
      "',' ',' ','Through',' ','this',' ','project',',',' ','we',' ','deeply',' ','explored',' ','the',' ','domain',' ','of',' ','multi','-','ling','ual',' ','entity',' ','normalization',' ','for',' ','Ind','ic',' ','languages','—','focusing',' ','on',' ','dates',',',' ','currencies',',',' ','and',' ','scientific',' ','units','.',' ','We',' ','produced',' ','a',' ','synthetic',' ','dataset',' ','of',' ','1',',','600',' ','examples',',',' ','balanced',' ','across',' ','10',' ','languages',' ','and',' ','multiple',' ','domains',',',' ','each',' ','carefully',' ','split',' ','into',' ','train',' ','and',' ','eval',' ','sets','.',' ','Then',' ','we',' ','implemented',':','\n",
      "',' ',' ','Agen','tic',' ','Method',':',' ','Straight',' ','prompts',',',' ','sub','optimal','.','\n",
      "',' ',' ','Algorithm','ic',' ','Method',':',' ','Rule','-','based',',',' ','extremely',' ','accurate',' ','for',' ','known',' ','patterns',' ','but',' ','limited',' ','coverage','.','\n",
      "',' ',' ','Fine','-','Tun','ed',' ','LL','M',':',' ','A',' ','4','-','bit',' ','LL','a','MA',' ','3','.','1',' ','model',' ','trained',' ','on',' ','our',' ','dataset','.',' ','This',' ','approach',' ','delivered',' ','high',' ','accuracy',' ','on',' ','the',' ','evaluation',' ','set','.',' ','It',' ','still',' ','had',' ','some',' ','corner','-','case',' ','issues',' ','(','decimal',' ','expansions',',',' ','rare',' ','date',' ','forms',',',' ','etc','.).','\n",
      "',' ',' ','Finally',',',' ','we',' ','converted',' ','the',' ','SFT',' ','model',' ','to',' ','a',' ','GG','UF',' ','for','mat',' ','for',' ','CPU',' ','hosting',',',' ','deployed',' ','on',' ','a',' ','Hug','ging',' ','Fa','ce',' ','Space',' ','with',' ','a',' ','Stream','lit',' ','front',' ','end',' ','and',' ','ll','ama','.','cpp','–','based',' ','server','.',' ','The',' ','entire',' ','pipeline',' ','is',' ','relatively',' ','cost','-','effective',',',' ','with',' ','total',' ','training',' ','cost',' ','under',' ','$','10',' ','on',' ','an',' ','L4',' ','GPU',' ','and',' ','about',' ','($','0','.','81',' ','for',' ','one',' ','run','),',' ','and',' ','free',' ','CPU',' ','inference',' ','on',' ','HF',' ','Spaces','.','\n",
      "',' ',' ','Next',' ','Steps',':',' ','We',' ','plan',' ','to',' ','add',' ','more',' ','data',',',' ','handle',' ','code','-','mixed',' ','text',',',' ','increase',' ','the',' ','vocabulary',' ','for',' ','the',' ','algorithmic',' ','approach',',',' ','and',' ','consider',' ','a',' ','<|unknown_token|>','hybrid',' ','approach','<|unknown_token|>',' ','that',' ','mer','ges',' ','LL','M',' ','outputs',' ','with',' ','rule','-','based',' ','validation',' ','for',' ','near','-','perfect',' ','coverage',' ','in',' ','real',' ','production','.','\n",
      "',' ',' ','8','.','2',' ','References',' ','&',' ','Links','\n",
      "',' ',' ','Primary',' ','Links',':','\n",
      "',' ',' ','Synthetic',' ','Dataset',':',' ','Tas','may','-','Ti','b','/','sar','vam','-','entity','-','recognition','-','ge','mini','-','2','.','0','-','flash','-','thinking','-','0','1','-','21','-','distill','-','1600','\n",
      "',' ',' ','Fine','-','Tun','ed',' ','Model',':',' ','Tas','may','-','Ti','b','/','sar','vam','-','entity','-','normali','sation','-','ll','ama','-','3','.','1','-','8b','\n",
      "',' ',' ','GG','UF',' ','Model',':',' ','Tas','may','-','Ti','b','/','sar','vam','-','entity','-','normali','sation','-','ll','ama','-','3','.','1','-','8b','-','gg','uf','\n",
      "',' ',' ','HF',' ','Spaces',' ','Deployment',':',' ','Tas','may','-','Ti','b','/','sar','vam','-','ai','-','entity','-','normali','sation','\n",
      "',' ',' ','Secondary',' ','Links',':','\n",
      "',' ',' ','Main',' ','Model',' ','training',' ','notebook',' ','with',' ','all',' ','three',' ','approaches',':',' ','https','://','co','lab','.','research','.','google','.','com','/','drive','/','16','_c','-','-?','u','sp','=','sharing','\n",
      "',' ',' ','Wa','ndb',' ','Plo','ts',' ','for',' ','train',' ','runs',' ','(','6',' ','major',' ','ones',' ','out',' ','of',' ','45',' ','total','):',' ','https','://','api','.','wa','ndb','.','ai','/','links','/','tas','may','tib','re','wal','-','iit','-','kha','rag','pur','/','\n",
      "',' ',' ','Model',' ','Infer','encing',' ','Col','ab',' ','notebook',':',' ','https','://','co','lab','.','research','.','google','.','com','/','drive','/','-?','u','sp','=','sharing','\n",
      "',' ',' ','GG','UF',' ','Model',' ','Infer','encing',' ','Col','ab',' ','notebook',':',' ','https','://','co','lab','.','research','.','google','.','com','/','drive','/?','u','sp','=','sharing','\n",
      "',' ',' ','Synthetic',' ','Dataset',' ','Creation',' ','chat',' ','(','Google',' ','AI',' ','Studio','):',' ','https','://','ais','tudio','.','google','.','com','/','app','/','prompts','?','state','=%','7B','%','%','22',':%','5B','%','%','22','%','5D',',%','22','action','%','22',':%','22','open','%','22',',%','22','userId','%','22',':%','221','077','459','876','078','420','02','805','%','22',',%','22','resource','Keys','%','22',':%','7B','%','7D','%','7D','&','u','sp','=','sharing','\n",
      "',' ',' ','HF',' ','Spaces',' ','Deployment',' ','GitHub',' ','Re','po',':',' ','https','://','github','.','com','/','Tas','may','-','Ti','bre','wal','/','GG','UF','-','HF','-','deployment','\n",
      "',' ',' ','GG','UF',' ','conversion',' ','notebook',':',' ','https','://','co','lab','.','research','.','google','.','com','/','drive','/?','u','sp','=','sharing','\n",
      "',' ',' ','Final',' ','Model',' ','Re','produc','ibi','lity',' ','Note','book',':',' ','https','://','co','lab','.','research','.','google','.','com','/','drive','/?','u','sp','=','sharing','\n",
      "',' ',' ','Wa','ndb',' ','Plo','ts',' ','for',' ','Re','produc','ibi','lity',' ','run',':',' ','https','://','api','.','wa','ndb','.','ai','/','links','/','tas','may','tib','re','wal','-','iit','-','kha','rag','pur','/','\n",
      "',' ',' ','Prediction',' ','creation',' ','notebook',':',' ','https','://','co','lab','.','research','.','google','.','com','/','drive','/?','u','sp','=','sharing','\n",
      "',' ',' ','GG','UF',' ','Model',' ','zip',' ','file',':',' ','https','://','drive','.','google','.','com','/','file','/','d','//','view','\n",
      "',' ',' ','Stop','-','words',' ','zip',' ','file',':',' ','https','://','drive','.','google','.','com','/','file','/','d','//','view','?','u','sp','=','sharing','\n",
      "',' ',' ','Model',' ','Predictions',' ','(','eval',',',' ','train',',',' ','and',' ','total',' ','data',' ','pre','ds',',',' ','in',' ','normal',' ','and',' ','exc','el',' ','for','mat',',',' ','which',' ','are',' ','u','tf','-','8',' ','and',' ','u','tf','-','8','-','s','ig',' ','encoded','.',' ','Excel',' ','reads',' ','u','tf','-','8','-','s','ig',' ','easily',',',' ','thus',' ','good',' ','for',' ','viewing',',',' ','u','tf','-','8',' ','is',' ','the',' ','standard',' ','encoding',' ','method',' ','used',' ','for',' ','programming',' ','purposes',',',' ','thus',' ','it',' ','is',' ','given',' ','as',' ','well','):','\n",
      "',' ',' ','1','.',' ','Eva','l',':','\t','\t','\t','\t','\t','\t','\t','\t','\t','\t','\t',' ',' ',' ',' ',' ',' ','-',' ','eval','_dat','a_0','01_','predictions','.','csv',' ','(','u','tf','-','8',' ','encoded','):',' ','https','://','drive','.','google','.','com','/','file','/','d','/--','le','/','view','?','u','sp','=','sharing',' ',' ',' ',' ',' ',' ',' ',' ',' ',' ','-',' ','eval','_dat','a_0','01_','prediction','s_ex','cel','.','csv',' ','(','u','tf','-','8','-','s','ig',' ','encoded','):',' ','https','://','drive','.','google','.','com','/','file','/','d','//','view','?','u','sp','=','sharing','\n",
      "',' ',' ','2','.',' ','Train',':','\t','\t','\t','\t','\t','\t','\t','\t','\t','\t','\t',' ',' ',' ',' ',' ',' ','-',' ','train_','data_','001','_pre','dictions','.','csv',' ','(','u','tf','-','8',' ','encoded','):',' ','https','://','drive','.','google','.','com','/','file','/','d','/','1','--','-/','view','?','u','sp','=','sharing',' ',' ',' ',' ',' ',' ',' ',' ',' ','-',' ','train_','data_','001','_pre','diction','s_ex','cel','.','csv',' ','(','u','tf','-','8','-','s','ig',' ','encoded','):',' ','https','://','drive','.','google','.','com','/','file','/','d','/-','/','view','?','u','sp','=','sharing','\n",
      "',' ',' ','3','.',' ','Total',':','\t','\t','\t','\t','\t','\t','\t','\t','\t','\t','\t',' ',' ',' ',' ',' ',' ','-',' ','data_','001','_pre','dictions','.','csv',' ','(','u','tf','-','8',' ','encoded','):',' ','https','://','drive','.','google','.','com','/','file','/','d','//','view','?','u','sp','=','sharing',' ',' ',' ',' ',' ',' ',' ',' ',' ',' ','-',' ','data_','001','_pre','diction','s_ex','cel','.','csv',' ','(','u','tf','-','8','-','s','ig',' ','encoded','):',' ','https','://','drive','.','google','.','com','/','file','/','d','/-','/','view','?','u','sp','=','sharing','\n",
      "',' ',' '\n"
     ]
    }
   ],
   "source": [
    "tokeniser.visualise_tokens(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46746, 29659, 29659, 67133, 131072, 118813, 29659, 111880, 92770, 115964, 29659, 18456, 76330, 29659, 66911, 46746, 46746, 46746, 46746, 46746, \n",
      "46746, 46746, 46746, 46746, 46746, 46746, 29659, 29659, 113489, 95769, 29659, 130643, 88847, 29659, 81009, 8901, 71026, 46746, 29659, 29659, \n",
      "48368, 84422, 29659, 50865, 46746, 29659, 29659, 117538, 29659, 103382, 29659, 44405, 29659, 29896, 29659, 27541, 29659, 122612, 29659, 22817, \n",
      "29659, 117264, 84422, 29659, 129289, 29659, 18456, 76330, 29659, 103028, 23354, 29659, 24487, 29659, 50871, 29659, 45484, 50416, 29659, 103839, \n",
      "29659, 97661, 29659, 92364, 29659, 83920, 29659, 902, 29659, 22054, 29659, 37143, 36093, 97424, 107595, 23354, 29659, 49746, 23354, 29659, \n",
      "43446, 23354, 29659, 101053, 19698, 29659, 6973, 29659, 59932, 29659, 59604, 29659, 81638, 29659, 38023, 23354, 29659, 112669, 23354, 29659, \n",
      "44717, 76330, 29659, 71390, 23354, 29659, 101053, 84422, 29659, 24429, 29659, 126869, 29659, 13653, 29659, 44405, 29659, 27541, 29659, 16699, \n",
      "29659, 59864, 29659, 95143, 29659, 130036, 23354, 29659, 104998, 23354, 29659, 118971, 29659, 92832, 29659, 11193, 29659, 118971, 29659, 54695, \n",
      "29659, 50572, 29659, 54225, 90001, 77197, 29659, 114210, 29659, 51398, 29659, 24487, 29659, 81882, 29659, 22315, 29659, 44833, 84422, 29659, \n",
      "125366, 29659, 92170, 29659, 87740, 29659, 64801, 29659, 123538, 29659, 59981, 118459, 29659, 72074, 23354, 29659, 114210, 29659, 103382, 23354, \n",
      "29659, 44833, 29659, 71231, 23354, 29659, 44717, 76330, 29659, 71390, 23354, 29659, 118971, 29659, 38802, 84422, 46746, 29659, 29659, 129289, \n",
      "29659, 11307, 29659, 126225, 23354, 29659, 97661, 29659, 54919, 29659, 112949, 29659, 89920, 29659, 111739, 104505, 46746, 29659, 29659, 35753, \n",
      "51175, 29659, 37143, 42653, 90001, 25108, 55972, 46746, 29659, 29659, 42765, 76330, 29659, 37143, 99884, 29659, 128237, 90001, 25108, 55972, \n",
      "46746, 29659, 29659, 27043, 90001, 26263, 58866, 29659, 78655, 38509, 29659, 37143, 116960, 126095, 29659, 128878, 29659, 123538, 29659, 76394, \n",
      "29659, 93136, 29659, 55153, 55972, 46746, 29659, 29659, 71553, 29659, 17132, 29659, 94528, 29659, 76394, 29659, 55153, 29659, 64018, 29659, \n",
      "12100, 29659, 48368, 23354, 64360, 29659, 93136, 29659, 14357, 29659, 56399, 29659, 46320, 29659, 62472, 29659, 63622, 29659, 103028, 29659, \n",
      "118971, 29659, 902, 29659, 120351, 84422, 29659, 59421, 29659, 44405, 29659, 78208, 29659, 51398, 90001, 76436, 29659, 63274, 29659, 64018, \n",
      "29659, 80863, 29659, 6319, 84422, 46746, 46746, 46746, 46746, 46746, 46746, 46746, 46746, 46746, 29659, 29659, 58811, 84422, 29659, 71336, \n",
      "29659, 24525, 103770, 29659, 51398, 29659, 117288, 29659, 13153, 46746, 29659, 29659, 20932, 29659, 46730, 29659, 130013, 29659, 69556, 59043, \n",
      "29659, 24242, 29659, 128878, 29659, 27541, 29659, 124590, 29659, 103382, 23354, 29659, 97661, 29659, 76665, 29659, 172, 29659, 39416, 114830, \n",
      "29659, 24487, 29659, 130013, 29659, 43177, 29659, 118971, 29659, 55265, 29659, 76960, 29659, 25486, 84422, 46746, 29659, 29659, 58811, 84422, \n",
      "48368, 29659, 121497, 29950, 29659, 118971, 29659, 71780, 46746, 29659, 29659, 124686, 29659, 15836, 29659, 71336, 91220, 46746, 29659, 29659, \n",
      "54462, 92891, 29659, 64018, 29659, 21518, 29659, 71336, 115661, 104505, 29659, 71553, 29659, 86867, 29659, 14357, 29659, 84010, 29659, 52955, \n",
      "29659, 83720, 29659, 73105, 29659, 55641, 29659, 44717, 76330, 29659, 112143, 29659, 72885, 29659, 46931, 29659, 27541, 29659, 54225, 90001, \n",
      "77197, 29659, 46931, 29659, 51398, 29659, 119792, 29659, 44833, 84422, 29659, 46746, 29659, 29659, 85368, 96038, 29659, 16688, 104505, 29659, \n",
      "116933, 29659, 64050, 114830, 29659, 76394, 29659, 97178, 29659, 96382, 29659, 27541, 29659, 54695, 29659, 59864, 29659, 10303, 29659, 902, \n",
      "29659, 124590, 29659, 46931, 29659, 37143, 130036, 23354, 29659, 104998, 23354, 29659, 11193, 55972, 29659, 51398, 29659, 119792, 29659, 22315, \n",
      "23354, 29659, 97661, 29659, 124461, 29659, 71869, 748, 29659, 57297, 29659, 76394, 29659, 37413, 29659, 81658, 29659, 64018, 29659, 75768, \n",
      "84422, 46746, 29659, 29659, 36566, 29659, 69977, 104505, 29659, 71553, 29659, 28423, 29659, 27541, 29659, 117248, 29659, 15706, 29659, 51398, \n",
      "29659, 131072, 110856, 23354, 131072, 29659, 131072, 109217, 23354, 131072, 29659, 131072, 31381, 23354, 131072, 29659, 131072, 92832, 23354, 131072, \n",
      "29659, 131072, 30201, 23354, 131072, 29659, 101053, 84422, 29659, 82437, 23354, 29659, 81173, 29659, 84010, 29659, 95449, 29659, 130013, 29659, \n",
      "18549, 29659, 125299, 29659, 57639, 29659, 21030, 29659, 77200, 29659, 31653, 103770, 84422, 46746, 29659, 29659, 4653, 29659, 98116, 46746, \n",
      "29659, 29659, 71553, 29659, 58021, 29659, 27541, 29659, 57297, 29659, 46320, 29659, 103028, 104505, 29659, 40071, 23354, 29659, 49746, 23354, \n",
      "29659, 43446, 23354, 29659, 1148, 33226, 23354, 29659, 99073, 19881, 23354, 29659, 51876, 31889, 23354, 29659, 92388, 23354, 29659, 116603, \n",
      "63674, 23354, 29659, 34787, 23354, 29659, 118971, 29659, 113369, 60469, 84422, 29659, 46424, 29659, 2021, 29659, 43574, 29659, 22054, 29659, \n",
      "118971, 29659, 64913, 29659, 91458, 84422, 29659, 73596, 29659, 114916, 104505, 46746, 29659, 29659, 40071, 29659, 1301, 29659, 113369, 60469, \n",
      "29659, 127, 29659, 36093, 97424, 107595, 29659, 37480, 29659, 101213, 114087, 29659, 51398, 29659, 111384, 29659, 123364, 84422, 46746, 29659, \n",
      "29659, 116603, 63674, 29659, 118971, 29659, 92388, 29659, 39285, 29659, 41293, 29659, 22054, 29659, 37480, 29659, 18549, 29659, 127, 29659, \n",
      "41402, 29659, 83540, 29659, 71390, 29659, 81638, 29659, 72110, 84422, 46746, 29659, 29659, 36566, 29659, 98116, 46746, 29659, 29659, 71336, \n",
      "29659, 44405, 29659, 94513, 29659, 55641, 29659, 119552, 29659, 120351, 23354, 29659, 120044, 29659, 84010, 29659, 24487, 29659, 24192, 29659, \n",
      "44405, 29659, 63308, 29659, 118971, 29659, 97079, 84422, 29659, 113649, 29659, 92171, 104505, 46746, 29659, 29659, 51957, 46746, 29659, 29659, \n",
      "35480, 46746, 29659, 29659, 35557, 46746, 29659, 29659, 64020, 46746, 29659, 29659, 77584, 46746, 29659, 29659, 79540, 46746, 29659, 29659, \n",
      "40880, 46746, 29659, 29659, 51912, 46746, 29659, 29659, 96447, 46746, 29659, 29659, 92336, 46746, 46746, 46746, 29659, 29659, 117538, 29659, \n",
      "16688, 46746, 29659, 29659, 54240, 104505, 29659, 71553, 29659, 86867, 29659, 902, 29659, 56095, 121287, 18646, 125160, 61210, 125160, 58074, \n",
      "58074, 23354, 61210, 125160, 18646, 125160, 58074, 58074, 23354, 29659, 18646, 90001, 61210, 90001, 58074, 23354, 29659, 87933, 29659, 25029, \n",
      "23354, 29659, 131072, 79199, 29659, 107316, 23354, 29659, 86690, 131072, 23354, 29659, 101053, 84422, 29659, 125366, 29659, 27733, 29659, 24487, \n",
      "29659, 11241, 29659, 96382, 29659, 112143, 29659, 54170, 29659, 124461, 29659, 15047, 29659, 27541, 29659, 119552, 29659, 95449, 90001, 8367, \n",
      "29659, 35389, 29659, 103397, 91024, 84422, 46746, 29659, 29659, 24525, 837, 23732, 104505, 29659, 64848, 129713, 23354, 29659, 62964, 20757, \n",
      "23354, 29659, 78855, 29659, 74929, 23354, 29659, 88870, 84422, 29659, 23383, 23354, 29659, 115112, 21705, 23354, 29659, 19215, 2811, 23354, \n",
      "29659, 101053, 84422, 29659, 21801, 29659, 25472, 29659, 46320, 29659, 75914, 29659, 104998, 23354, 29659, 119792, 29659, 128314, 29659, 54225, \n",
      "29659, 77197, 29659, 20407, 29659, 51398, 29659, 119792, 29659, 22315, 29659, 44833, 84422, 46746, 29659, 29659, 51957, 29659, 102183, 104505, \n",
      "29659, 50036, 29659, 83684, 29659, 37143, 81161, 113040, 55972, 29659, 27541, 29659, 51077, 29659, 37143, 46320, 29659, 91860, 125160, 58643, \n",
      "54433, 29659, 6973, 29659, 69254, 29659, 37143, 109761, 79649, 112475, 54433, 29659, 88963, 29659, 37143, 58811, 29659, 59343, 54433, 29659, \n",
      "98733, 29659, 37143, 113857, 23354, 29659, 27541, 70874, 54433, 29659, 118971, 29659, 38802, 84422, 46746, 29659, 29659, 58811, 84422, 58811, \n",
      "29659, 42653, 29659, 65709, 29659, 81638, 29659, 71336, 29659, 43245, 46746, 29659, 29659, 21801, 29659, 130013, 29659, 48848, 29659, 74265, \n",
      "116454, 29659, 33813, 29659, 29396, 29659, 849, 29659, 125259, 29659, 57029, 29659, 37143, 12972, 54433, 29659, 10303, 29659, 45382, 29659, \n",
      "66595, 29659, 122428, 84422, 29659, 64548, 29659, 92637, 29659, 43187, 29659, 6848, 29659, 118807, 29659, 81173, 29659, 90575, 104505, 46746, \n",
      "29659, 29659, 21801, 29659, 122428, 29659, 124461, 29659, 42626, 29659, 65208, 29659, 51398, 29659, 24487, 29659, 130013, 29659, 43177, 29659, \n",
      "122428, 29659, 24337, 6458, 84422, 29659, 13740, 29659, 16683, 29659, 128878, 29659, 48848, 29659, 107734, 29659, 27541, 29659, 77464, 29659, \n",
      "24487, 29659, 130013, 29659, 51398, 29659, 24487, 29659, 61337, 84422, 29659, 31308, 29659, 107734, 29659, 81638, 29659, 93136, 29659, 130013, \n",
      "29659, 43177, 29659, 44405, 29659, 74810, 89431, 100216, 90001, 58811, 84422, 80502, 90001, 81652, 90001, 73258, 90001, 80502, 48368, 90001, \n",
      "8219, 74810, 29659, 76631, 29659, 124759, 29659, 59675, 29659, 63235, 29659, 39670, 29659, 98703, 23354, 29659, 44405, 29659, 83874, 29659, \n",
      "64018, 29659, 63739, 29659, 118971, 29659, 2021, 29659, 76394, 29659, 118942, 4922, 29659, 24192, 29659, 100713, 29659, 75175, 105010, 58501, \n",
      "29659, 84564, 15158, 46746, 29659, 29659, 125366, 29659, 16683, 29659, 128878, 29659, 36350, 29659, 33834, 29659, 54695, 29659, 37604, 48368, \n",
      "23354, 64360, 29659, 14357, 23354, 29659, 120044, 29659, 88932, 29659, 108142, 104505, 46746, 29659, 29659, 75865, 105533, 104505, 29659, 46320, \n",
      "29659, 120351, 29659, 51398, 29659, 72016, 84422, 46746, 29659, 29659, 66911, 104505, 29659, 46320, 29659, 103028, 29659, 51398, 29659, 72016, \n",
      "84422, 46746, 29659, 29659, 58359, 104505, 29659, 80502, 84422, 48368, 23354, 29659, 80502, 84422, 19473, 23354, 29659, 80502, 84422, 14147, \n",
      "23354, 29659, 48368, 84422, 80502, 29659, 37143, 17434, 29659, 14357, 29659, 92171, 29659, 16821, 29659, 118971, 29659, 69710, 23354, 29659, \n",
      "57121, 29659, 76384, 29659, 92171, 29659, 63235, 29659, 112143, 29659, 38802, 29659, 44108, 15158, 46746, 29659, 29659, 58811, 84422, 63150, \n",
      "29659, 71336, 29659, 119037, 29659, 1301, 29659, 3039, 29659, 35857, 46746, 29659, 29659, 110472, 29659, 117324, 104505, 29659, 46424, 29659, \n",
      "93136, 29659, 76910, 29659, 48848, 29659, 64103, 29659, 27541, 29659, 117248, 29659, 124759, 29659, 59675, 29659, 24487, 29659, 94043, 29659, \n",
      "83694, 15596, 84422, 29659, 109428, 64074, 29659, 56921, 22315, 30346, 29659, 56921, 80365, 30346, 29659, 56921, 24192, 30346, 29659, 56921, \n",
      "104419, 91062, 29659, 71553, 29659, 17132, 29659, 124063, 29659, 76394, 29659, 84521, 69254, 29659, 90338, 29659, 27541, 29659, 26822, 29659, \n",
      "81111, 29659, 69254, 29659, 15448, 29659, 91293, 29659, 51398, 29659, 24487, 29659, 88633, 84422, 46746, 46746, 46746, 29659, 29659, 83264, \n",
      "29659, 43417, 104505, 46746, 29659, 29659, 17434, 29659, 112045, 29659, 59675, 29659, 109428, 29659, 124590, 84422, 29659, 37143, 12508, 84422, \n",
      "113040, 100523, 29659, 131072, 82019, 54516, 29659, 120440, 105102, 29659, 14212, 54829, 88423, 54829, 4882, 73113, 82019, 29659, 83932, 54829, \n",
      "105102, 73113, 82019, 29659, 54516, 48275, 73743, 131072, 29659, 51398, 29659, 40071, 23354, 29659, 10303, 29659, 109428, 29659, 44717, 76330, \n",
      "29659, 59810, 54748, 46746, 29659, 29659, 17434, 29659, 112045, 29659, 59675, 29659, 902, 29659, 37143, 63150, 114854, 19473, 55972, 29659, \n",
      "83648, 84422, 46746, 46746, 46746, 29659, 29659, 95138, 68590, 34039, 104505, 29659, 122642, 100222, 23354, 29659, 12972, 29659, 109550, 29659, \n",
      "54695, 29659, 69450, 29659, 61355, 29659, 112143, 29659, 60727, 44068, 29659, 114210, 84422, 29659, 71553, 29659, 19910, 29659, 112143, 29659, \n",
      "36033, 29659, 6471, 29659, 172, 29659, 107159, 90001, 23391, 29659, 112143, 29659, 120894, 29659, 97864, 100765, 29659, 61086, 29659, 110466, \n",
      "29659, 115474, 29659, 82470, 29659, 92812, 29659, 1549, 84422, 46746, 46746, 46746, 29659, 29659, 47372, 11138, 114636, 104505, 29659, 47372, \n",
      "11138, 114636, 29659, 48848, 29659, 116263, 23354, 29659, 124759, 29659, 48848, 29659, 61302, 29659, 84010, 29659, 24487, 29659, 120047, 29659, \n",
      "96382, 29659, 59675, 29659, 24487, 29659, 39670, 29659, 64018, 29659, 24487, 29659, 122976, 29659, 61337, 84422, 29659, 30598, 29659, 59604, \n",
      "29659, 82470, 29659, 65208, 29659, 27541, 29659, 42626, 29659, 33584, 29659, 51398, 29659, 76394, 29659, 36915, 29659, 59864, 49957, 29659, \n",
      "69159, 23354, 29659, 65611, 29659, 24487, 29659, 39686, 29659, 64018, 29659, 59604, 29659, 48848, 29659, 51398, 29659, 76394, 29659, 119552, \n",
      "29659, 39670, 23354, 29659, 112143, 29659, 36064, 29659, 48848, 29659, 45370, 29659, 122942, 29659, 96251, 29659, 24487, 29659, 59864, 84422, \n",
      "29659, 105002, 29659, 59864, 29659, 71767, 29659, 125299, 29659, 42626, 29659, 120894, 29659, 64103, 23354, 29659, 37480, 29659, 76394, 29659, \n",
      "88633, 29659, 128169, 29659, 48848, 29659, 120375, 29659, 85624, 29659, 37143, 125880, 29659, 127635, 15158, 46746, 29659, 29659, 58811, 84422, \n",
      "19473, 29659, 29335, 41503, 29659, 119970, 29659, 74120, 29659, 1301, 29659, 75961, 102206, 46746, 29659, 29659, 58811, 84422, 19473, 84422, \n",
      "48368, 29659, 115029, 29659, 66496, 46746, 29659, 29659, 46424, 29659, 88633, 29659, 59675, 29659, 76394, 29659, 37143, 22315, 23354, 29659, \n",
      "104419, 23354, 29659, 84521, 69254, 55972, 29659, 19965, 84422, 29659, 71553, 29659, 42869, 29659, 11307, 29659, 76394, 29659, 131072, 121770, \n",
      "131072, 84422, 29659, 71553, 29659, 62791, 58866, 29659, 81638, 29659, 119792, 29659, 121770, 29659, 27541, 29659, 40362, 29659, 51398, 29659, \n",
      "65458, 29659, 94344, 29659, 118971, 29659, 51080, 29659, 51398, 29659, 76394, 29659, 63150, 104505, 48368, 29659, 18134, 107742, 103122, 84422, \n",
      "29659, 85315, 23354, 29659, 97661, 29659, 17132, 29659, 59675, 29659, 104188, 104505, 46746, 29659, 29659, 108962, 29659, 76394, 29659, 121770, \n",
      "29659, 59675, 29659, 26845, 29659, 48368, 29659, 88633, 23354, 29659, 97661, 29659, 120375, 29659, 120552, 29659, 124759, 29659, 10303, 29659, \n",
      "111461, 36937, 29659, 29543, 29659, 27541, 29659, 94344, 29659, 112143, 29659, 51080, 84422, 46746, 29659, 29659, 108962, 29659, 76394, 29659, \n",
      "121770, 29659, 59675, 29659, 58811, 29659, 112143, 29659, 38802, 29659, 1025, 23354, 29659, 97661, 29659, 68608, 29659, 76394, 29659, 63150, \n",
      "104505, 48368, 29659, 18134, 107742, 103122, 84422, 29659, 31869, 29659, 110466, 29659, 84010, 29659, 72891, 29659, 76394, 29659, 121770, 29659, \n",
      "10303, 29659, 80502, 29659, 51398, 29659, 51080, 23354, 29659, 97661, 29659, 37530, 29659, 100751, 29659, 54359, 29659, 113299, 29659, 88633, \n",
      "29659, 119970, 29659, 51080, 84422, 46746, 29659, 29659, 27750, 23354, 29659, 97661, 29659, 65426, 29659, 116826, 29659, 10303, 29659, 37604, \n",
      "48368, 23354, 69317, 29659, 94344, 29659, 1025, 29659, 118971, 29659, 37604, 11999, 29659, 51080, 29659, 1025, 84422, 46746, 29659, 29659, \n",
      "17902, 104505, 29659, 64548, 29659, 85382, 29659, 1087, 29659, 64018, 29659, 24487, 29659, 130013, 29659, 71767, 29659, 42626, 29659, 24487, \n",
      "29659, 106947, 29659, 64018, 29659, 902, 29659, 67091, 29659, 64856, 29659, 51398, 29659, 119792, 29659, 121770, 84422, 29659, 2153, 29659, \n",
      "79790, 29659, 76394, 29659, 41606, 29659, 38802, 29659, 122942, 23354, 29659, 3339, 29659, 83685, 29659, 118971, 29659, 76394, 29659, 14823, \n",
      "29659, 55153, 84422, 46746, 29659, 29659, 58811, 84422, 19473, 84422, 58811, 29659, 43634, 29659, 60677, 46746, 29659, 29659, 71553, 29659, \n",
      "45382, 29659, 85185, 29659, 6471, 29659, 27541, 29659, 117248, 29659, 109428, 29659, 62472, 29659, 104419, 90001, 22315, 29659, 1322, 29659, \n",
      "48848, 29659, 43982, 84422, 29659, 21801, 29659, 88932, 29659, 64018, 29659, 24487, 29659, 72016, 29659, 130013, 29659, 124461, 29659, 42626, \n",
      "29659, 65208, 29659, 51398, 29659, 24487, 29659, 130013, 29659, 88932, 29659, 24337, 6458, 84422, 46746, 29659, 29659, 58811, 84422, 75401, \n",
      "29659, 43867, 29659, 117220, 29659, 1301, 29659, 5839, 46746, 29659, 29659, 123425, 104505, 29659, 48368, 23354, 64360, 29659, 72016, 84422, \n",
      "46746, 29659, 29659, 74120, 104505, 29659, 37604, 48368, 23354, 69317, 84422, 46746, 29659, 29659, 75961, 102206, 104505, 29659, 37604, 11999, \n",
      "84422, 46746, 29659, 29659, 110472, 104505, 29659, 58061, 29659, 76631, 29659, 98708, 29659, 10303, 29659, 73790, 29659, 119745, 109428, 23354, \n",
      "29659, 22315, 23354, 29659, 80365, 23354, 29659, 24192, 23354, 29659, 104419, 23354, 29659, 84521, 69254, 84422, 29659, 83405, 29659, 3241, \n",
      "29659, 123538, 29659, 79333, 92109, 29659, 34713, 76480, 29659, 100751, 29659, 113489, 95769, 90001, 81009, 62012, 125160, 127043, 85152, 90001, \n",
      "124590, 90001, 14892, 90001, 89431, 100216, 90001, 58811, 84422, 80502, 90001, 81652, 90001, 73258, 90001, 80502, 48368, 90001, 8219, 90001, \n",
      "4269, 90001, 124761, 84422, 46746, 29659, 29659, 17902, 104505, 29659, 28982, 29659, 93136, 23354, 29659, 124759, 29659, 18549, 29659, 125299, \n",
      "29659, 73326, 29659, 36704, 29659, 95449, 29659, 62630, 29659, 51398, 29659, 64913, 29659, 112143, 29659, 104419, 90001, 61461, 29659, 124078, \n",
      "84422, 29659, 85315, 23354, 29659, 124759, 131072, 82505, 29659, 21528, 29659, 96956, 29659, 81638, 29659, 76394, 29659, 117022, 29659, 115428, \n",
      "29659, 100751, 29659, 24487, 29659, 103382, 29659, 46425, 84422, 46746, 46746, 46746, 46746, 29659, 29659, 58811, 84422, 105010, 29659, 55572, \n",
      "29659, 15853, 29659, 51398, 29659, 24487, 29659, 117220, 29659, 37143, 64922, 99687, 29659, 31308, 29659, 115847, 55972, 46746, 29659, 29659, \n",
      "14778, 29659, 21055, 29659, 127364, 29659, 50680, 60217, 46746, 29659, 29659, 21801, 29659, 55153, 29659, 38058, 29659, 80644, 29659, 16821, \n",
      "29659, 27541, 29659, 71964, 90001, 7556, 29659, 59864, 29659, 37143, 69556, 29659, 37604, 127635, 29659, 84564, 15158, 29659, 60512, 23354, \n",
      "29659, 84219, 29659, 63235, 29659, 59864, 29659, 37143, 26178, 114854, 102735, 8824, 29659, 84564, 55972, 29659, 92171, 29659, 73098, 107969, \n",
      "84422, 46746, 46746, 46746, 29659, 29659, 91877, 29659, 96586, 29659, 118519, 29659, 20332, 46746, 29659, 29659, 80217, 29659, 27297, 29659, \n",
      "72110, 29659, 40362, 29659, 51398, 29659, 24487, 29659, 55153, 23354, 29659, 115474, 29659, 92171, 29659, 125299, 29659, 32836, 100222, 29659, \n",
      "115079, 29659, 37143, 97713, 84422, 113040, 100523, 29659, 58811, 84422, 75401, 29659, 37480, 29659, 107373, 29659, 58811, 84422, 85246, 23354, \n",
      "29659, 63150, 84422, 82660, 70283, 23354, 29659, 101053, 48464, 29659, 125366, 29659, 52669, 29659, 76120, 61343, 29659, 35935, 29659, 27541, \n",
      "29659, 24487, 29659, 96382, 29659, 81680, 13546, 29659, 38802, 29659, 59827, 29659, 27297, 29659, 71390, 84422, 46746, 46746, 46746, 29659, \n",
      "29659, 51410, 29659, 51894, 29659, 33049, 46746, 29659, 29659, 33049, 29659, 90575, 29659, 131072, 58811, 29659, 100694, 125436, 29659, 79199, \n",
      "29659, 21253, 29659, 126885, 91544, 131072, 29659, 92171, 29659, 124760, 84422, 29659, 123978, 29659, 14357, 29659, 80025, 29659, 27541, 29659, \n",
      "38802, 29659, 77224, 29659, 46931, 29659, 37143, 18646, 125160, 61210, 125160, 58074, 58074, 23354, 29659, 18646, 90001, 61210, 90001, 58074, \n",
      "58074, 23354, 29659, 129262, 29659, 24495, 29659, 53876, 29659, 51398, 29659, 22054, 23354, 29659, 101053, 48464, 29659, 27750, 23354, 29659, \n",
      "24487, 29659, 96382, 29659, 18549, 29659, 52567, 29659, 27541, 29659, 108194, 29659, 112143, 29659, 24505, 29659, 130036, 29659, 39904, 29659, \n",
      "51398, 29659, 42026, 41969, 88372, 29659, 112143, 29659, 93273, 90001, 26311, 23074, 124336, 29659, 56381, 84422, 46746, 46746, 46746, 29659, \n",
      "29659, 65805, 29659, 112143, 29659, 116860, 103688, 29659, 95510, 24337, 29659, 105072, 46746, 29659, 29659, 51410, 29659, 112143, 29659, 104419, \n",
      "90001, 61461, 29659, 11193, 29659, 37143, 97713, 84422, 113040, 100523, 29659, 39133, 122016, 23354, 29659, 15009, 56193, 125160, 80877, 55972, \n",
      "29659, 92171, 29659, 125299, 29659, 43246, 90001, 115079, 84422, 29659, 21801, 29659, 55153, 29659, 29246, 29659, 123538, 29659, 38802, 29659, \n",
      "122803, 29659, 11193, 29659, 37143, 59617, 113040, 23354, 29659, 27987, 23354, 29659, 91860, 125160, 58643, 23354, 29659, 101053, 19698, 29659, \n",
      "31384, 29659, 24487, 29659, 96382, 29659, 95769, 29659, 5142, 22670, 29659, 112143, 29659, 129637, 29659, 71390, 29659, 81638, 29659, 95879, \n",
      "29659, 59827, 23354, 29659, 90807, 29659, 87644, 29659, 11193, 84422, 46746, 46746, 46746, 29659, 29659, 129289, 84283, 29659, 47198, 29659, \n",
      "96735, 46746, 29659, 29659, 15836, 29659, 14357, 29659, 108829, 29659, 82722, 29659, 63673, 29659, 112143, 29659, 130809, 29659, 44757, 29659, \n",
      "72110, 84422, 29659, 21055, 29659, 101457, 29659, 72110, 29659, 112143, 29659, 57687, 29659, 44717, 76330, 29659, 34686, 29659, 37143, 97713, \n",
      "84422, 113040, 100523, 29659, 128700, 29659, 83391, 84422, 29659, 85246, 55972, 29659, 40362, 29659, 26845, 29659, 108837, 84422, 29659, 125366, \n",
      "29659, 124461, 29659, 64170, 29659, 24487, 29659, 96382, 29659, 27541, 29659, 11961, 109290, 29659, 83324, 90001, 17329, 29659, 34686, 23354, \n",
      "29659, 5783, 29659, 76394, 29659, 39024, 29659, 51398, 29659, 44717, 76330, 29659, 68946, 29659, 97394, 29659, 24487, 29659, 24550, 29659, \n",
      "130013, 84422, 46746, 46746, 46746, 29659, 29659, 129289, 84283, 29659, 69608, 29659, 64018, 29659, 14357, 46746, 29659, 29659, 80217, 29659, \n",
      "24487, 29659, 55153, 29659, 48848, 29659, 1199, 29659, 97079, 23354, 29659, 21528, 29659, 76394, 29659, 14823, 29659, 55153, 29659, 103608, \n",
      "29659, 51982, 29659, 119552, 29659, 66084, 29659, 29036, 23354, 29659, 90575, 29659, 37014, 29659, 35389, 29659, 29036, 23354, 29659, 902, \n",
      "29659, 14357, 29659, 81638, 29659, 119792, 29659, 121770, 84422, 29659, 16633, 29659, 122942, 29659, 129806, 29659, 22315, 23354, 29659, 14823, \n",
      "29659, 59864, 23354, 29659, 59864, 29659, 10303, 29659, 27297, 82505, 29659, 118971, 29659, 14823, 29659, 44717, 76330, 29659, 16216, 23354, \n",
      "29659, 101053, 84422, 29659, 116828, 29659, 39285, 29659, 53687, 29659, 76394, 29659, 46730, 29659, 45111, 29659, 81638, 29659, 24550, 84422, \n",
      "29659, 84895, 29659, 90575, 29659, 37604, 838, 29659, 125944, 29659, 109550, 29659, 92559, 29659, 24487, 29659, 96382, 29659, 27541, 29659, \n",
      "75977, 29659, 120494, 84422, 46746, 46746, 46746, 46746, 46746, 46746, 46746, 29659, 29659, 63150, 84422, 29659, 31156, 29659, 51329, 98460, \n",
      "29659, 27541, 29659, 117538, 29659, 111880, 45108, 46746, 29659, 29659, 71553, 29659, 68608, 29659, 112949, 29659, 78269, 29659, 27541, 29659, \n",
      "118368, 29659, 80365, 29659, 34424, 29659, 109783, 29659, 24192, 29659, 81638, 29659, 24487, 29659, 81882, 29659, 55153, 84422, 29659, 59421, \n",
      "23354, 29659, 119792, 29659, 44405, 29659, 63618, 29659, 51398, 29659, 61087, 84422, 46746, 29659, 29659, 63150, 84422, 48368, 29659, 35753, \n",
      "51175, 29659, 37143, 42653, 90001, 48093, 55972, 29659, 90686, 46746, 29659, 29659, 63150, 84422, 48368, 84422, 48368, 29659, 123769, 29659, \n",
      "101055, 46746, 29659, 29659, 125366, 29659, 121913, 29659, 38032, 29659, 76394, 29659, 92170, 29659, 101457, 29659, 22315, 29659, 96382, 23354, \n",
      "29659, 97713, 84422, 113040, 100523, 29659, 11867, 5226, 79390, 125160, 60422, 90001, 24672, 28518, 90001, 63150, 84422, 48368, 90001, 1472, \n",
      "23354, 29659, 127043, 85152, 88579, 125160, 127043, 85152, 90001, 48368, 23354, 29659, 118971, 29659, 47333, 49525, 125160, 47333, 88392, 84422, \n",
      "75401, 90001, 13541, 84422, 29659, 71553, 29659, 52180, 29659, 116826, 29659, 76394, 29659, 61337, 90001, 90575, 29659, 6292, 104505, 46746, \n",
      "29659, 29659, 71553, 29659, 119167, 29659, 24487, 29659, 96382, 29659, 51398, 29659, 76394, 29659, 113669, 90001, 123925, 29659, 57243, 29659, \n",
      "24560, 29659, 97661, 29659, 82470, 29659, 29396, 29659, 24487, 29659, 45111, 29659, 96382, 29659, 58216, 84422, 29659, 71553, 29659, 33813, \n",
      "29659, 76394, 29659, 3229, 90001, 44833, 90001, 90575, 29659, 6848, 84422, 29659, 11245, 50537, 29659, 24487, 29659, 118702, 90001, 118971, \n",
      "90001, 68718, 29659, 96251, 29659, 24487, 29659, 54929, 29659, 109033, 29659, 91412, 84422, 29659, 125366, 29659, 41946, 104505, 46746, 29659, \n",
      "29659, 21801, 29659, 109033, 29659, 17730, 29659, 101662, 29659, 24487, 29659, 46425, 46746, 29659, 29659, 21801, 29659, 109033, 29659, 17730, \n",
      "29659, 68508, 29659, 78208, 29659, 114916, 46746, 29659, 29659, 21801, 29659, 12164, 29659, 17730, 29659, 68508, 29659, 76394, 29659, 96378, \n",
      "29659, 64018, 29659, 128095, 29659, 115474, 29659, 4329, 46746, 29659, 29659, 21801, 29659, 109033, 29659, 17730, 29659, 58662, 29659, 27541, \n",
      "29659, 24505, 29659, 76394, 29659, 66084, 46746, 29659, 29659, 21801, 29659, 12164, 29659, 17730, 29659, 68508, 29659, 76394, 29659, 63214, \n",
      "29659, 64018, 29659, 24487, 29659, 66084, 46746, 29659, 29659, 125366, 29659, 112176, 29659, 66408, 29659, 33584, 29659, 37143, 38453, 90001, \n",
      "65111, 29659, 23391, 55972, 46746, 29659, 29659, 71553, 29659, 16699, 29659, 80365, 29659, 24487, 29659, 66084, 46746, 29659, 29659, 71553, \n",
      "29659, 88820, 29659, 24487, 29659, 22315, 29659, 64018, 29659, 24487, 29659, 66084, 29659, 37143, 29396, 29659, 78208, 29659, 45813, 29659, \n",
      "22315, 29659, 66587, 29659, 90001, 29659, 51398, 29659, 45813, 29659, 128878, 55972, 29659, 118971, 29659, 82722, 29659, 24487, 29659, 22315, \n",
      "131072, 82505, 29659, 61461, 29659, 6848, 29659, 27541, 29659, 20806, 29659, 24487, 29659, 66084, 29659, 81638, 29659, 112404, 84422, 46746, \n",
      "29659, 29659, 21801, 29659, 96382, 29659, 9510, 29659, 27541, 29659, 54695, 29659, 78208, 29659, 71750, 23354, 29659, 69710, 29659, 112404, \n",
      "84422, 46746, 29659, 29659, 17902, 104505, 29659, 21801, 29659, 22315, 29659, 66587, 29659, 48848, 29659, 107734, 29659, 27541, 29659, 82722, \n",
      "29659, 24487, 29659, 3085, 29659, 32868, 90001, 28030, 29659, 6848, 29659, 51398, 29659, 24487, 29659, 81882, 29659, 22315, 29659, 64018, \n",
      "29659, 24487, 29659, 80365, 29659, 114210, 23354, 29659, 24560, 29659, 24487, 29659, 96382, 29659, 48848, 29659, 106915, 29659, 76394, 29659, \n",
      "33402, 29659, 64018, 29659, 4998, 29659, 10303, 29659, 81893, 29659, 110466, 29659, 24487, 29659, 6848, 29659, 41946, 29659, 902, 29659, \n",
      "103028, 29659, 37143, 63674, 84422, 97713, 84422, 29659, 114894, 29659, 51398, 29659, 113299, 29659, 22315, 23354, 29659, 14357, 29659, 51398, \n",
      "29659, 39442, 29659, 118971, 29659, 11241, 29659, 66084, 29659, 51398, 29659, 76394, 29659, 114131, 29659, 113299, 15158, 29659, 83405, 23354, \n",
      "29659, 24560, 29659, 11307, 29659, 121913, 29659, 48848, 29659, 26845, 29659, 107314, 29659, 118971, 29659, 26332, 29659, 71066, 23354, 29659, \n",
      "26845, 29659, 40071, 29659, 14357, 29659, 82470, 29659, 43879, 29659, 37143, 24560, 29659, 97661, 29659, 59675, 29659, 26845, 29659, 43071, \n",
      "29659, 24487, 29659, 40071, 29659, 6848, 15158, 29659, 108349, 23354, 29659, 110466, 29659, 24487, 29659, 6848, 29659, 44405, 29659, 28030, \n",
      "29659, 27541, 29659, 39442, 29659, 103028, 29659, 118971, 29659, 3085, 23354, 29659, 24487, 29659, 99481, 29659, 124461, 29659, 17132, 29659, \n",
      "42626, 29659, 43879, 29659, 123538, 29659, 39442, 29659, 22315, 29659, 1025, 84422, 29659, 40071, 29659, 48848, 29659, 49224, 29659, 73747, \n",
      "29659, 81638, 29659, 67606, 29659, 118971, 29659, 34162, 29659, 58598, 23354, 29659, 118971, 29659, 24560, 29659, 11307, 29659, 121913, 29659, \n",
      "48848, 29659, 125299, 29659, 57061, 23354, 29659, 39442, 29659, 22315, 29659, 1025, 29659, 82470, 29659, 125299, 29659, 58999, 84422, 46746, \n",
      "29659, 29659, 63150, 84422, 48368, 84422, 58811, 29659, 111103, 114943, 29659, 1301, 29659, 120875, 46746, 29659, 29659, 52829, 90001, 6237, \n",
      "29659, 112143, 29659, 94545, 29659, 103379, 104505, 46746, 29659, 29659, 21801, 29659, 96382, 29659, 110283, 29659, 82444, 121924, 29659, 13593, \n",
      "29659, 55332, 29659, 112143, 29659, 89128, 29659, 59864, 23354, 29659, 29974, 29659, 124759, 29659, 115323, 29659, 24487, 29659, 114210, 29659, \n",
      "64018, 29659, 24487, 29659, 44833, 84422, 46746, 29659, 29659, 81889, 29659, 124759, 29659, 33584, 29659, 24487, 29659, 103652, 131072, 82505, \n",
      "29659, 87501, 29659, 30247, 29659, 64018, 29659, 41349, 29659, 76394, 29659, 69710, 29659, 11241, 29659, 35804, 84422, 46746, 46746, 29659, \n",
      "29659, 56203, 29659, 47198, 29659, 40726, 11526, 104505, 46746, 29659, 29659, 12508, 84422, 113040, 100523, 29659, 19215, 129713, 29659, 88590, \n",
      "29659, 129713, 29659, 5033, 29659, 112143, 29659, 114147, 65022, 29659, 116826, 29659, 24487, 29659, 54225, 90001, 77197, 29659, 35389, 29659, \n",
      "81638, 29659, 111384, 29659, 103028, 84422, 46746, 29659, 29659, 44515, 29659, 96781, 29659, 24550, 29659, 48848, 29659, 119381, 23354, 29659, \n",
      "31384, 29659, 124759, 131072, 82505, 29659, 129560, 118916, 84422, 46746, 46746, 46746, 29659, 29659, 129289, 105986, 29659, 67281, 29659, 66911, \n",
      "104505, 46746, 29659, 29659, 129289, 29659, 49746, 29659, 112143, 29659, 51876, 31889, 23354, 29659, 24487, 29659, 96382, 29659, 18549, 29659, \n",
      "24508, 29659, 27541, 29659, 26311, 123531, 62769, 29659, 112143, 29659, 13593, 29659, 71390, 29659, 51398, 29659, 108867, 84422, 46746, 29659, \n",
      "29659, 69977, 29659, 51398, 29659, 92741, 29659, 48848, 29659, 72713, 29659, 55641, 29659, 113299, 29659, 6848, 29659, 27541, 29659, 110613, \n",
      "84422, 46746, 29659, 29659, 63150, 84422, 48368, 84422, 63150, 29659, 31308, 29659, 92741, 29659, 118971, 29659, 52170, 94056, 29659, 82722, \n",
      "46746, 29659, 29659, 24672, 28518, 29659, 26332, 29659, 91236, 29659, 43246, 29659, 30466, 29659, 27541, 29659, 47333, 49525, 29659, 118971, \n",
      "29659, 123713, 85152, 84422, 29659, 73596, 29659, 24487, 29659, 76384, 23354, 29659, 124759, 29659, 101422, 29659, 96230, 29659, 24487, 29659, \n",
      "83648, 29659, 27541, 29659, 59604, 29659, 51398, 29659, 108867, 29659, 118971, 29659, 101422, 29659, 79235, 29659, 27541, 29659, 99343, 29659, \n",
      "72110, 29659, 27541, 29659, 59604, 84422, 29659, 2153, 23354, 29659, 78208, 29659, 16683, 29659, 68382, 29659, 52180, 29659, 64018, 29659, \n",
      "122428, 29659, 48848, 29659, 3085, 23354, 29659, 81111, 29659, 1388, 29659, 24487, 29659, 96382, 131072, 82505, 29659, 89128, 29659, 91701, \n",
      "29659, 118971, 29659, 100751, 29659, 119792, 29659, 6319, 23354, 29659, 119970, 29659, 49762, 29659, 122428, 29659, 25108, 29659, 123538, 29659, \n",
      "24487, 29659, 3229, 90001, 44833, 29659, 87235, 23354, 29659, 64050, 114830, 29659, 123538, 29659, 24487, 29659, 120566, 29659, 64018, 29659, \n",
      "108867, 29659, 59604, 29659, 118971, 29659, 114676, 29659, 64018, 29659, 72110, 29659, 27541, 29659, 59604, 84422, 29659, 125366, 29659, 68443, \n",
      "29659, 47333, 49525, 29659, 118971, 29659, 123713, 85152, 16578, 82505, 29659, 92741, 29659, 1199, 29659, 43246, 84422, 46746, 46746, 29659, \n",
      "29659, 105002, 29659, 24487, 29659, 122428, 29659, 81638, 29659, 11307, 29659, 40740, 29659, 124461, 29659, 42626, 29659, 65208, 29659, 51398, \n",
      "29659, 24487, 29659, 35753, 51175, 29659, 6848, 29659, 24337, 6458, 84422, 46746, 29659, 29659, 88261, 104505, 29659, 21801, 29659, 52170, \n",
      "94056, 29659, 128878, 29659, 44405, 29659, 24487, 29659, 114349, 29659, 27541, 29659, 52180, 29659, 116826, 29659, 37480, 29659, 53885, 29659, \n",
      "24487, 29659, 27853, 29659, 25635, 29659, 51398, 29659, 71869, 748, 29659, 122262, 94252, 29659, 38453, 90001, 80898, 64143, 29659, 44717, \n",
      "76330, 29659, 130013, 84422, 29659, 10347, 131072, 82505, 29659, 76394, 29659, 47308, 118702, 29659, 121913, 29659, 110466, 29659, 113299, 29659, \n",
      "124461, 131072, 24337, 29659, 112143, 29659, 37636, 131072, 24337, 29659, 93894, 29659, 113408, 90001, 85363, 29659, 112143, 29659, 121041, 29659, \n",
      "76394, 29659, 128237, 90001, 25108, 29659, 75193, 23354, 29659, 37480, 29659, 124759, 29659, 44405, 29659, 125299, 29659, 25641, 29659, 81638, \n",
      "29659, 55706, 29659, 62630, 84422, 46746, 29659, 29659, 63150, 84422, 58811, 29659, 42765, 76330, 29659, 37143, 74803, 90001, 48093, 55972, \n",
      "29659, 90686, 29659, 46746, 29659, 29659, 63150, 84422, 58811, 84422, 48368, 29659, 51015, 90001, 69109, 29659, 37554, 46746, 29659, 29659, \n",
      "119281, 29659, 1301, 29659, 4653, 29659, 131064, 46746, 29659, 29659, 71553, 29659, 117022, 29659, 126790, 29659, 81111, 29659, 18456, 76330, \n",
      "29659, 22315, 29659, 37143, 40071, 23354, 29659, 49746, 23354, 29659, 43446, 23354, 29659, 101053, 54748, 29659, 97661, 131072, 107159, 29659, \n",
      "3852, 29659, 10303, 23354, 29659, 108829, 29659, 9823, 29659, 44833, 29659, 81658, 29659, 53310, 29659, 112143, 29659, 22704, 29659, 74504, \n",
      "108918, 84422, 46746, 46746, 46746, 29659, 29659, 51894, 29659, 114676, 46746, 29659, 29659, 54240, 29659, 92171, 29659, 94074, 29659, 172, \n",
      "29659, 120523, 29659, 87534, 29659, 51398, 29659, 24487, 29659, 66084, 46746, 29659, 29659, 83653, 29659, 24487, 29659, 46931, 29659, 64018, \n",
      "29659, 6471, 29659, 130036, 29659, 92171, 29659, 94074, 23354, 29659, 118971, 29659, 76394, 29659, 52354, 29659, 44405, 29659, 83336, 29659, \n",
      "41431, 29659, 84010, 29659, 124759, 29659, 44405, 29659, 76394, 29659, 35389, 29659, 124251, 23354, 29659, 27541, 29659, 42626, 29659, 96230, \n",
      "29659, 27541, 29659, 59604, 29659, 51398, 29659, 76394, 29659, 119552, 29659, 57243, 29659, 37143, 75532, 55972, 46746, 46746, 46746, 29659, \n",
      "29659, 92862, 29659, 1301, 29659, 81436, 29659, 105179, 20028, 46746, 29659, 29659, 71553, 29659, 21341, 29659, 62957, 23354, 29659, 96781, \n",
      "29659, 12078, 16031, 29659, 81638, 29659, 130036, 23354, 29659, 104998, 23354, 29659, 11193, 23354, 29659, 118971, 29659, 27297, 125160, 120047, \n",
      "29659, 72110, 84422, 46746, 29659, 29659, 46424, 29659, 35389, 29659, 114922, 29659, 44405, 29659, 104386, 29659, 112143, 29659, 131072, 62788, \n",
      "131072, 29659, 10303, 29659, 76394, 29659, 52354, 29659, 37143, 97713, 84422, 113040, 84422, 29659, 111037, 35389, 23354, 29659, 101053, 54748, \n",
      "29659, 57121, 29659, 97661, 29659, 95253, 29659, 24487, 29659, 66595, 29659, 108194, 29659, 130013, 29659, 51398, 29659, 41402, 29659, 60257, \n",
      "84422, 46746, 46746, 46746, 29659, 29659, 4653, 90001, 4214, 29659, 40137, 45279, 90001, 107529, 29659, 71589, 9775, 46746, 29659, 29659, \n",
      "73596, 29659, 119792, 29659, 52354, 23354, 29659, 97661, 29659, 4828, 29659, 116826, 29659, 24487, 29659, 71390, 29659, 51398, 29659, 24487, \n",
      "29659, 121306, 29659, 44833, 104505, 29659, 97713, 84422, 47747, 84422, 29659, 81638, 29659, 40071, 104505, 29659, 117880, 56921, 29659, 34424, \n",
      "29659, 56921, 111070, 117448, 60670, 116655, 30346, 29659, 27987, 29659, 34424, 29659, 56921, 88423, 112461, 60670, 11126, 90854, 73113, 116655, \n",
      "54829, 88423, 30346, 29659, 101053, 84422, 46746, 29659, 29659, 73596, 29659, 24487, 29659, 72110, 23354, 29659, 97661, 29659, 37782, 29659, \n",
      "76394, 29659, 72664, 85896, 59604, 29659, 37987, 29659, 55641, 29659, 76394, 29659, 32868, 90001, 45649, 29659, 110881, 29659, 27541, 29659, \n",
      "18849, 29659, 24487, 29659, 95098, 29659, 81638, 29659, 24487, 29659, 69608, 29659, 12064, 29659, 24487, 29659, 114676, 29659, 128718, 84422, \n",
      "46746, 46746, 46746, 29659, 29659, 45242, 90001, 105176, 46746, 29659, 29659, 74584, 29659, 51982, 29659, 54800, 49961, 29659, 92171, 29659, \n",
      "8930, 23354, 29659, 97661, 29659, 60434, 29659, 61086, 29659, 113299, 29659, 172, 29659, 113299, 29659, 51398, 29659, 24487, 29659, 114210, \n",
      "84422, 29659, 71553, 29659, 117248, 29659, 84010, 29659, 80373, 23354, 29659, 31917, 103770, 23354, 29659, 112143, 29659, 51051, 29659, 114210, \n",
      "29659, 44520, 29659, 44405, 29659, 84180, 84422, 46746, 46746, 46746, 29659, 29659, 110172, 46746, 29659, 29659, 71553, 29659, 76986, 29659, \n",
      "24487, 29659, 114210, 29659, 10303, 29659, 51982, 29659, 71390, 23354, 29659, 22272, 29659, 81638, 29659, 35913, 29659, 4956, 8930, 29659, \n",
      "56095, 29659, 112143, 29659, 108278, 29659, 13593, 29659, 114835, 84422, 46746, 29659, 29659, 59421, 23354, 29659, 97661, 29659, 46422, 29659, \n",
      "102041, 29659, 119792, 29659, 19671, 29659, 51398, 29659, 38802, 29659, 110542, 29659, 61087, 84422, 46746, 29659, 29659, 63150, 84422, 58811, \n",
      "84422, 58811, 29659, 4653, 29659, 1301, 29659, 119281, 29659, 122424, 46746, 29659, 29659, 123978, 29659, 64018, 29659, 24487, 29659, 54170, \n",
      "131072, 82505, 29659, 71390, 29659, 92171, 29659, 22315, 125160, 44833, 90001, 67631, 117569, 121287, 81638, 29659, 98804, 23354, 29659, 131072, \n",
      "64145, 131072, 29659, 51398, 29659, 40071, 29659, 44405, 29659, 131072, 111070, 117448, 60670, 116655, 23354, 131072, 29659, 37480, 29659, 51398, \n",
      "29659, 49746, 29659, 124759, 131072, 82505, 29659, 131072, 45657, 94076, 21187, 126014, 112334, 84422, 131072, 29659, 76115, 23354, 29659, 97661, \n",
      "29659, 92364, 29659, 62929, 29659, 81111, 29659, 22315, 29659, 97661, 131072, 107159, 29659, 3852, 29659, 10303, 84422, 46746, 29659, 29659, \n",
      "119281, 29659, 96735, 46746, 29659, 29659, 73596, 29659, 119792, 29659, 17730, 23354, 29659, 95166, 29659, 110466, 29659, 124759, 29659, 71958, \n",
      "29659, 51398, 29659, 76394, 29659, 106541, 29659, 72104, 29659, 37143, 97713, 84422, 113040, 84422, 29659, 36093, 97424, 107595, 104505, 29659, \n",
      "58793, 8824, 78659, 25071, 114854, 58793, 8824, 78659, 114865, 15158, 29659, 108962, 29659, 90525, 36937, 8824, 29659, 64018, 29659, 24487, \n",
      "29659, 114210, 131072, 82505, 29659, 91412, 29659, 92171, 29659, 51398, 29659, 24487, 29659, 36093, 97424, 107595, 29659, 81658, 23354, 29659, \n",
      "97661, 29659, 129560, 29659, 124759, 131072, 82505, 29659, 40071, 29659, 112143, 29659, 113369, 60469, 84422, 46746, 29659, 29659, 110906, 27363, \n",
      "37610, 29659, 72104, 104505, 29659, 123097, 29659, 34787, 23354, 29659, 101053, 84422, 46746, 29659, 29659, 64548, 29659, 44833, 29659, 128280, \n",
      "29659, 99872, 29659, 44405, 29659, 123661, 29659, 25108, 29659, 123538, 29659, 72016, 29659, 59604, 29659, 65208, 29659, 51398, 29659, 44833, \n",
      "29659, 126959, 29659, 83391, 29659, 44833, 29659, 37995, 84422, 46746, 46746, 46746, 29659, 29659, 102192, 128526, 103770, 29659, 37143, 110466, \n",
      "29659, 902, 29659, 103028, 29659, 127, 29659, 24487, 29659, 81882, 29659, 44833, 23354, 29659, 97713, 84422, 113040, 84422, 29659, 40071, \n",
      "29659, 83391, 84422, 29659, 113369, 60469, 55972, 46746, 29659, 29659, 113369, 60469, 29659, 64881, 29659, 27541, 29659, 82722, 29659, 76394, \n",
      "29659, 120495, 29659, 52180, 29659, 64018, 29659, 91412, 29659, 112143, 29659, 57772, 31909, 29659, 37143, 8212, 104505, 29659, 118825, 29659, \n",
      "112143, 29659, 38324, 127254, 125160, 6113, 56729, 55972, 29659, 46746, 29659, 29659, 45310, 29659, 40071, 29659, 2021, 29659, 41402, 29659, \n",
      "122803, 29659, 52180, 29659, 64018, 29659, 57772, 31909, 29659, 37143, 8212, 104505, 29659, 54829, 29659, 112143, 29659, 98627, 45108, 29659, \n",
      "115738, 55972, 46746, 29659, 29659, 45310, 23354, 29659, 75581, 29659, 115738, 29659, 39285, 29659, 119552, 29659, 73582, 29659, 51398, 29659, \n",
      "119792, 29659, 22315, 84422, 46746, 46746, 46746, 29659, 29659, 113646, 32150, 29659, 8099, 29659, 46746, 29659, 29659, 4653, 131072, 82505, \n",
      "29659, 44833, 90001, 25108, 29659, 19951, 29659, 92171, 29659, 38068, 29659, 123661, 29659, 25108, 29659, 123538, 29659, 44833, 29659, 88932, \n",
      "29659, 118971, 29659, 22315, 90001, 61461, 29659, 117635, 29659, 46746, 29659, 29659, 46424, 29659, 44833, 29659, 44405, 29659, 105953, 29659, \n",
      "44863, 29659, 27541, 29659, 76394, 29659, 22315, 49957, 29659, 237, 23354, 29659, 24487, 29659, 22315, 131072, 82505, 29659, 99872, 29659, \n",
      "44405, 29659, 38068, 29659, 87870, 84422, 46746, 29659, 29659, 108962, 29659, 24487, 29659, 69608, 29659, 64018, 29659, 103028, 29659, 29177, \n",
      "29659, 27541, 29659, 24487, 29659, 44833, 29659, 44405, 29659, 113299, 23354, 29659, 38852, 29659, 76394, 29659, 69710, 29659, 99872, 29659, \n",
      "44405, 29659, 38068, 29659, 120495, 29659, 81638, 29659, 24487, 29659, 18685, 29659, 64018, 29659, 24487, 29659, 91412, 29659, 64018, 29659, \n",
      "24487, 29659, 44833, 29659, 77197, 29659, 64018, 29659, 72016, 29659, 113863, 46746, 29659, 29659, 73596, 29659, 40071, 29659, 118971, 29659, \n",
      "8964, 60469, 23354, 29659, 97661, 29659, 18727, 29659, 10303, 29659, 76394, 29659, 45111, 29659, 99872, 29659, 64018, 29659, 111461, 29659, \n",
      "118971, 29659, 76394, 29659, 90423, 29659, 6552, 29659, 129806, 29659, 118306, 29659, 99872, 29659, 64018, 29659, 111461, 29659, 37143, 97661, \n",
      "29659, 95166, 29659, 81638, 29659, 63150, 29659, 18157, 23354, 29659, 54929, 29659, 18157, 29659, 77700, 99687, 29659, 119792, 29659, 22315, \n",
      "29659, 37143, 113299, 29659, 44405, 29659, 122803, 29659, 81638, 29659, 65458, 54433, 29659, 118971, 29659, 81638, 29659, 24487, 29659, 75902, \n",
      "29659, 64018, 29659, 119792, 29659, 118306, 23354, 29659, 97661, 29659, 10918, 29659, 75977, 29659, 18685, 29659, 75902, 23354, 29659, 118971, \n",
      "29659, 55669, 29659, 24487, 29659, 99872, 29659, 15367, 29659, 103732, 46746, 29659, 29659, 21597, 29659, 99872, 29659, 6552, 29659, 71767, \n",
      "29659, 42626, 29659, 37604, 127635, 29659, 37143, 81638, 29659, 20129, 29659, 128280, 29659, 51398, 29659, 54929, 29659, 77700, 99687, 29659, \n",
      "118971, 29659, 54271, 29659, 128280, 29659, 51398, 29659, 40997, 29659, 18157, 23354, 29659, 81111, 29659, 71767, 29659, 16699, 29659, 116826, \n",
      "29659, 24487, 29659, 120494, 29659, 22315, 29659, 99872, 29659, 27541, 29659, 42626, 29659, 26178, 15158, 29659, 31869, 29659, 6329, 23354, \n",
      "29659, 84010, 29659, 44405, 29659, 125299, 29659, 123097, 29659, 103839, 29659, 64018, 29659, 11961, 86488, 87729, 29659, 118971, 29659, 39442, \n",
      "29659, 22315, 90001, 25108, 29659, 91412, 29659, 64856, 29659, 51398, 29659, 24487, 29659, 66084, 23354, 29659, 4842, 29659, 24487, 29659, \n",
      "95075, 29659, 115251, 29659, 102041, 29659, 37143, 24560, 29659, 115474, 29659, 92171, 29659, 25108, 29659, 123538, 29659, 17730, 29659, 29543, \n",
      "55972, 29659, 112143, 29659, 34815, 29659, 24487, 29659, 40997, 29659, 115251, 84422, 46746, 29659, 29659, 108349, 23354, 29659, 76394, 29659, \n",
      "120494, 29659, 79472, 29659, 64018, 29659, 95236, 29659, 118971, 29659, 76394, 29659, 105680, 29659, 79472, 29659, 64018, 29659, 46320, 29659, \n",
      "44405, 29659, 52964, 29659, 27541, 29659, 38895, 29659, 24487, 29659, 19951, 29659, 55641, 29659, 66408, 29659, 105419, 84422, 46746, 46746, \n",
      "46746, 29659, 29659, 122490, 29659, 37306, 61343, 29659, 118971, 29659, 102192, 128526, 103770, 46746, 29659, 29659, 1590, 29659, 22315, 90001, \n",
      "25108, 29659, 18157, 23354, 29659, 97661, 29659, 4828, 29659, 81638, 29659, 76394, 29659, 38802, 29659, 100192, 29659, 121913, 29659, 64018, \n",
      "29659, 22315, 29659, 71231, 46746, 29659, 29659, 125366, 29659, 119250, 29659, 88169, 90001, 88988, 38077, 29659, 29543, 29659, 50247, 23354, \n",
      "29659, 122803, 29659, 95098, 29659, 83399, 29659, 15367, 29659, 129806, 29659, 22315, 23354, 29659, 118971, 29659, 60257, 90001, 25108, 29659, \n",
      "12381, 90001, 95098, 29659, 87534, 84422, 46746, 29659, 29659, 56850, 90001, 25108, 29659, 87534, 46746, 29659, 29659, 64548, 29659, 60257, \n",
      "29659, 64018, 29659, 119792, 29659, 22315, 29659, 44405, 29659, 33813, 29659, 25108, 29659, 123538, 29659, 59810, 29659, 110801, 29659, 123538, \n",
      "29659, 14196, 23354, 29659, 51398, 29659, 22817, 29659, 17392, 23354, 29659, 112143, 29659, 124759, 29659, 44405, 29659, 73747, 29659, 94513, \n",
      "29659, 29396, 29659, 12972, 29659, 37143, 81638, 29659, 100751, 29659, 54359, 29659, 127635, 90001, 23383, 29659, 97517, 29659, 122803, 29659, \n",
      "59604, 55972, 46746, 29659, 29659, 48093, 29659, 123538, 29659, 84010, 23354, 29659, 83684, 29659, 95098, 29659, 87534, 29659, 44405, 29659, \n",
      "26332, 29659, 118971, 29659, 85624, 29659, 81638, 29659, 24487, 29659, 84822, 29659, 64018, 29659, 60257, 29659, 12381, 29659, 59604, 29659, \n",
      "51398, 29659, 24487, 29659, 66084, 23354, 29659, 118971, 29659, 76394, 29659, 99872, 29659, 44405, 29659, 123661, 29659, 25108, 29659, 123538, \n",
      "29659, 24487, 29659, 18685, 29659, 64018, 29659, 122803, 29659, 59604, 29659, 51398, 29659, 119792, 29659, 22315, 84422, 46746, 29659, 29659, \n",
      "74584, 29659, 24487, 29659, 12381, 59604, 29659, 130013, 29659, 48848, 29659, 33813, 23354, 29659, 124759, 29659, 48848, 29659, 3085, 29659, \n",
      "51398, 29659, 84422, 504, 29659, 78072, 29659, 10303, 29659, 96446, 29659, 81638, 39854, 29659, 37143, 22315, 24153, 19028, 59604, 84422, \n",
      "504, 54433, 29659, 118971, 29659, 38852, 29659, 51982, 29659, 64018, 29659, 11307, 29659, 48848, 29659, 3085, 29659, 51398, 29659, 76394, \n",
      "29659, 125732, 29659, 17442, 29659, 16578, 12381, 26963, 20331, 128712, 84422, 125732, 647, 46746, 46746, 46746, 29659, 29659, 89164, 29659, \n",
      "107572, 90001, 25108, 29659, 87534, 46746, 29659, 29659, 71553, 29659, 124461, 29659, 4828, 29659, 81638, 29659, 122803, 29659, 59604, 29659, \n",
      "112143, 29659, 122055, 78985, 29659, 37143, 131072, 54516, 48275, 131072, 23354, 29659, 131072, 105102, 11126, 131072, 23354, 29659, 131072, 88423, \n",
      "5063, 131072, 29659, 81638, 29659, 40071, 29659, 83391, 84422, 29659, 131072, 42207, 54516, 118825, 131072, 23354, 29659, 131072, 54027, 14212, \n",
      "96423, 86882, 131072, 29659, 51398, 29659, 113369, 60469, 23354, 29659, 131072, 125254, 94076, 114273, 112334, 131072, 23354, 29659, 131072, 125254, \n",
      "27151, 131072, 23354, 29659, 131072, 5697, 45314, 113023, 112334, 131072, 29659, 51398, 29659, 49746, 23354, 29659, 101053, 48464, 46746, 29659, \n",
      "29659, 113649, 29659, 92171, 29659, 119552, 29659, 55641, 29659, 60257, 29659, 59604, 23354, 29659, 24560, 29659, 115474, 29659, 92171, 29659, \n",
      "1660, 29659, 100751, 29659, 81111, 29659, 59604, 29659, 82470, 29659, 64856, 29659, 55641, 29659, 81111, 29659, 22315, 29659, 118971, 29659, \n",
      "9098, 29659, 27541, 29659, 18849, 29659, 76394, 29659, 99872, 29659, 123538, 29659, 24487, 29659, 69608, 29659, 64018, 29659, 43574, 29659, \n",
      "59604, 29659, 129806, 29659, 22315, 29659, 125160, 29659, 72016, 29659, 59604, 29659, 25108, 29659, 123538, 29659, 127635, 29659, 112143, 29659, \n",
      "102735, 29659, 97517, 29659, 122803, 29659, 59604, 84422, 46746, 29659, 29659, 125366, 29659, 44405, 29659, 73747, 29659, 29396, 29659, 24487, \n",
      "29659, 83399, 23354, 29659, 81111, 29659, 92171, 29659, 27541, 29659, 42626, 29659, 23596, 29659, 65208, 29659, 108142, 23354, 29659, 118971, \n",
      "29659, 9098, 29659, 27541, 29659, 83882, 29659, 78208, 29659, 75902, 29659, 99872, 29659, 37143, 35954, 29659, 116023, 29659, 83399, 29659, \n",
      "124151, 55972, 29659, 129806, 29659, 22315, 29659, 12064, 29659, 83473, 29659, 124759, 29659, 172, 29659, 72016, 29659, 107572, 29659, 47917, \n",
      "84422, 46746, 46746, 46746, 29659, 29659, 56288, 78393, 29659, 25108, 29659, 87534, 46746, 29659, 29659, 56288, 78393, 29659, 44405, 29659, \n",
      "24487, 29659, 97239, 29659, 64018, 29659, 29396, 29659, 88169, 90001, 7556, 29659, 17730, 29659, 130032, 29659, 118971, 29659, 6619, 29659, \n",
      "76394, 29659, 68118, 76330, 29659, 114922, 29659, 123538, 29659, 25472, 29659, 64018, 29659, 124759, 84422, 46746, 29659, 29659, 125366, 29659, \n",
      "93799, 29659, 24487, 29659, 72016, 29659, 43574, 29659, 52180, 29659, 64018, 29659, 122803, 29659, 83399, 29659, 118971, 29659, 43574, 29659, \n",
      "12381, 29659, 59604, 29659, 37143, 55641, 29659, 24487, 29659, 60257, 55972, 29659, 84010, 29659, 92171, 29659, 122743, 29659, 70186, 29659, \n",
      "10303, 29659, 44946, 84422, 46746, 29659, 29659, 83653, 23354, 29659, 24487, 29659, 44946, 29659, 92171, 29659, 104386, 29659, 172, 29659, \n",
      "37143, 88169, 90001, 48368, 55972, 29659, 131072, 30647, 131072, 29659, 75241, 84422, 29659, 73596, 29659, 114916, 104505, 29659, 131072, 32277, \n",
      "29659, 128484, 29659, 38077, 29659, 113489, 95769, 131072, 29659, 81638, 29659, 88169, 31655, 63150, 29659, 44405, 29659, 131072, 32277, 118817, \n",
      "65081, 122622, 118817, 113489, 95769, 131072, 29659, 29974, 29659, 119792, 29659, 68935, 29659, 48848, 29659, 104386, 29659, 10303, 29659, 54929, \n",
      "29659, 84466, 29659, 69556, 115251, 29659, 131072, 118817, 131072, 84422, 29659, 46746, 29659, 29659, 125366, 29659, 75908, 29659, 28840, 29659, \n",
      "76394, 29659, 114210, 29659, 123538, 29659, 81111, 29659, 97661, 29659, 124461, 29659, 26770, 29659, 113583, 29659, 76394, 29659, 100713, 29659, \n",
      "64018, 29659, 7556, 23354, 29659, 118971, 29659, 81638, 29659, 119792, 29659, 43574, 29659, 100713, 23354, 29659, 97661, 29659, 124461, 29659, \n",
      "1986, 29659, 24487, 29659, 93975, 84422, 46746, 29659, 29659, 74584, 29659, 97661, 29659, 39285, 29659, 24487, 29659, 72016, 29659, 100713, \n",
      "29659, 93975, 29659, 118971, 29659, 24487, 29659, 95075, 29659, 15367, 29659, 64018, 29659, 24487, 29659, 43574, 29659, 23259, 23354, 29659, \n",
      "97661, 29659, 39285, 29659, 45649, 29659, 78208, 29659, 88169, 90001, 88988, 38077, 29659, 96382, 84422, 46746, 29659, 29659, 115217, 23354, \n",
      "29659, 97661, 29659, 11130, 29659, 27541, 29659, 93894, 29659, 24487, 29659, 81882, 29659, 58099, 29659, 10303, 29659, 24487, 29659, 80365, \n",
      "29659, 114210, 29659, 118971, 29659, 114922, 29659, 10303, 29659, 24487, 29659, 110801, 29659, 88169, 90001, 88988, 114612, 84422, 29659, 108962, \n",
      "29659, 76394, 29659, 114922, 29659, 44405, 29659, 64856, 23354, 29659, 97661, 29659, 1986, 29659, 24487, 29659, 22736, 29659, 29543, 29659, \n",
      "64018, 29659, 24487, 29659, 114922, 29659, 51398, 29659, 47633, 29659, 99872, 84422, 29659, 37929, 23354, 29659, 97661, 29659, 20579, 29659, \n",
      "76394, 29659, 22736, 29659, 64018, 29659, 114116, 90001, 109761, 29659, 37143, 76394, 29659, 94340, 29659, 79809, 29659, 69608, 29659, 30247, \n",
      "29659, 64018, 29659, 80502, 29659, 27541, 29659, 18863, 66458, 29659, 24487, 29659, 29543, 55972, 46746, 29659, 29659, 71553, 29659, 98250, \n",
      "29659, 24487, 29659, 99872, 29659, 172, 29659, 24487, 29659, 72016, 29659, 69608, 29659, 64018, 29659, 88169, 90001, 88988, 114612, 29659, \n",
      "65208, 29659, 51398, 29659, 24487, 29659, 22315, 29659, 27541, 29659, 18849, 29659, 78208, 29659, 126312, 29659, 99872, 29659, 81638, 29659, \n",
      "119792, 29659, 22315, 29659, 37143, 126312, 29659, 99872, 29659, 30247, 29659, 64018, 29659, 72016, 29659, 99872, 29659, 27541, 29659, 83882, \n",
      "29659, 24487, 29659, 52669, 29659, 3094, 29659, 64018, 29659, 119792, 29659, 22315, 29659, 118971, 29659, 125299, 29659, 73747, 29659, 24487, \n",
      "29659, 8132, 29659, 99992, 84422, 29659, 10347, 29659, 95769, 29659, 17132, 29659, 42626, 29659, 84010, 29659, 24487, 29659, 60257, 125160, \n",
      "107572, 29659, 3602, 29659, 64018, 29659, 76394, 29659, 22315, 29659, 48848, 29659, 14823, 23354, 29659, 237, 29659, 6619, 29659, 76394, \n",
      "29659, 74158, 29659, 21452, 29659, 114922, 29659, 64018, 29659, 24487, 29659, 88169, 90001, 88988, 114612, 29659, 81638, 29659, 24487, 29659, \n",
      "22315, 84422, 46746, 29659, 29659, 83653, 23354, 29659, 97661, 29659, 101928, 29659, 24487, 29659, 115251, 29659, 96251, 29659, 80502, 29659, \n",
      "118971, 29659, 48368, 29659, 172, 29659, 24487, 29659, 120494, 29659, 22315, 29659, 99872, 29659, 172, 29659, 78208, 29659, 72777, 29659, \n",
      "103382, 29659, 64221, 29659, 37143, 124083, 93866, 90001, 114903, 99872, 15158, 46746, 29659, 29659, 74584, 29659, 11307, 23354, 29659, 76394, \n",
      "29659, 11241, 29659, 29543, 29659, 99872, 29659, 44405, 29659, 123661, 29659, 172, 29659, 83473, 29659, 22315, 29659, 115251, 29659, 172, \n",
      "29659, 24487, 29659, 72016, 29659, 115251, 84422, 46746, 29659, 29659, 71553, 29659, 17132, 29659, 9777, 40846, 29659, 10303, 29659, 64335, \n",
      "29659, 39442, 29659, 77117, 11448, 29659, 109257, 29659, 103839, 29659, 64018, 29659, 24487, 29659, 116263, 29659, 19951, 29659, 118971, 29659, \n",
      "24487, 29659, 54614, 90001, 27541, 90001, 83920, 29659, 64758, 29659, 64018, 29659, 88169, 90001, 88988, 38077, 29659, 110585, 29659, 37143, \n",
      "37480, 29659, 51692, 29659, 58498, 29659, 10303, 29659, 11307, 55972, 46746, 29659, 29659, 73596, 29659, 24487, 29659, 80598, 29659, 64018, \n",
      "29659, 88169, 23354, 29659, 97661, 29659, 19422, 29659, 10303, 29659, 88169, 31655, 63150, 29659, 118971, 29659, 9777, 40846, 29659, 10303, \n",
      "29659, 88169, 31655, 58811, 29659, 118971, 29659, 88169, 31655, 19473, 29659, 118971, 29659, 65208, 29659, 88169, 31655, 19473, 29659, 27541, \n",
      "29659, 42626, 29659, 24487, 29659, 120769, 29659, 13831, 84422, 29659, 77062, 29659, 34686, 29659, 82470, 29659, 125299, 29659, 39202, 29659, \n",
      "24560, 29659, 115474, 29659, 39285, 29659, 91236, 29659, 76394, 29659, 72713, 29659, 29543, 29659, 64018, 29659, 78458, 29659, 119970, 29659, \n",
      "51208, 29659, 59604, 29659, 37143, 81111, 29659, 95769, 29659, 9915, 29659, 24487, 29659, 82722, 29659, 64630, 29659, 125299, 29659, 95542, \n",
      "15158, 46746, 29659, 29659, 17902, 104505, 29659, 108962, 29659, 97661, 29659, 129560, 29659, 43646, 23354, 29659, 71390, 29659, 18549, 29659, \n",
      "48813, 29659, 24487, 29659, 100106, 29659, 60257, 29659, 37143, 97713, 84422, 113040, 100523, 29659, 131072, 111070, 117448, 60670, 116655, 131072, \n",
      "29659, 83391, 84422, 29659, 131072, 111070, 54829, 60670, 116655, 131072, 54433, 29659, 37480, 29659, 108829, 29659, 24487, 29659, 87534, 29659, \n",
      "99481, 29659, 48848, 29659, 58397, 29659, 41293, 29659, 45370, 29659, 81638, 29659, 96686, 29659, 62472, 29659, 103028, 29659, 34572, 29659, \n",
      "37143, 49746, 23354, 29659, 43446, 23354, 29659, 1148, 33226, 23354, 29659, 101053, 48464, 46746, 29659, 29659, 63150, 84422, 58811, 84422, \n",
      "63150, 29659, 118519, 29659, 114676, 46746, 29659, 29659, 73596, 29659, 24487, 29659, 69608, 90001, 27541, 90001, 114210, 29659, 114676, 29659, \n",
      "97661, 29659, 94190, 104505, 46746, 29659, 29659, 71553, 29659, 82722, 29659, 76394, 29659, 32868, 90001, 45649, 29659, 110881, 29659, 37143, \n",
      "51398, 75482, 90001, 72664, 85896, 59604, 55972, 29659, 27541, 29659, 99343, 29659, 72110, 29659, 119970, 29659, 63622, 29659, 103028, 84422, \n",
      "46746, 29659, 29659, 71553, 29659, 25356, 29659, 45649, 29659, 78208, 29659, 68443, 29659, 72664, 85896, 59604, 29659, 37987, 29659, 66526, \n",
      "29659, 24487, 29659, 120008, 29659, 113299, 23354, 29659, 27541, 29659, 55669, 29659, 81638, 29659, 27297, 29659, 72110, 29659, 118971, 29659, \n",
      "35389, 90001, 25108, 29659, 69608, 99687, 29659, 76631, 29659, 43246, 84422, 46746, 29659, 29659, 96586, 29659, 72110, 29659, 73747, 29659, \n",
      "41946, 29659, 50338, 29659, 24487, 29659, 81455, 29659, 83336, 29659, 27541, 29659, 24487, 29659, 72110, 29659, 118971, 29659, 38852, 29659, \n",
      "51546, 41503, 29659, 63538, 29659, 24487, 29659, 51208, 29659, 46422, 23354, 29659, 69406, 29659, 61086, 29659, 119970, 29659, 72110, 29659, \n",
      "94859, 124759, 29659, 172, 29659, 94859, 124759, 29659, 30247, 29659, 64018, 29659, 24487, 29659, 120047, 29659, 69608, 29659, 100751, 29659, \n",
      "55148, 84422, 46746, 29659, 29659, 73596, 29659, 35389, 90001, 25108, 29659, 69608, 99687, 29659, 124759, 29659, 7752, 29659, 3852, 29659, \n",
      "10303, 29659, 103028, 29659, 84010, 29659, 93894, 29659, 71951, 29659, 61086, 29659, 37143, 40071, 23354, 29659, 116603, 63674, 23354, 29659, \n",
      "113369, 60469, 23354, 29659, 92388, 23354, 29659, 8261, 8893, 15151, 23354, 29659, 18296, 121158, 55972, 29659, 118971, 29659, 84010, 29659, \n",
      "93894, 29659, 125299, 29659, 37143, 49746, 23354, 29659, 43446, 23354, 29659, 1148, 33226, 23354, 29659, 99073, 19881, 55972, 29659, 20407, \n",
      "84422, 29659, 46746, 29659, 29659, 90586, 29659, 48848, 29659, 109428, 29659, 110316, 29659, 51398, 29659, 24487, 29659, 69608, 90001, 27541, \n",
      "90001, 114210, 29659, 114676, 29659, 123538, 29659, 24487, 29659, 124040, 29659, 52180, 84422, 46746, 29659, 29659, 73596, 29659, 24487, 29659, \n",
      "117022, 29659, 52180, 23354, 29659, 124759, 29659, 101273, 29659, 92662, 29659, 24487, 29659, 69608, 29659, 51398, 29659, 34004, 29659, 30247, \n",
      "29659, 64018, 29659, 119354, 29659, 37143, 110466, 29659, 24487, 29659, 34004, 131072, 29659, 80598, 29659, 48848, 29659, 30424, 29659, 41880, \n",
      "29659, 80502, 15158, 29659, 14653, 104505, 29659, 99484, 29659, 44405, 29659, 125299, 29659, 131072, 97713, 59617, 29659, 113195, 35231, 29659, \n",
      "86396, 29659, 31384, 29659, 56252, 131072, 29659, 37143, 113299, 29659, 8568, 29659, 72465, 29659, 114445, 29659, 118971, 29659, 1864, 55972, \n",
      "29659, 51398, 29659, 86964, 23354, 29659, 37480, 29659, 131072, 99108, 46344, 29659, 21193, 29659, 56252, 131072, 29659, 37143, 57324, 29659, \n",
      "114445, 29659, 118971, 29659, 1864, 15158, 29659, 79676, 29659, 24487, 29659, 19230, 23354, 29659, 110466, 29659, 24487, 29659, 114445, 29659, \n",
      "105902, 131072, 82505, 29659, 80598, 29659, 44405, 29659, 80502, 23354, 29659, 59908, 29659, 81638, 29659, 75981, 23354, 29659, 38852, 29659, \n",
      "24487, 29659, 114210, 29659, 44405, 29659, 131072, 93894, 29659, 113195, 35231, 29659, 56252, 131072, 29659, 37143, 54929, 29659, 8568, 29659, \n",
      "118971, 29659, 1864, 55972, 29659, 30247, 29659, 64018, 29659, 131072, 56252, 29659, 21193, 29659, 56252, 131072, 37143, 1864, 29659, 114445, \n",
      "29659, 118971, 29659, 1864, 15158, 29659, 46746, 46746, 29659, 29659, 63150, 84422, 58811, 84422, 19473, 29659, 51894, 29659, 76696, 29659, \n",
      "1301, 29659, 595, 46746, 29659, 29659, 129289, 29659, 24487, 29659, 45813, 29659, 128878, 23354, 29659, 130036, 29659, 92171, 29659, 60077, \n",
      "29659, 6370, 29659, 20407, 29659, 55641, 29659, 39442, 29659, 44717, 76330, 29659, 112143, 29659, 63075, 90001, 25108, 29659, 56095, 84422, \n",
      "29659, 99902, 29659, 64018, 29659, 5025, 29659, 61086, 29659, 10303, 29659, 69035, 29659, 44717, 76330, 29659, 84564, 29659, 38068, 23354, \n",
      "29659, 97661, 29659, 82722, 29659, 76394, 29659, 96781, 29659, 75193, 29659, 27541, 29659, 126776, 23354, 29659, 73768, 23354, 29659, 118971, \n",
      "29659, 24505, 29659, 35389, 29659, 43615, 29659, 119970, 29659, 76394, 29659, 77224, 29659, 129262, 29659, 81638, 39854, 23354, 29659, 82444, \n",
      "19226, 29659, 24487, 29659, 52354, 111037, 35389, 29659, 100751, 29659, 24487, 29659, 126869, 84422, 29659, 59421, 29659, 44405, 29659, 76394, \n",
      "29659, 120538, 29659, 4828, 29659, 100751, 29659, 35954, 29659, 124759, 29659, 110375, 23354, 29659, 51045, 75711, 29659, 24487, 29659, 67550, \n",
      "29659, 14648, 29659, 121041, 84422, 46746, 29659, 29659, 63150, 84422, 58811, 84422, 19473, 84422, 48368, 29659, 99653, 29659, 105978, 29659, \n",
      "118971, 29659, 37554, 46746, 29659, 29659, 92862, 29659, 122424, 46746, 29659, 29659, 21801, 29659, 121041, 29659, 38032, 29659, 76394, 29659, \n",
      "96156, 29659, 125621, 29659, 27541, 29659, 23217, 22670, 29659, 24242, 29659, 88646, 26088, 29659, 84010, 29659, 18549, 29659, 42626, 29659, \n",
      "76394, 29659, 35389, 84422, 29659, 64548, 29659, 92637, 29659, 120523, 29659, 44405, 29659, 81173, 29659, 90575, 104505, 46746, 29659, 29659, \n",
      "125366, 29659, 88579, 33641, 29659, 27541, 29659, 10918, 29659, 116826, 29659, 27541, 29659, 112949, 29659, 44717, 76330, 29659, 75538, 23354, \n",
      "29659, 128314, 29659, 99822, 29659, 172, 29659, 131072, 90001, 131072, 23354, 29659, 131072, 125160, 131072, 23354, 29659, 131072, 84422, 131072, \n",
      "23354, 29659, 112143, 29659, 131072, 23354, 131072, 23354, 29659, 10303, 29659, 20434, 29659, 107451, 68935, 29659, 51398, 29659, 96251, 84422, \n",
      "29659, 113649, 29659, 75538, 29659, 124461, 29659, 109884, 104505, 46746, 29659, 29659, 67103, 90001, 99992, 29659, 130036, 29659, 37143, 97713, \n",
      "84422, 113040, 100523, 29659, 61210, 125160, 58074, 23354, 29659, 18646, 125160, 61210, 23354, 29659, 61210, 125160, 18646, 23354, 29659, 101053, \n",
      "48464, 46746, 29659, 29659, 31156, 90001, 99992, 29659, 130036, 29659, 37143, 97713, 84422, 113040, 100523, 29659, 18646, 125160, 61210, 125160, \n",
      "58074, 37143, 58074, 54433, 29659, 58074, 58074, 125160, 61210, 125160, 18646, 23354, 29659, 101053, 48464, 46746, 46746, 46746, 29659, 29659, \n",
      "26952, 29659, 40697, 46746, 29659, 29659, 74584, 29659, 59703, 29659, 76394, 29659, 54239, 29659, 35389, 29659, 88646, 26088, 23354, 29659, \n",
      "97661, 29659, 8404, 29659, 124759, 29659, 90882, 29659, 3935, 29659, 22736, 76330, 104505, 46746, 29659, 29659, 71013, 105759, 75538, 37143, \n",
      "75538, 23354, 29659, 51051, 30647, 54239, 55972, 46746, 29659, 29659, 15957, 75977, 29659, 24487, 29659, 44717, 76330, 29659, 44629, 29659, \n",
      "119970, 29659, 58811, 29659, 112143, 29659, 63150, 29659, 67091, 29659, 118971, 29659, 53899, 11968, 29659, 27541, 29659, 65611, 29659, 71013, \n",
      "112949, 69897, 24183, 29659, 112143, 29659, 71013, 54929, 69897, 24183, 84422, 46746, 29659, 29659, 71013, 112949, 69897, 24183, 37143, 75538, \n",
      "55972, 46746, 29659, 29659, 127577, 29659, 123048, 94157, 29659, 90575, 29659, 18646, 125160, 61210, 125160, 58074, 23354, 29659, 61210, 125160, \n",
      "18646, 125160, 58074, 23354, 29659, 58074, 58074, 125160, 61210, 125160, 18646, 23354, 29659, 112143, 29659, 76394, 29659, 47308, 118702, 29659, \n",
      "64018, 29659, 58074, 125160, 61210, 125160, 18646, 84422, 29659, 10347, 29659, 27733, 29659, 84010, 29659, 119792, 29659, 20029, 23354, 29659, \n",
      "24495, 23354, 29659, 112143, 29659, 43502, 29659, 44405, 29659, 32263, 29659, 37143, 97713, 84422, 113040, 100523, 29659, 24495, 29659, 51398, \n",
      "29659, 48368, 93846, 24763, 23354, 29659, 20029, 29659, 33916, 29659, 46557, 29659, 51398, 29659, 84010, 29659, 24495, 15158, 46746, 29659, \n",
      "29659, 71013, 54929, 69897, 24183, 37143, 75538, 23354, 29659, 51051, 30647, 54239, 55972, 46746, 29659, 29659, 105002, 114822, 29659, 131072, \n",
      "127836, 90001, 94302, 131072, 29659, 46931, 29659, 37143, 97713, 84422, 113040, 100523, 29659, 93948, 125160, 75966, 29659, 112143, 29659, 107316, \n",
      "125160, 113868, 55972, 29659, 26845, 29659, 110466, 29659, 24487, 29659, 88646, 26088, 29659, 38032, 29659, 125160, 29659, 37143, 118971, 29659, \n",
      "125299, 29659, 90001, 29659, 112143, 29659, 84422, 29659, 24560, 29659, 48368, 84422, 58811, 29659, 95769, 29659, 31890, 29659, 24487, 29659, \n",
      "69608, 29659, 118971, 29659, 48368, 90001, 58811, 29659, 95769, 29659, 31890, 29659, 81173, 29659, 90575, 29659, 48368, 29659, 27541, 29659, \n",
      "58811, 15158, 29659, 125366, 29659, 37987, 29659, 102034, 29659, 18646, 125160, 61210, 23354, 29659, 38852, 29659, 61210, 125160, 18646, 23354, \n",
      "29659, 38852, 29659, 61210, 125160, 58074, 84422, 29659, 21801, 29659, 43502, 29659, 44405, 29659, 66020, 29659, 10303, 29659, 76394, 29659, \n",
      "16821, 90001, 43502, 29659, 128237, 29659, 65907, 29659, 46452, 29659, 105338, 29659, 53851, 8824, 116059, 23354, 29659, 112106, 29659, 89804, \n",
      "8824, 116059, 15158, 46746, 46746, 46746, 29659, 29659, 117324, 29659, 1301, 29659, 110472, 29659, 28180, 46746, 29659, 29659, 21801, 29659, \n",
      "7555, 29659, 53310, 29659, 119792, 29659, 75485, 29659, 27541, 29659, 117248, 29659, 84010, 29659, 24487, 29659, 20029, 29659, 44405, 29659, \n",
      "97394, 29659, 24487, 29659, 120494, 29659, 53927, 29659, 81638, 29659, 84010, 29659, 24495, 29659, 37143, 97569, 29659, 81638, 29659, 73457, \n",
      "29659, 93497, 29659, 110466, 29659, 24487, 29659, 43502, 29659, 44405, 29659, 50572, 29659, 106541, 15158, 29659, 108962, 29659, 109428, 29659, \n",
      "32263, 29659, 35389, 29659, 87235, 29659, 44405, 29659, 65208, 23354, 29659, 97661, 29659, 19296, 29659, 24487, 29659, 54239, 29659, 118971, \n",
      "29659, 35522, 29659, 124759, 29659, 76631, 90001, 44405, 84422, 46746, 46746, 46746, 29659, 29659, 58253, 29659, 27541, 29659, 103379, 64143, \n",
      "29659, 110472, 46746, 29659, 29659, 108962, 29659, 76394, 29659, 32263, 29659, 35389, 29659, 44405, 29659, 8930, 23354, 29659, 97661, 29659, \n",
      "38852, 104505, 46746, 29659, 29659, 58237, 29659, 129637, 29659, 24487, 29659, 20029, 29659, 112143, 29659, 43502, 29659, 110466, 29659, 115474, \n",
      "131072, 107159, 29659, 125299, 29659, 64856, 29659, 37143, 81638, 29659, 127836, 114854, 94302, 29659, 46931, 15158, 46746, 29659, 29659, 54177, \n",
      "29659, 22758, 29659, 55641, 29659, 24495, 29659, 69608, 29659, 37143, 48368, 114854, 24763, 55972, 29659, 27541, 29659, 76394, 29659, 24495, \n",
      "29659, 96446, 29659, 51398, 29659, 24487, 29659, 22315, 29659, 64018, 29659, 22710, 29659, 37143, 97713, 84422, 113040, 100523, 29659, 51398, \n",
      "29659, 40071, 23354, 29659, 63150, 29659, 34424, 29659, 131072, 88423, 54829, 116655, 73113, 99602, 131072, 15158, 46746, 29659, 29659, 104579, \n",
      "29659, 24487, 29659, 52354, 111037, 35389, 29659, 100751, 29659, 24487, 29659, 126869, 84422, 29659, 12508, 84422, 113040, 100523, 29659, 110466, \n",
      "29659, 97661, 29659, 108194, 29659, 107316, 125160, 53609, 125160, 86690, 23354, 29659, 118971, 29659, 24487, 29659, 22315, 29659, 44405, 29659, \n",
      "52180, 29659, 27541, 29659, 131072, 40071, 23354, 131072, 29659, 24487, 29659, 121041, 29659, 18549, 29659, 24505, 29659, 124759, 29659, 27541, \n",
      "29659, 131072, 107316, 29659, 88423, 54829, 116655, 73113, 99602, 29659, 86690, 111037, 35389, 131072, 84422, 46746, 29659, 29659, 125366, 29659, \n",
      "27733, 29659, 84010, 29659, 36994, 29659, 44717, 76330, 29659, 71390, 29659, 124461, 29659, 83920, 29659, 24487, 29659, 131072, 43502, 131072, \n",
      "29659, 128169, 29659, 20407, 29659, 37143, 97713, 84422, 113040, 100523, 29659, 27597, 29659, 71390, 29659, 90575, 29659, 131072, 92995, 4882, \n",
      "73113, 4882, 11126, 14212, 29659, 14212, 67301, 29659, 4882, 1253, 73113, 1253, 118825, 131072, 29659, 30247, 29659, 64018, 29659, 131072, \n",
      "120440, 105102, 29659, 54516, 95110, 54829, 116655, 29659, 4882, 67301, 29659, 14212, 67301, 29659, 4882, 1253, 73113, 1253, 118825, 131072, \n",
      "15158, 46746, 29659, 29659, 92862, 29659, 73864, 46746, 29659, 29659, 71553, 29659, 82722, 29659, 76394, 29659, 37987, 104505, 29659, 60434, \n",
      "24183, 118575, 72302, 116384, 29659, 81638, 29659, 114676, 29659, 64018, 29659, 24487, 29659, 130036, 29659, 51398, 29659, 24487, 29659, 114210, \n",
      "29659, 119970, 29659, 35389, 29659, 62788, 29659, 83648, 29659, 37143, 10303, 29659, 94074, 29659, 22414, 29659, 118971, 29659, 96230, 29659, \n",
      "24495, 29659, 59604, 55972, 46746, 46746, 29659, 29659, 10347, 29659, 38032, 29659, 76394, 29659, 966, 23778, 29659, 108853, 29659, 84010, \n",
      "29659, 22522, 29659, 120297, 57188, 3662, 100439, 29659, 123538, 29659, 119792, 29659, 118537, 29659, 88646, 26088, 84422, 29659, 108962, 29659, \n",
      "120297, 57188, 3662, 29659, 27304, 29659, 73000, 29659, 37143, 115992, 29659, 35389, 54433, 29659, 97661, 29659, 85823, 29659, 27541, 29659, \n",
      "24487, 29659, 51051, 29659, 54239, 84422, 29659, 117513, 23354, 29659, 97661, 29659, 52250, 13745, 29659, 24487, 29659, 129262, 29659, 35389, \n",
      "29659, 10303, 111037, 35389, 84422, 46746, 29659, 29659, 63150, 84422, 58811, 84422, 19473, 84422, 58811, 29659, 112821, 90001, 172, 90001, \n",
      "112821, 29659, 10303, 29659, 43655, 29659, 40697, 46746, 29659, 29659, 60434, 24183, 118575, 72302, 104505, 46746, 29659, 29659, 284, 29659, \n",
      "85382, 29659, 58811, 90001, 29659, 112143, 29659, 63150, 90001, 99992, 29659, 44717, 76330, 29659, 73248, 126469, 84422, 46746, 29659, 29659, \n",
      "73596, 29659, 119792, 29659, 114922, 104505, 46746, 29659, 29659, 54239, 29659, 31655, 29659, 114922, 84422, 56534, 37143, 48368, 55972, 46746, \n",
      "29659, 29659, 96230, 29659, 31655, 29659, 120297, 57188, 3662, 37143, 54239, 23354, 29659, 46164, 55972, 46746, 29659, 29659, 108962, 29659, \n",
      "96230, 29659, 44405, 29659, 125299, 29659, 73000, 23354, 29659, 97661, 29659, 60434, 29659, 24487, 29659, 51051, 29659, 88646, 26088, 29659, \n",
      "10303, 29659, 84010, 29659, 129262, 29659, 37208, 29659, 37143, 41721, 29659, 64848, 35389, 15158, 46746, 46746, 46746, 29659, 29659, 120297, \n",
      "57188, 3662, 37143, 35389, 127408, 23354, 29659, 46164, 69601, 86964, 58492, 46746, 29659, 29659, 5259, 29659, 24487, 29659, 98909, 29659, \n",
      "37143, 117535, 29659, 83592, 29659, 44946, 29659, 55371, 29659, 114601, 82505, 15158, 46746, 29659, 29659, 15957, 75977, 29659, 123538, 29659, \n",
      "55921, 125160, 100523, 105066, 46746, 29659, 29659, 66069, 29659, 71013, 105759, 75538, 37143, 75538, 23354, 29659, 35389, 127408, 15158, 46746, \n",
      "29659, 29659, 108962, 29659, 108194, 29659, 44405, 29659, 79479, 23354, 29659, 124759, 29659, 67265, 29659, 37143, 20029, 23354, 29659, 24495, \n",
      "23354, 29659, 43502, 23354, 29659, 55725, 120849, 15158, 46746, 29659, 29659, 71553, 29659, 11108, 29659, 24487, 29659, 24495, 29659, 96446, \n",
      "29659, 55641, 29659, 24495, 47168, 40952, 46164, 84422, 79984, 20799, 29659, 112143, 29659, 24508, 29659, 108867, 29659, 53876, 84422, 46746, \n",
      "29659, 29659, 74117, 31921, 29659, 24487, 29659, 24192, 29659, 98909, 29659, 76631, 104505, 46746, 29659, 29659, 7637, 20029, 13920, 29659, \n",
      "88908, 24495, 72302, 13920, 29659, 88908, 43502, 26217, 64848, 35389, 30346, 29659, 129637, 41503, 29659, 20029, 29659, 112143, 29659, 43502, \n",
      "29659, 110466, 29659, 115474, 131072, 107159, 29659, 73000, 84422, 46746, 46746, 46746, 29659, 29659, 71013, 112949, 69897, 24183, 37143, 75538, \n",
      "101946, 46746, 29659, 29659, 111525, 29659, 119792, 29659, 120523, 29659, 51398, 29659, 34262, 104505, 46746, 29659, 29659, 18646, 125160, 61210, \n",
      "125160, 58074, 23354, 29659, 61210, 125160, 18646, 125160, 58074, 23354, 29659, 58074, 58074, 125160, 61210, 125160, 18646, 23354, 29659, 47308, \n",
      "118702, 29659, 58074, 125160, 61210, 125160, 18646, 84422, 46746, 29659, 29659, 90319, 29659, 24487, 29659, 20029, 23354, 29659, 24495, 23354, \n",
      "29659, 43502, 84422, 29659, 108962, 29659, 65208, 29659, 32263, 23354, 29659, 116078, 29659, 37143, 20029, 23354, 29659, 24495, 23354, 29659, \n",
      "43502, 23354, 29659, 647, 93846, 15122, 46746, 46746, 46746, 29659, 29659, 71013, 54929, 69897, 24183, 37143, 75538, 23354, 29659, 51051, \n",
      "30647, 54239, 101946, 46746, 29659, 29659, 95515, 29659, 113290, 29659, 110466, 29659, 97661, 29659, 2105, 29659, 125160, 29659, 117096, 29659, \n",
      "37143, 109428, 29659, 90001, 29659, 112143, 29659, 48464, 46746, 29659, 29659, 128383, 29659, 18646, 125160, 61210, 29659, 110466, 29659, 24487, \n",
      "29659, 124040, 29659, 99992, 29659, 33916, 29659, 24763, 84422, 46746, 29659, 29659, 83653, 29659, 61210, 125160, 18646, 29659, 110466, 29659, \n",
      "24487, 29659, 117022, 29659, 99992, 29659, 33916, 29659, 24763, 84422, 46746, 29659, 29659, 54907, 23354, 29659, 110466, 29659, 95879, 29659, \n",
      "52567, 23354, 29659, 73768, 29659, 124759, 29659, 76631, 29659, 61210, 125160, 58074, 84422, 46746, 29659, 29659, 60915, 29659, 13593, 29659, \n",
      "37143, 20029, 23354, 29659, 24495, 23354, 29659, 43502, 23354, 29659, 81577, 54433, 29659, 29974, 29659, 41402, 29659, 18549, 29659, 42626, \n",
      "29659, 73000, 84422, 46746, 46746, 46746, 29659, 29659, 51894, 29659, 117324, 104505, 46746, 29659, 29659, 96332, 29659, 8401, 104505, 46746, \n",
      "29659, 29659, 123297, 73457, 73680, 41865, 37143, 43502, 55972, 29659, 88590, 29659, 95166, 29659, 110466, 29659, 43502, 29659, 44405, 29659, \n",
      "73457, 84422, 46746, 29659, 29659, 114903, 20029, 53985, 24495, 37143, 24495, 23354, 29659, 43502, 55972, 29659, 88590, 29659, 18849, 29659, \n",
      "24487, 29659, 20029, 90001, 69252, 29659, 81638, 29659, 84010, 29659, 24495, 29659, 37143, 110832, 29659, 115308, 15158, 46746, 29659, 29659, \n",
      "32263, 80573, 53985, 24495, 37143, 20029, 23354, 29659, 24495, 23354, 29659, 43502, 55972, 29659, 88590, 29659, 27733, 29659, 20029, 29659, \n",
      "33916, 29659, 90423, 29659, 53927, 84422, 46746, 29659, 29659, 120297, 33954, 113439, 86768, 37143, 43502, 127408, 55972, 29659, 88590, 29659, \n",
      "16821, 90001, 43502, 29659, 22736, 76330, 29659, 37143, 110466, 29659, 88908, 29659, 46452, 29659, 105338, 29659, 53851, 29659, 8824, 29659, \n",
      "34116, 23354, 29659, 112106, 29659, 89804, 29659, 8824, 29659, 34116, 15158, 46746, 29659, 29659, 63150, 84422, 58811, 84422, 19473, 84422, \n",
      "63150, 29659, 12667, 29659, 1379, 29659, 107504, 9775, 46746, 29659, 29659, 56921, 107316, 125160, 53609, 125160, 86690, 56921, 29659, 37143, \n",
      "40071, 55972, 46746, 29659, 29659, 15957, 75977, 29659, 88590, 29659, 83694, 107316, 64214, 53609, 64214, 86690, 91062, 46746, 29659, 29659, \n",
      "71013, 112949, 69897, 24183, 29659, 102034, 29659, 91536, 125160, 39133, 125160, 116059, 37143, 116059, 55972, 29659, 88590, 29659, 20029, 31655, \n",
      "107316, 23354, 29659, 24495, 31655, 63150, 23354, 29659, 43502, 31655, 86690, 84422, 29659, 124404, 84422, 46746, 29659, 29659, 64235, 29659, \n",
      "56921, 107316, 29659, 88423, 54829, 116655, 73113, 99602, 29659, 86690, 111037, 35389, 80210, 46746, 46746, 46746, 29659, 29659, 56921, 93948, \n",
      "125160, 7598, 56921, 29659, 37143, 40071, 23354, 29659, 127836, 114854, 94302, 55972, 46746, 29659, 29659, 15957, 75977, 29659, 88590, 29659, \n",
      "83694, 93948, 64214, 7598, 91062, 46746, 29659, 29659, 95515, 29659, 38032, 29659, 1588, 29659, 31384, 29659, 124759, 29659, 102034, 29659, \n",
      "91536, 125160, 39133, 23354, 29659, 39133, 125160, 91536, 23354, 29659, 112143, 29659, 39133, 125160, 116059, 84422, 46746, 29659, 29659, 108962, \n",
      "29659, 7598, 29659, 44405, 29659, 59252, 29659, 76631, 29659, 76394, 29659, 43502, 29659, 9823, 29659, 39133, 125160, 116059, 29659, 22736, \n",
      "76330, 23354, 29659, 124759, 131072, 82505, 29659, 131072, 39133, 31655, 93948, 23354, 29659, 43502, 31655, 7598, 23354, 131072, 29659, 31384, \n",
      "29659, 20029, 29659, 44405, 29659, 73000, 84422, 46746, 29659, 29659, 43867, 104505, 29659, 56921, 88423, 69575, 29659, 7598, 111037, 35389, \n",
      "80210, 46746, 46746, 46746, 29659, 29659, 56921, 24763, 84422, 46452, 84422, 31154, 56921, 46746, 29659, 29659, 20932, 29659, 84422, 29659, \n",
      "44405, 29659, 64856, 23354, 29659, 97661, 29659, 116511, 29659, 124759, 29659, 76631, 29659, 131072, 112949, 90001, 99992, 29659, 44717, 76330, \n",
      "23354, 131072, 29659, 31384, 29659, 65611, 29659, 18646, 84422, 61210, 84422, 58074, 58074, 29659, 112143, 29659, 61210, 84422, 18646, 84422, \n",
      "58074, 58074, 29659, 112143, 29659, 47308, 118702, 84422, 46746, 29659, 29659, 108962, 29659, 124759, 131072, 82505, 29659, 8930, 29659, 32263, \n",
      "23354, 29659, 97661, 29659, 99343, 29659, 24495, 31655, 46452, 29659, 105338, 29659, 115992, 84422, 29659, 125366, 29659, 27304, 29659, 73000, \n",
      "23354, 29659, 31384, 29659, 97661, 29659, 85823, 29659, 27541, 29659, 24487, 29659, 51051, 29659, 88646, 26088, 84422, 46746, 29659, 29659, \n",
      "2153, 23354, 29659, 24487, 29659, 35389, 29659, 88646, 26088, 29659, 44405, 29659, 104386, 29659, 51398, 29659, 24487, 29659, 114210, 29659, \n",
      "10303, 29659, 76394, 29659, 52354, 90001, 99574, 29659, 52935, 23354, 29659, 97713, 84422, 113040, 100523, 29659, 56921, 107316, 29659, 88423, \n",
      "54829, 116655, 73113, 99602, 29659, 86690, 111037, 35389, 80210, 29659, 79512, 23354, 29659, 24487, 29659, 44717, 76330, 29659, 71390, 29659, \n",
      "124461, 29659, 2105, 29659, 24487, 29659, 126655, 111037, 35389, 29659, 118971, 29659, 62041, 29659, 96781, 29659, 43502, 29659, 71390, 29659, \n",
      "37143, 90575, 29659, 131072, 92995, 4882, 73113, 4882, 11126, 14212, 29659, 14212, 67301, 29659, 4882, 1253, 73113, 1253, 118825, 131072, \n",
      "29659, 83391, 84422, 29659, 131072, 120440, 105102, 29659, 54516, 95110, 54829, 116655, 29659, 4882, 67301, 29659, 14212, 67301, 81577, 131072, \n",
      "55972, 29659, 51398, 29659, 24487, 29659, 11241, 29659, 115428, 84422, 46746, 46746, 46746, 29659, 29659, 63150, 84422, 58811, 84422, 75401, \n",
      "29659, 92862, 29659, 1301, 29659, 81436, 29659, 76696, 46746, 29659, 29659, 74584, 29659, 97661, 131072, 102930, 29659, 119155, 29659, 24487, \n",
      "29659, 22315, 29659, 37143, 112143, 29659, 100751, 29659, 54359, 29659, 24487, 29659, 44833, 29659, 81658, 55972, 29659, 51398, 29659, 12129, \n",
      "29659, 63150, 84422, 58811, 84422, 58811, 23354, 29659, 24487, 29659, 51208, 29659, 6319, 29659, 44405, 29659, 27541, 29659, 2786, 29659, \n",
      "24487, 29659, 114210, 29659, 81638, 29659, 130036, 23354, 29659, 126776, 29659, 24487, 29659, 35389, 29659, 81638, 39854, 23354, 29659, 99343, \n",
      "29659, 24487, 29659, 24495, 29659, 119970, 29659, 76394, 29659, 95098, 23354, 29659, 118971, 29659, 120849, 29659, 24487, 29659, 35389, 29659, \n",
      "10303, 29659, 131072, 111037, 35389, 131072, 29659, 52354, 84422, 29659, 125366, 29659, 44405, 29659, 38852, 29659, 57061, 29659, 172, 29659, \n",
      "24487, 29659, 108084, 29659, 118971, 29659, 120523, 29659, 87534, 29659, 99992, 23354, 29659, 29974, 29659, 97661, 29659, 2786, 29659, 8930, \n",
      "29659, 56095, 121287, 104998, 23354, 29659, 11193, 23354, 29659, 72110, 23354, 29659, 101053, 35229, 118971, 29659, 99343, 29659, 61086, 29659, \n",
      "119970, 29659, 59604, 84422, 29659, 125366, 29659, 44405, 29659, 108829, 29659, 11408, 29659, 172, 104505, 46746, 29659, 29659, 114714, 94252, \n",
      "29659, 24487, 29659, 114210, 29659, 119970, 29659, 44629, 29659, 37143, 59604, 23354, 29659, 31917, 103770, 23354, 29659, 107451, 68935, 15158, \n",
      "46746, 29659, 29659, 10111, 29659, 111384, 29659, 84564, 29659, 84010, 29659, 22736, 76333, 29659, 122247, 29659, 70186, 29659, 37143, 97713, \n",
      "84422, 113040, 100523, 29659, 3102, 111576, 90001, 99822, 29659, 72110, 29659, 90575, 29659, 58811, 23354, 43712, 23354, 29659, 27297, 90001, \n",
      "99822, 29659, 72110, 29659, 90575, 29659, 63150, 84422, 82660, 23354, 29659, 35389, 29659, 56095, 29659, 90575, 29659, 46452, 125160, 24763, \n",
      "125160, 84772, 23354, 29659, 112669, 29659, 84564, 29659, 90575, 29659, 62964, 20757, 23354, 29659, 101053, 48464, 46746, 29659, 29659, 98695, \n",
      "127440, 29659, 24487, 29659, 11193, 23354, 29659, 72110, 23354, 29659, 130036, 29659, 118971, 29659, 112669, 29659, 84564, 29659, 119970, 29659, \n",
      "59604, 29659, 37143, 72110, 29659, 10303, 29659, 24487, 29659, 79757, 29659, 64018, 29659, 72664, 85896, 59604, 29659, 99992, 55972, 29659, \n",
      "118971, 29659, 57314, 29659, 10303, 29659, 78208, 29659, 77200, 29659, 52180, 29659, 64018, 29659, 60257, 29659, 95143, 29659, 112669, 29659, \n",
      "118971, 29659, 63075, 29659, 56359, 125160, 16821, 59321, 29659, 44863, 29659, 27541, 29659, 59604, 29659, 51398, 29659, 119792, 29659, 22315, \n",
      "84422, 46746, 29659, 29659, 114714, 29659, 29335, 41503, 29659, 37143, 92962, 129319, 55972, 46746, 29659, 29659, 21801, 29659, 44833, 131072, \n",
      "82505, 29659, 37987, 104505, 46746, 29659, 29659, 129357, 29659, 107159, 84422, 10918, 51982, 29659, 27541, 29659, 130851, 104505, 46746, 29659, \n",
      "29659, 124432, 43644, 29659, 64018, 29659, 95098, 29659, 91412, 23354, 46746, 29659, 29659, 92170, 29659, 113669, 90001, 95098, 23354, 29659, \n",
      "113669, 90001, 107451, 68935, 29659, 91412, 29659, 37143, 31917, 103770, 55972, 46746, 29659, 29659, 44946, 84422, 46746, 29659, 29659, 125366, \n",
      "29659, 27733, 29659, 24487, 29659, 114210, 29659, 44405, 29659, 1549, 29659, 119970, 29659, 84564, 104505, 29659, 59604, 23354, 29659, 31917, \n",
      "103770, 23354, 29659, 118971, 29659, 107451, 68935, 23354, 29659, 119792, 29659, 99726, 29659, 90929, 84422, 29659, 10347, 131072, 82505, 29659, \n",
      "47875, 29659, 103839, 29659, 97661, 29659, 101422, 29659, 59089, 29659, 27541, 29659, 68559, 29659, 63761, 29659, 80373, 29659, 118971, 29659, \n",
      "31917, 103770, 29659, 11023, 29659, 56007, 73984, 29659, 24487, 29659, 11241, 29659, 98909, 84422, 46746, 29659, 29659, 10111, 29659, 51894, \n",
      "29659, 45289, 29659, 37143, 117505, 11424, 87251, 8030, 55972, 46746, 29659, 29659, 125366, 29659, 37987, 29659, 52955, 29659, 101446, 29659, \n",
      "81638, 29659, 76394, 29659, 120523, 29659, 90575, 29659, 40952, 29659, 16578, 8696, 114348, 29659, 38304, 114348, 29659, 7915, 114348, 29659, \n",
      "16578, 35389, 16578, 29659, 101480, 29659, 118971, 29659, 10593, 119030, 29659, 61086, 29659, 119970, 29659, 76394, 29659, 92170, 29659, 124251, \n",
      "104505, 29659, 56921, 8696, 111037, 35389, 80210, 46746, 29659, 29659, 77337, 99479, 104505, 29659, 71553, 29659, 73768, 29659, 111037, 35389, \n",
      "29659, 76631, 29659, 76394, 29659, 107572, 29659, 84010, 29659, 64578, 29659, 76394, 29659, 43502, 29659, 112143, 29659, 13593, 29659, 35389, \n",
      "29659, 98909, 29659, 44405, 29659, 98022, 29659, 122353, 29659, 27541, 29659, 42626, 29659, 76394, 29659, 35389, 29659, 84010, 29659, 129400, \n",
      "29659, 42626, 29659, 54225, 29659, 77197, 29659, 20407, 29659, 37143, 90575, 29659, 81638, 29659, 131072, 20029, 29659, 24495, 29659, 43502, \n",
      "131072, 29659, 71390, 29659, 51398, 29659, 47633, 29659, 68443, 29659, 72664, 85896, 59604, 29659, 37987, 15158, 46746, 29659, 29659, 10347, \n",
      "29659, 5775, 19442, 29659, 90882, 29659, 84564, 23354, 29659, 122907, 94466, 29659, 107451, 68935, 23354, 29659, 118971, 29659, 102034, 29659, \n",
      "27541, 29659, 62929, 29659, 24487, 29659, 112949, 29659, 84466, 29659, 84564, 29659, 7165, 30346, 29659, 117880, 30346, 29659, 56921, 35389, \n",
      "56921, 29659, 51398, 29659, 35606, 84422, 29659, 108962, 29659, 65208, 23354, 29659, 124759, 29659, 10593, 119030, 29659, 61086, 29659, 10303, \n",
      "29659, 24487, 29659, 27167, 29659, 44717, 76330, 29659, 124251, 84422, 46746, 29659, 29659, 125366, 29659, 17694, 29659, 24487, 29659, 35389, \n",
      "29659, 71225, 29659, 48441, 23354, 29659, 97713, 84422, 113040, 100523, 29659, 56921, 8696, 111037, 35389, 56921, 29659, 112143, 29659, 56921, \n",
      "29379, 125160, 93948, 125160, 125839, 111037, 35389, 30346, 29659, 31384, 29659, 97661, 29659, 124461, 29659, 93894, 29659, 96781, 29659, 71390, \n",
      "29659, 75532, 29659, 37143, 90575, 29659, 131072, 92995, 4882, 73113, 4882, 11126, 14212, 29659, 14212, 67301, 29659, 65496, 102260, 99602, \n",
      "54829, 4882, 83932, 118825, 131072, 29659, 112143, 29659, 131072, 99001, 86882, 29659, 88423, 69575, 29659, 92995, 4882, 73113, 4882, 11126, \n",
      "14212, 29659, 14212, 67301, 29659, 54027, 31050, 73113, 15825, 54829, 4882, 83932, 118825, 23354, 131072, 29659, 101053, 48464, 46746, 46746, \n",
      "46746, 29659, 29659, 10111, 29659, 114004, 28518, 90001, 81568, 58866, 29659, 85851, 29659, 37143, 117505, 74678, 71648, 108784, 128373, 72110, \n",
      "55972, 46746, 29659, 29659, 78934, 29659, 51398, 29659, 63622, 29659, 44717, 76330, 29659, 90599, 23354, 29659, 60442, 29659, 2105, 29659, \n",
      "48368, 23354, 43712, 23354, 29659, 58811, 23354, 111461, 23354, 43712, 23354, 29659, 101053, 84422, 29659, 21801, 29659, 121041, 29659, 10593, \n",
      "119030, 29659, 40952, 29659, 16578, 58811, 114348, 29659, 114348, 114348, 29659, 16578, 43712, 16578, 29659, 101480, 29659, 119970, 29659, 40952, \n",
      "29659, 16578, 53851, 16578, 29659, 105066, 46746, 29659, 29659, 125366, 29659, 44405, 29659, 76394, 29659, 83684, 29659, 115428, 29659, 84010, \n",
      "29659, 53310, 29659, 110466, 29659, 76394, 29659, 124251, 29659, 44405, 29659, 76394, 29659, 94859, 124759, 23354, 29659, 118971, 29659, 110466, \n",
      "29659, 24487, 29659, 36994, 29659, 84564, 29659, 92171, 29659, 131072, 23354, 29659, 6973, 29659, 38802, 29659, 33106, 114257, 131072, 29659, \n",
      "10593, 119030, 29659, 61086, 29659, 51982, 29659, 119970, 29659, 76394, 29659, 92170, 29659, 44717, 76330, 29659, 124251, 84422, 46746, 29659, \n",
      "29659, 12508, 84422, 113040, 110230, 29659, 32203, 58811, 114348, 29659, 114348, 114348, 29659, 16578, 43712, 130710, 29659, 88590, 29659, 32203, \n",
      "53851, 78021, 46746, 29659, 29659, 10111, 29659, 86014, 90001, 81568, 58866, 29659, 85851, 29659, 37143, 87528, 121581, 17401, 111422, 59216, \n",
      "72110, 55972, 46746, 29659, 29659, 122587, 29659, 27541, 29659, 73248, 79643, 23354, 29659, 124759, 29659, 10593, 119030, 29659, 40952, 29659, \n",
      "16578, 63150, 114348, 29659, 647, 114348, 29659, 16578, 82660, 16578, 29659, 101480, 29659, 119970, 29659, 32203, 63150, 84422, 82660, 78021, \n",
      "46746, 29659, 29659, 5317, 23354, 29659, 124759, 29659, 53310, 29659, 110466, 29659, 76394, 29659, 124251, 29659, 44405, 29659, 46616, 29659, \n",
      "33106, 29659, 118971, 29659, 110466, 29659, 24487, 29659, 51208, 29659, 84564, 29659, 37208, 29659, 76394, 29659, 131072, 84422, 29659, 8824, \n",
      "29659, 33106, 131072, 29659, 120523, 84422, 29659, 108962, 29659, 31384, 23354, 29659, 124759, 29659, 11961, 53543, 19442, 29659, 61086, 29659, \n",
      "119970, 29659, 97713, 84422, 113040, 84422, 29659, 56921, 63150, 84422, 82660, 26498, 84422, 46746, 29659, 29659, 125366, 29659, 6319, 29659, \n",
      "118510, 29659, 33834, 29659, 83920, 29659, 27297, 82505, 29659, 51398, 29659, 78208, 29659, 74563, 29659, 115428, 84422, 46746, 29659, 29659, \n",
      "26243, 29659, 870, 29659, 37143, 117505, 90066, 11155, 54244, 8030, 55972, 46746, 29659, 29659, 81436, 29659, 64548, 104505, 29659, 88908, \n",
      "69608, 13920, 29659, 88908, 112669, 29659, 124251, 13920, 46746, 29659, 29659, 81436, 29659, 88832, 104505, 29659, 88908, 112669, 29659, 124251, \n",
      "13920, 29659, 88908, 69608, 13920, 46746, 29659, 29659, 21801, 29659, 121041, 29659, 53310, 104505, 46746, 29659, 29659, 108962, 29659, 76394, \n",
      "29659, 124251, 29659, 44405, 29659, 76394, 29659, 8930, 29659, 112669, 29659, 72109, 29659, 112143, 29659, 6458, 27917, 29659, 68895, 23354, \n",
      "29659, 62964, 23354, 29659, 117405, 23354, 29659, 8718, 23354, 29659, 101053, 19698, 29659, 124759, 29659, 53310, 29659, 86208, 29659, 124759, \n",
      "131072, 82505, 29659, 57061, 29659, 112143, 29659, 42745, 29659, 172, 29659, 76394, 29659, 94859, 124759, 29659, 124251, 84422, 46746, 29659, \n",
      "29659, 71553, 29659, 38895, 29659, 76394, 29659, 75914, 29659, 60257, 23354, 29659, 97713, 84422, 47747, 110230, 46746, 29659, 29659, 29659, \n",
      "29659, 48368, 55972, 29659, 112669, 119929, 73690, 32203, 62964, 130710, 29659, 31655, 29659, 16578, 8718, 16578, 29659, 112143, 29659, 112669, \n",
      "119929, 73690, 32203, 72863, 130710, 29659, 31655, 29659, 16578, 8718, 16578, 29659, 112143, 29659, 29659, 112669, 119929, 73690, 32203, 8718, \n",
      "130710, 29659, 31655, 29659, 16578, 8718, 114348, 29659, 91544, 29659, 101053, 84422, 46746, 29659, 29659, 58811, 55972, 29659, 112669, 83867, \n",
      "65347, 5919, 22758, 32203, 117405, 78199, 37610, 130710, 29659, 31655, 29659, 56921, 111070, 117448, 60670, 116655, 56921, 46746, 29659, 29659, \n",
      "125366, 29659, 118510, 29659, 33834, 29659, 26822, 29659, 51982, 29659, 119552, 29659, 112669, 29659, 56359, 29659, 118971, 29659, 16821, 59321, \n",
      "23354, 29659, 55641, 29659, 81111, 29659, 97661, 29659, 124461, 29659, 99343, 29659, 61086, 29659, 27541, 29659, 59604, 29659, 51398, 29659, \n",
      "24487, 29659, 116223, 29659, 22315, 29659, 41003, 29659, 119155, 29659, 74563, 84422, 46746, 29659, 29659, 21801, 29659, 37987, 29659, 10593, \n",
      "119030, 29659, 61086, 29659, 119970, 29659, 76394, 29659, 92170, 29659, 124251, 29659, 10303, 29659, 24487, 29659, 44717, 76330, 29659, 99992, \n",
      "29659, 6973, 29659, 24487, 29659, 54225, 90001, 77197, 29659, 112669, 29659, 37143, 90575, 29659, 56921, 20757, 29659, 111070, 117448, 60670, \n",
      "116655, 67437, 46746, 29659, 29659, 108962, 29659, 76394, 29659, 112669, 29659, 124251, 29659, 44405, 29659, 71328, 23354, 29659, 124759, 29659, \n",
      "18549, 29659, 126003, 29659, 99343, 29659, 64848, 29659, 88590, 29659, 131072, 111070, 117448, 60670, 116655, 131072, 29659, 51398, 29659, 24487, \n",
      "29659, 49224, 29659, 22315, 84422, 46746, 29659, 29659, 16615, 29659, 30967, 104505, 29659, 116195, 29659, 81455, 29659, 92171, 29659, 94941, \n",
      "29659, 37143, 97713, 84422, 113040, 100523, 29659, 131072, 88870, 84422, 131072, 29659, 44405, 29659, 8930, 29659, 76631, 29659, 131072, 72863, \n",
      "23354, 131072, 29659, 38852, 29659, 44863, 29659, 27541, 29659, 131072, 8718, 23354, 131072, 29659, 101053, 48464, 46746, 29659, 29659, 20932, \n",
      "29659, 24487, 29659, 103652, 29659, 18549, 29659, 54800, 29659, 24487, 29659, 112669, 29659, 65611, 29659, 1626, 29659, 112143, 29659, 12064, \n",
      "29659, 24487, 29659, 44717, 76330, 29659, 99992, 23354, 29659, 24487, 29659, 37987, 29659, 53310, 29659, 81638, 29659, 65458, 29659, 56095, \n",
      "84422, 46746, 29659, 29659, 105002, 29659, 83819, 29659, 50342, 29659, 31497, 29659, 44946, 29659, 118971, 29659, 81455, 29659, 92171, 29659, \n",
      "85624, 23354, 29659, 81638, 29659, 38802, 29659, 100192, 61343, 84422, 46746, 29659, 29659, 95510, 24337, 29659, 870, 29659, 37143, 117505, \n",
      "63375, 39206, 8030, 55972, 46746, 29659, 29659, 81436, 29659, 64548, 104505, 29659, 88908, 69608, 13920, 29659, 88908, 63075, 29659, 124251, \n",
      "13920, 46746, 29659, 29659, 81436, 29659, 88832, 104505, 29659, 88908, 63075, 29659, 124251, 13920, 29659, 88908, 69608, 13920, 46746, 29659, \n",
      "29659, 21801, 29659, 44833, 29659, 2021, 29659, 76394, 29659, 51372, 29659, 33809, 103382, 23354, 29659, 33809, 22315, 2027, 94466, 29659, \n",
      "118971, 29659, 76394, 29659, 33809, 7635, 29659, 108977, 23354, 29659, 97713, 84422, 113040, 110230, 46746, 29659, 29659, 29659, 29659, 48368, \n",
      "55972, 29659, 33809, 7635, 83694, 126122, 129169, 29659, 31655, 29659, 32203, 75298, 114348, 29659, 16578, 18202, 114348, 29659, 16578, 15372, \n",
      "72863, 114348, 29659, 16578, 22318, 107159, 114348, 29659, 16578, 126122, 114348, 29659, 16578, 85459, 114348, 29659, 16578, 13309, 82151, 29659, \n",
      "11307, 29659, 80644, 29659, 16821, 59321, 29659, 118971, 29659, 56359, 29659, 37143, 51398, 29659, 119354, 55972, 29659, 81638, 29659, 76394, \n",
      "29659, 81658, 29659, 64018, 29659, 119552, 29659, 11193, 29659, 37143, 38802, 29659, 41880, 29659, 127635, 15158, 46746, 29659, 29659, 29659, \n",
      "29659, 29659, 29659, 58811, 55972, 29659, 33809, 103382, 83694, 22318, 107159, 129169, 29659, 31655, 29659, 56921, 126122, 30346, 29659, 11307, \n",
      "29659, 80644, 29659, 24487, 29659, 11193, 29659, 8744, 29659, 77197, 29659, 51398, 29659, 78208, 29659, 103387, 29659, 57243, 23354, 29659, \n",
      "10303, 29659, 41402, 29659, 122803, 29659, 14622, 29659, 58216, 84422, 29659, 46746, 29659, 29659, 29659, 29659, 29659, 29659, 63150, 55972, \n",
      "29659, 33809, 22315, 2027, 94466, 29659, 84493, 29659, 64018, 29659, 51982, 29659, 64018, 29659, 6471, 29659, 11193, 29659, 51398, 29659, \n",
      "33809, 35893, 21978, 29659, 118971, 29659, 45821, 29659, 61086, 29659, 27541, 29659, 119552, 29659, 103028, 29659, 25108, 29659, 123538, 29659, \n",
      "19417, 29659, 22315, 29659, 121041, 84422, 46746, 29659, 29659, 29659, 29659, 97713, 84422, 47747, 110230, 29659, 33809, 22315, 2027, 94466, \n",
      "40952, 131072, 126122, 131072, 20167, 131072, 37610, 131072, 101480, 29659, 31655, 29659, 56921, 88423, 11126, 31050, 116655, 56921, 46746, 29659, \n",
      "29659, 108962, 29659, 97661, 29659, 10918, 29659, 131072, 111461, 127714, 23354, 131072, 29659, 97661, 29659, 117022, 29659, 2105, 29659, 24487, \n",
      "29659, 69608, 29659, 131072, 20757, 131072, 29659, 118971, 29659, 24487, 29659, 63075, 29659, 131072, 27987, 84422, 131072, 29659, 52482, 29659, \n",
      "8930, 23354, 29659, 97661, 29659, 54695, 29659, 81173, 29659, 90575, 29659, 131072, 65496, 54829, 107765, 99602, 29659, 14212, 67301, 29659, \n",
      "88423, 112461, 60670, 11126, 90854, 73113, 116655, 54829, 88423, 131072, 29659, 51398, 29659, 40071, 84422, 46746, 29659, 29659, 21801, 29659, \n",
      "37987, 29659, 102034, 29659, 27541, 29659, 42288, 29659, 38453, 90001, 17730, 29659, 11193, 29659, 37143, 90575, 29659, 131072, 91860, 125160, \n",
      "58643, 23354, 131072, 29659, 131072, 59617, 113040, 23354, 131072, 29659, 131072, 75727, 131072, 15158, 29659, 10347, 29659, 17132, 29659, 78468, \n",
      "29659, 81638, 29659, 108278, 29659, 80373, 29659, 112143, 29659, 31917, 103770, 29659, 59729, 23354, 29659, 101053, 54748, 29659, 51398, 29659, \n",
      "96251, 84422, 46746, 29659, 29659, 20932, 29659, 24487, 29659, 103652, 29659, 18549, 29659, 54800, 29659, 24487, 29659, 63075, 29659, 65611, \n",
      "29659, 1626, 29659, 112143, 29659, 12064, 29659, 24487, 29659, 44717, 76330, 29659, 99992, 23354, 29659, 24487, 29659, 37987, 29659, 53310, \n",
      "29659, 81638, 29659, 65458, 29659, 56095, 84422, 29659, 108962, 29659, 26845, 29659, 24487, 29659, 63075, 29659, 44405, 29659, 65208, 23354, \n",
      "29659, 26845, 29659, 24487, 29659, 63075, 29659, 44405, 29659, 96230, 29659, 27541, 29659, 95098, 29659, 37208, 84422, 46746, 29659, 29659, \n",
      "105002, 29659, 83819, 29659, 50342, 29659, 31497, 29659, 44946, 29659, 118971, 29659, 81455, 29659, 92171, 29659, 85624, 29659, 81638, 29659, \n",
      "38802, 29659, 100192, 61343, 84422, 46746, 29659, 29659, 72819, 29659, 110466, 29659, 114714, 29659, 44405, 29659, 47198, 29659, 37143, 123297, \n",
      "69608, 55972, 46746, 29659, 29659, 125366, 29659, 44405, 29659, 76394, 29659, 12164, 29659, 84010, 29659, 53310, 29659, 110466, 29659, 24487, \n",
      "29659, 124251, 29659, 44405, 29659, 76394, 29659, 69608, 23354, 29659, 120044, 29659, 84010, 29659, 110466, 29659, 97661, 29659, 39285, 29659, \n",
      "113299, 29659, 37143, 112143, 29659, 54271, 55972, 29659, 27297, 29659, 70779, 29659, 51398, 29659, 24487, 29659, 124251, 23354, 29659, 124759, \n",
      "131072, 82505, 29659, 39202, 29659, 44717, 76330, 84422, 46746, 29659, 29659, 108962, 29659, 124759, 131072, 82505, 29659, 46616, 29659, 33106, \n",
      "29659, 112143, 29659, 33106, 29659, 10303, 29659, 113299, 29659, 54570, 23354, 29659, 97661, 29659, 116511, 29659, 124759, 29659, 76631, 29659, \n",
      "76394, 29659, 54239, 29659, 69608, 84422, 46746, 29659, 29659, 108962, 29659, 36064, 29659, 92171, 29659, 24242, 29659, 44946, 23354, 29659, \n",
      "124759, 29659, 44405, 29659, 125299, 29659, 39202, 29659, 76394, 29659, 69608, 29659, 24560, 29659, 44946, 29659, 71767, 29659, 31890, 29659, \n",
      "84010, 29659, 24487, 29659, 54570, 29659, 44405, 29659, 9171, 29659, 76394, 29659, 13945, 29659, 12381, 23354, 29659, 118971, 29659, 172, \n",
      "29659, 24508, 23354, 29659, 44946, 29659, 109550, 29659, 125299, 29659, 39285, 29659, 123534, 29659, 1388, 29659, 119970, 29659, 24487, 29659, \n",
      "25629, 29659, 124251, 84422, 46746, 29659, 29659, 1401, 29659, 25138, 29659, 27541, 29659, 120966, 29659, 45289, 29659, 37143, 99343, 3850, \n",
      "101040, 59604, 55972, 46746, 29659, 29659, 54907, 29659, 37143, 11307, 29659, 44405, 29659, 24487, 29659, 37987, 29659, 27541, 29659, 99343, \n",
      "29659, 24487, 29659, 109783, 29659, 66084, 29659, 12064, 29659, 35389, 29659, 130129, 29659, 37143, 108110, 29659, 75532, 55972, 29659, 119970, \n",
      "29659, 59604, 23354, 29659, 38058, 29659, 31497, 29659, 72110, 23354, 29659, 104998, 23354, 29659, 54800, 49961, 23354, 29659, 118971, 29659, \n",
      "11193, 29659, 122237, 29659, 10303, 29659, 83684, 29659, 114210, 101946, 46746, 29659, 29659, 111657, 29659, 99343, 3850, 101040, 59604, 37143, \n",
      "114210, 23354, 29659, 46164, 92402, 37610, 66705, 46746, 29659, 29659, 29659, 29659, 29659, 29659, 84564, 29659, 31655, 29659, 92962, 129319, \n",
      "37143, 114210, 55972, 46746, 29659, 29659, 29659, 29659, 29659, 29659, 84564, 29659, 31655, 29659, 117505, 11424, 87251, 8030, 37143, 84564, \n",
      "55972, 46746, 29659, 29659, 29659, 29659, 29659, 29659, 84564, 29659, 31655, 29659, 117505, 74678, 71648, 108784, 128373, 72110, 37143, 84564, \n",
      "55972, 46746, 29659, 29659, 29659, 29659, 29659, 29659, 84564, 29659, 31655, 29659, 87528, 121581, 17401, 111422, 59216, 72110, 37143, 84564, \n",
      "55972, 46746, 29659, 29659, 29659, 29659, 29659, 29659, 84564, 29659, 31655, 29659, 117505, 90066, 11155, 54244, 8030, 37143, 84564, 23354, \n",
      "29659, 46164, 31655, 46164, 55972, 46746, 29659, 29659, 29659, 29659, 29659, 29659, 84564, 29659, 31655, 29659, 117505, 63375, 39206, 8030, \n",
      "37143, 84564, 23354, 29659, 46164, 31655, 46164, 55972, 46746, 29659, 29659, 29659, 29659, 29659, 29659, 81577, 46746, 29659, 29659, 29659, \n",
      "29659, 29659, 29659, 34583, 29659, 124040, 29659, 115428, 29659, 27541, 29659, 36191, 29659, 35913, 29659, 44717, 76330, 29659, 84564, 46746, \n",
      "46746, 29659, 29659, 114714, 35290, 84422, 46746, 29659, 29659, 123848, 29659, 35389, 29659, 54800, 49961, 23354, 29659, 3102, 111576, 29659, \n",
      "72110, 23354, 29659, 27297, 29659, 72110, 23354, 29659, 112669, 29659, 84564, 23354, 29659, 118971, 29659, 63075, 29659, 84564, 84422, 46746, \n",
      "29659, 29659, 57870, 29659, 24242, 29659, 124786, 29659, 44717, 76330, 29659, 84564, 29659, 29396, 29659, 24487, 29659, 37987, 29659, 68443, \n",
      "11609, 23202, 95098, 3164, 48464, 46746, 29659, 29659, 45242, 31033, 29659, 61086, 29659, 57121, 29659, 34476, 29659, 80373, 29659, 37143, \n",
      "172, 29659, 9749, 99044, 29659, 61086, 29659, 45382, 121287, 47013, 29659, 124759, 29659, 38032, 29659, 84533, 11935, 37143, 69969, 84564, \n",
      "55972, 29659, 37480, 29659, 119517, 29659, 93385, 29659, 107451, 68935, 29659, 84564, 29659, 101280, 29659, 106592, 15158, 46746, 29659, 29659, \n",
      "125366, 29659, 127346, 29659, 64018, 29659, 73105, 29659, 78294, 29659, 10593, 119030, 29659, 13593, 29659, 84564, 29659, 119970, 29659, 92170, \n",
      "29659, 84564, 29659, 108610, 29659, 8930, 29659, 83648, 29659, 37143, 130036, 23354, 29659, 27297, 82505, 23354, 29659, 112669, 29659, 71225, \n",
      "23354, 29659, 63075, 29659, 71225, 55972, 29659, 31384, 29659, 115474, 29659, 124461, 29659, 42626, 29659, 54225, 29659, 77197, 29659, 112143, \n",
      "29659, 60077, 29659, 16661, 84422, 29659, 125366, 29659, 44405, 29659, 76394, 29659, 96594, 90001, 33841, 29659, 6319, 29659, 27541, 29659, \n",
      "24487, 29659, 35389, 90001, 27541, 90001, 95098, 29659, 114676, 29659, 6319, 23354, 29659, 81111, 29659, 44061, 29659, 96686, 29659, 24487, \n",
      "29659, 35389, 29659, 120523, 23354, 29659, 96744, 29659, 24487, 29659, 35389, 29659, 81638, 39854, 23354, 29659, 39095, 29659, 24487, 29659, \n",
      "24495, 29659, 10303, 29659, 75977, 29659, 95098, 23354, 29659, 118971, 29659, 107453, 99687, 29659, 76394, 29659, 35389, 29659, 52354, 84422, \n",
      "46746, 29659, 29659, 63150, 84422, 58811, 84422, 105010, 29659, 86961, 45749, 29659, 73864, 46746, 29659, 29659, 71553, 29659, 95253, 29659, \n",
      "24487, 29659, 35389, 29659, 52354, 111037, 35389, 23354, 29659, 51398, 29659, 24487, 29659, 35389, 29659, 14892, 29659, 99992, 29659, 81111, \n",
      "29659, 44405, 29659, 94941, 29659, 55641, 29659, 24487, 29659, 11241, 29659, 96230, 29659, 66084, 84422, 29659, 21801, 29659, 97239, 29659, \n",
      "44405, 29659, 76631, 29659, 96638, 104505, 46746, 29659, 29659, 54240, 29659, 124461, 29659, 38867, 23354, 29659, 97713, 84422, 47747, 110230, \n",
      "29659, 131072, 24763, 29659, 80183, 29659, 125839, 111037, 35389, 23354, 131072, 29659, 38852, 29659, 97661, 29659, 83920, 29659, 61086, 29659, \n",
      "51398, 29659, 76394, 29659, 17205, 29659, 115428, 84422, 46746, 29659, 29659, 42717, 23354, 29659, 97661, 29659, 95679, 29659, 44717, 76330, \n",
      "29659, 8824, 29659, 112669, 29659, 119970, 29659, 76394, 29659, 92170, 29659, 124251, 23354, 29659, 97713, 84422, 47747, 110230, 29659, 56921, \n",
      "20757, 29659, 111070, 117448, 60670, 116655, 56921, 29659, 81111, 29659, 44405, 29659, 78294, 29659, 76394, 29659, 131072, 52354, 131072, 29659, \n",
      "81638, 29659, 24487, 29659, 112669, 29659, 52935, 84422, 46746, 29659, 29659, 117647, 29659, 54800, 49961, 29659, 40362, 104505, 46746, 29659, \n",
      "29659, 5317, 29659, 81638, 29659, 130036, 23354, 29659, 97661, 29659, 82722, 29659, 56095, 29659, 90575, 111037, 35389, 29659, 82444, 121924, \n",
      "29659, 27541, 29659, 24487, 29659, 11241, 29659, 124251, 29659, 51398, 29659, 117505, 11424, 87251, 8030, 84422, 29659, 83653, 23354, 29659, \n",
      "51398, 29659, 68443, 11609, 23202, 95098, 3164, 19698, 29659, 110466, 29659, 124759, 29659, 128842, 111037, 35389, 23354, 29659, 124759, 29659, \n",
      "113290, 29659, 35389, 90001, 61461, 29659, 71390, 29659, 37143, 90575, 29659, 131072, 14212, 4882, 73113, 29659, 11632, 29659, 69575, 14212, \n",
      "73113, 83932, 11126, 131072, 29659, 112143, 29659, 131072, 92995, 4882, 73113, 4882, 11126, 14212, 29659, 14212, 67301, 29659, 14212, 48275, \n",
      "102260, 96423, 54829, 60670, 11126, 14212, 131072, 15158, 46746, 29659, 29659, 27750, 23354, 29659, 24487, 29659, 52354, 29659, 32877, 29659, \n",
      "44405, 29659, 47092, 29659, 119970, 29659, 24487, 29659, 44717, 76330, 29659, 71390, 23354, 29659, 120044, 29659, 76394, 29659, 35389, 90001, \n",
      "43502, 29659, 44405, 29659, 54225, 29659, 77197, 29659, 33411, 29659, 27541, 29659, 119792, 29659, 22315, 131072, 82505, 29659, 93925, 29659, \n",
      "7025, 84422, 29659, 108962, 29659, 109428, 29659, 52354, 29659, 44405, 29659, 65208, 23354, 29659, 124759, 131072, 82505, 29659, 102508, 29659, \n",
      "76631, 29659, 76394, 29659, 122262, 29659, 44717, 76330, 29659, 124251, 84422, 46746, 29659, 29659, 63150, 84422, 58811, 84422, 14147, 29659, \n",
      "117612, 29659, 12667, 46746, 29659, 29659, 115217, 29659, 97661, 131072, 26711, 29659, 43079, 29659, 90882, 29659, 76394, 29659, 20129, 29659, \n",
      "75193, 29659, 114916, 29659, 84010, 29659, 40667, 29659, 35389, 29659, 14892, 23354, 29659, 44717, 76330, 29659, 71390, 23354, 29659, 112669, \n",
      "125160, 63075, 29659, 71231, 23354, 29659, 118971, 29659, 26296, 74231, 84422, 29659, 37752, 29659, 24487, 29659, 80365, 104505, 46746, 29659, \n",
      "29659, 56921, 54027, 4882, 112461, 60670, 29659, 105102, 54829, 29659, 95110, 4882, 73113, 88423, 29659, 107316, 125160, 53609, 125160, 86690, \n",
      "29659, 105102, 86882, 29659, 54516, 96090, 42207, 73743, 29659, 92995, 14212, 4882, 118825, 29659, 64848, 58811, 23354, 43712, 29659, 1253, \n",
      "99602, 54829, 120440, 29659, 109205, 118825, 23354, 29659, 31423, 116655, 29659, 99001, 86882, 29659, 99001, 112461, 4882, 29659, 1253, 54829, \n",
      "99001, 29659, 35384, 125160, 53609, 125160, 86690, 29659, 105102, 86882, 29659, 37143, 88423, 112461, 96423, 73113, 116655, 50515, 29659, 14212, \n",
      "118825, 29659, 46320, 1527, 29659, 92995, 75919, 54829, 116655, 29659, 60670, 118825, 37099, 55972, 29659, 58811, 84422, 62497, 29659, 99602, \n",
      "60670, 54829, 73743, 56921, 46746, 46746, 29659, 29659, 112821, 29659, 48368, 104505, 29659, 51894, 29659, 595, 46746, 29659, 29659, 21801, \n",
      "29659, 37987, 29659, 60434, 24183, 118575, 72302, 29659, 10303, 29659, 24487, 29659, 108084, 104505, 46746, 29659, 29659, 120523, 29659, 31655, \n",
      "29659, 81983, 120607, 40011, 62678, 48368, 23354, 19473, 83035, 82505, 34556, 62691, 100523, 18725, 82505, 9986, 40011, 62678, 48368, 23354, \n",
      "19473, 92697, 5601, 78517, 82505, 34556, 62691, 100523, 18725, 82505, 9986, 40011, 62678, 48368, 23354, 19473, 9687, 46046, 16578, 46746, \n",
      "29659, 29659, 21798, 104505, 46746, 29659, 29659, 56921, 107316, 125160, 53609, 125160, 86690, 56921, 46746, 29659, 29659, 56921, 35384, 125160, \n",
      "53609, 125160, 86690, 56921, 46746, 46746, 46746, 29659, 29659, 73596, 29659, 119792, 29659, 114922, 23354, 29659, 97661, 29659, 42869, 29659, \n",
      "120297, 57188, 3662, 84422, 46746, 29659, 29659, 56921, 107316, 125160, 53609, 125160, 86690, 56921, 29659, 88590, 29659, 108194, 29659, 76631, \n",
      "29659, 18646, 125160, 61210, 125160, 58074, 58074, 84422, 29659, 20029, 31655, 107316, 23354, 29659, 24495, 31655, 63150, 23354, 29659, 43502, \n",
      "31655, 86690, 29659, 88590, 29659, 56921, 107316, 29659, 88423, 54829, 116655, 73113, 99602, 29659, 86690, 111037, 35389, 80210, 46746, 29659, \n",
      "29659, 56921, 35384, 125160, 53609, 125160, 86690, 56921, 29659, 88590, 29659, 20029, 31655, 35384, 23354, 29659, 24495, 31655, 63150, 23354, \n",
      "29659, 43502, 31655, 86690, 29659, 88590, 29659, 56921, 35384, 29659, 88423, 54829, 116655, 73113, 99602, 29659, 86690, 111037, 35389, 80210, \n",
      "46746, 29659, 29659, 21801, 29659, 114210, 29659, 107370, 104505, 46746, 29659, 29659, 56921, 54027, 4882, 112461, 60670, 29659, 105102, 54829, \n",
      "29659, 95110, 4882, 73113, 88423, 29659, 107316, 29659, 88423, 54829, 116655, 73113, 99602, 29659, 86690, 111037, 35389, 29659, 105102, 86882, \n",
      "29659, 54516, 96090, 42207, 73743, 29659, 92995, 14212, 4882, 118825, 29659, 64848, 58811, 23354, 43712, 29659, 1253, 99602, 54829, 120440, \n",
      "29659, 109205, 118825, 23354, 29659, 31423, 116655, 29659, 99001, 86882, 29659, 99001, 112461, 4882, 29659, 1253, 54829, 99001, 29659, 35384, \n",
      "29659, 88423, 54829, 116655, 73113, 99602, 29659, 86690, 111037, 35389, 29659, 105102, 86882, 29659, 37143, 88423, 112461, 96423, 73113, 116655, \n",
      "50515, 29659, 14212, 118825, 29659, 46320, 1527, 29659, 92995, 75919, 54829, 116655, 29659, 60670, 118825, 37099, 55972, 29659, 58811, 84422, \n",
      "62497, 29659, 99602, 60670, 54829, 73743, 56921, 46746, 29659, 29659, 112821, 29659, 58811, 104505, 29659, 98695, 127440, 29659, 85851, 29659, \n",
      "118971, 29659, 91081, 46746, 29659, 29659, 71553, 29659, 115428, 29659, 24487, 29659, 91172, 29659, 98909, 29659, 119970, 29659, 99343, 3850, \n",
      "101040, 59604, 37143, 114210, 23354, 29659, 46164, 92402, 37610, 119052, 29659, 81111, 104505, 46746, 29659, 29659, 114714, 35290, 104505, 46746, \n",
      "29659, 29659, 15957, 75977, 29659, 123538, 29659, 59604, 23354, 29659, 31917, 103770, 23354, 29659, 107451, 68935, 29659, 37143, 90575, 29659, \n",
      "56921, 54027, 4882, 112461, 60670, 30346, 29659, 56921, 29659, 30346, 29659, 56921, 105102, 54829, 30346, 29659, 56921, 29659, 30346, 29659, \n",
      "101053, 48464, 46746, 46746, 46746, 29659, 29659, 117505, 11424, 87251, 8030, 104505, 46746, 29659, 29659, 75447, 29659, 81638, 29659, 56095, \n",
      "29659, 40952, 29659, 16578, 86690, 114348, 29659, 38304, 114348, 29659, 7915, 114348, 29659, 16578, 35389, 16578, 29659, 101480, 29659, 88590, \n",
      "29659, 10593, 119030, 29659, 88590, 29659, 83694, 86690, 111037, 35389, 91062, 46746, 29659, 29659, 12508, 84422, 113040, 100523, 29659, 24487, \n",
      "29659, 88646, 26088, 29659, 40952, 29659, 56921, 107316, 30346, 29659, 56921, 29659, 30346, 29659, 56921, 88423, 54829, 116655, 73113, 99602, \n",
      "30346, 29659, 56921, 29659, 30346, 29659, 56921, 86690, 30346, 29659, 7165, 30346, 29659, 117880, 30346, 29659, 56921, 35389, 56921, 29659, \n",
      "101480, 29659, 107370, 29659, 40952, 29659, 56921, 107316, 30346, 29659, 56921, 29659, 30346, 29659, 56921, 88423, 54829, 116655, 73113, 99602, \n",
      "30346, 29659, 56921, 29659, 30346, 29659, 56921, 86690, 111037, 35389, 56921, 29659, 105066, 46746, 29659, 29659, 115217, 29659, 97661, 29659, \n",
      "39285, 29659, 84564, 29659, 90575, 29659, 56921, 107316, 30346, 29659, 56921, 88423, 54829, 116655, 73113, 99602, 30346, 29659, 56921, 86690, \n",
      "111037, 35389, 30346, 29659, 101053, 84422, 46746, 46746, 46746, 29659, 29659, 117505, 74678, 71648, 108784, 128373, 72110, 29659, 118971, 29659, \n",
      "87528, 121581, 17401, 111422, 59216, 72110, 104505, 46746, 29659, 29659, 10050, 119030, 29659, 40952, 29659, 16578, 58811, 114348, 29659, 114348, \n",
      "114348, 29659, 16578, 43712, 16578, 29659, 101480, 29659, 119970, 29659, 40952, 29659, 16578, 53851, 16578, 29659, 101480, 29659, 110466, 29659, \n",
      "24242, 84422, 46746, 29659, 29659, 10050, 119030, 29659, 40952, 29659, 16578, 58811, 114348, 29659, 647, 114348, 29659, 16578, 75401, 16578, \n",
      "29659, 101480, 29659, 119970, 29659, 40952, 29659, 16578, 58811, 84422, 75401, 16578, 29659, 105066, 46746, 29659, 29659, 129289, 29659, 47633, \n",
      "29659, 114916, 23354, 29659, 64848, 58811, 23354, 43712, 29659, 88590, 29659, 84564, 29659, 18549, 29659, 42626, 29659, 83694, 10345, 23354, \n",
      "29659, 56921, 58811, 30346, 29659, 30346, 30346, 29659, 56921, 43712, 129169, 29659, 88590, 29659, 51692, 29659, 83694, 10345, 23354, 29659, \n",
      "56921, 53851, 129169, 118971, 29659, 29659, 58811, 84422, 62497, 29659, 88590, 29659, 83694, 58811, 30346, 29659, 80210, 30346, 29659, 56921, \n",
      "75401, 30346, 29659, 56921, 91860, 129169, 29659, 88590, 29659, 51692, 29659, 83694, 58811, 84422, 75401, 30346, 29659, 56921, 91860, 91062, \n",
      "46746, 46746, 46746, 29659, 29659, 117505, 90066, 11155, 54244, 8030, 104505, 46746, 29659, 29659, 35721, 46344, 29659, 76394, 29659, 120523, \n",
      "29659, 90575, 29659, 83694, 10345, 23354, 29659, 56921, 53851, 91062, 29659, 64848, 29659, 34424, 29659, 56921, 117405, 30346, 29659, 37480, \n",
      "29659, 10303, 29659, 112669, 83867, 65347, 5919, 22758, 83694, 117405, 129169, 83694, 37610, 129169, 29659, 31655, 29659, 56921, 111070, 117448, \n",
      "60670, 116655, 80210, 46746, 29659, 29659, 69943, 29659, 10593, 119030, 29659, 119970, 29659, 76394, 29659, 92170, 29659, 124251, 29659, 56921, \n",
      "53851, 29659, 111070, 117448, 60670, 116655, 56921, 29659, 112143, 29659, 115661, 29659, 116826, 29659, 81638, 29659, 44717, 76330, 29659, 52935, \n",
      "84422, 46746, 46746, 46746, 29659, 29659, 117505, 63375, 39206, 8030, 104505, 46746, 29659, 29659, 73596, 29659, 40952, 29659, 56921, 58811, \n",
      "84422, 75401, 30346, 29659, 56921, 91860, 56921, 29659, 96041, 29659, 97661, 29659, 95166, 29659, 110466, 29659, 131072, 91860, 131072, 29659, \n",
      "44405, 29659, 8930, 84422, 29659, 83653, 29659, 97661, 29659, 54695, 29659, 81173, 29659, 90575, 29659, 56921, 58811, 84422, 75401, 29659, \n",
      "105102, 112461, 60670, 86882, 88423, 11126, 31050, 116655, 80210, 46746, 29659, 29659, 108962, 29659, 131072, 32224, 131072, 29659, 44405, 29659, \n",
      "8930, 29659, 88590, 29659, 16578, 32224, 16578, 29659, 34424, 29659, 16578, 65496, 54829, 92995, 102260, 111070, 16578, 29659, 51398, 29659, \n",
      "40071, 29659, 112143, 29659, 16578, 65496, 54829, 92995, 61265, 73113, 111070, 16578, 29659, 112746, 29659, 123538, 29659, 24487, 29659, 60257, \n",
      "84422, 46746, 29659, 29659, 75094, 29659, 83694, 127635, 30346, 29659, 56921, 32224, 129169, 29659, 18549, 29659, 38867, 29659, 56921, 127635, \n",
      "29659, 65496, 54829, 92995, 102260, 111070, 80210, 46746, 46746, 46746, 29659, 29659, 43867, 29659, 47198, 29659, 83359, 104505, 46746, 29659, \n",
      "29659, 108962, 29659, 76394, 29659, 124251, 29659, 44405, 29659, 56921, 58811, 84422, 75401, 30346, 29659, 97661, 29659, 42869, 29659, 68443, \n",
      "11609, 23202, 95098, 17399, 58811, 84422, 75401, 30346, 29659, 56921, 37610, 92866, 29659, 88590, 29659, 131072, 99001, 86882, 29659, 99001, \n",
      "106221, 88423, 60670, 83932, 29659, 65496, 54829, 107765, 99602, 84422, 131072, 46746, 29659, 29659, 108962, 29659, 76394, 29659, 124251, 29659, \n",
      "44405, 29659, 56921, 53851, 56921, 29659, 88590, 29659, 131072, 99001, 86882, 29659, 54516, 95110, 89809, 54829, 116655, 84422, 131072, 46746, \n",
      "29659, 29659, 108962, 29659, 76394, 29659, 124251, 29659, 44405, 29659, 56921, 86690, 111037, 35389, 30346, 29659, 68443, 11609, 23202, 95098, \n",
      "29659, 128842, 29659, 111037, 35389, 29659, 118971, 29659, 40153, 29659, 27597, 29659, 35389, 90001, 43502, 29659, 71390, 29659, 37143, 90575, \n",
      "29659, 131072, 92995, 4882, 73113, 4882, 11126, 14212, 29659, 14212, 67301, 29659, 4882, 1253, 73113, 1253, 118825, 131072, 29659, 51398, \n",
      "29659, 40071, 15158, 29659, 21801, 29659, 84822, 29659, 64018, 29659, 111037, 35389, 29659, 113290, 29659, 76394, 29659, 119552, 29659, 128237, \n",
      "29659, 84010, 29659, 18549, 29659, 83920, 29659, 93925, 29659, 22736, 76330, 29659, 37143, 81638, 29659, 125625, 90001, 125857, 29659, 103028, \n",
      "29659, 97661, 29659, 18549, 29659, 93894, 29659, 131072, 14212, 4882, 73113, 29659, 81577, 29659, 69575, 14212, 73113, 83932, 11126, 131072, \n",
      "15158, 46746, 29659, 29659, 27750, 29659, 97661, 29659, 18849, 29659, 76394, 29659, 11241, 29659, 114210, 29659, 81173, 29659, 90575, 104505, \n",
      "46746, 29659, 29659, 56921, 54027, 4882, 112461, 60670, 29659, 105102, 54829, 29659, 95110, 4882, 73113, 88423, 29659, 65496, 102260, 99001, \n",
      "73113, 116655, 54516, 29659, 88423, 54829, 116655, 73113, 99602, 29659, 92995, 4882, 73113, 4882, 11126, 14212, 29659, 14212, 67301, 29659, \n",
      "4882, 1253, 73113, 1253, 118825, 29659, 105102, 86882, 29659, 54516, 96090, 42207, 73743, 29659, 92995, 14212, 4882, 118825, 29659, 99001, \n",
      "86882, 29659, 54516, 95110, 89809, 54829, 116655, 29659, 111070, 117448, 60670, 116655, 29659, 1253, 99602, 54829, 120440, 29659, 109205, 118825, \n",
      "23354, 29659, 31423, 116655, 29659, 99001, 86882, 29659, 99001, 112461, 4882, 29659, 1253, 54829, 99001, 29659, 54027, 15825, 54829, 116655, \n",
      "54516, 29659, 88423, 54829, 116655, 73113, 99602, 29659, 92995, 4882, 73113, 4882, 11126, 14212, 29659, 14212, 67301, 29659, 4882, 1253, \n",
      "73113, 1253, 118825, 29659, 105102, 86882, 29659, 37143, 88423, 112461, 96423, 73113, 116655, 50515, 29659, 14212, 118825, 29659, 120440, 105102, \n",
      "29659, 14212, 67301, 29659, 65496, 54829, 92995, 102260, 111070, 29659, 92995, 75919, 54829, 116655, 29659, 60670, 118825, 37099, 55972, 29659, \n",
      "99001, 86882, 29659, 99001, 106221, 88423, 60670, 83932, 29659, 65496, 54829, 107765, 99602, 29659, 105102, 112461, 60670, 86882, 88423, 11126, \n",
      "31050, 116655, 29659, 99602, 60670, 54829, 73743, 56921, 46746, 29659, 29659, 84097, 29659, 119792, 29659, 44717, 76330, 29659, 62965, 29659, \n",
      "37143, 107316, 23354, 29659, 86690, 111037, 35389, 23354, 29659, 53851, 23354, 29659, 127635, 23354, 29659, 58811, 84422, 75401, 55972, 29659, \n",
      "44405, 29659, 54225, 29659, 77197, 29659, 51398, 29659, 40071, 84422, 46746, 29659, 29659, 21801, 29659, 24495, 29659, 71390, 29659, 37143, \n",
      "131072, 88423, 54829, 116655, 73113, 99602, 131072, 55972, 29659, 14701, 29659, 55641, 29659, 35389, 29659, 14892, 23354, 29659, 118971, 29659, \n",
      "24487, 29659, 43502, 29659, 71390, 29659, 107734, 29659, 24487, 29659, 35389, 90001, 25108, 29659, 44717, 76330, 29659, 22736, 76330, 29659, \n",
      "10303, 29659, 111037, 35389, 29659, 54800, 49961, 84422, 46746, 29659, 29659, 26243, 29659, 64848, 29659, 118538, 29659, 119970, 29659, 131072, \n",
      "111070, 117448, 60670, 116655, 23354, 131072, 29659, 118971, 29659, 32224, 29659, 118538, 29659, 119970, 29659, 131072, 65496, 54829, 92995, 102260, \n",
      "111070, 84422, 131072, 46746, 29659, 29659, 129289, 29659, 83968, 23354, 29659, 24487, 29659, 11241, 29659, 75193, 29659, 81638, 29659, 76394, \n",
      "29659, 59827, 29659, 80365, 29659, 10303, 29659, 902, 29659, 35389, 29659, 71225, 29659, 44405, 104505, 46746, 29659, 29659, 60434, 24183, \n",
      "118575, 72302, 29659, 27541, 29659, 108194, 23354, 29659, 73768, 23354, 29659, 118971, 29659, 24505, 29659, 35389, 29659, 56095, 29659, 119970, \n",
      "29659, 76394, 29659, 129262, 29659, 88908, 20029, 13920, 29659, 88908, 24495, 13920, 29659, 88908, 43502, 26217, 64848, 35389, 29659, 37208, \n",
      "84422, 46746, 29659, 29659, 99343, 3850, 101040, 59604, 29659, 27541, 29659, 83920, 29659, 35389, 29659, 54800, 49961, 23354, 29659, 44717, \n",
      "76330, 29659, 71390, 23354, 29659, 112669, 29659, 71390, 23354, 29659, 118971, 29659, 63075, 29659, 71390, 84422, 46746, 29659, 29659, 45242, \n",
      "31033, 29659, 84564, 29659, 45382, 29659, 27541, 29659, 68559, 29659, 24487, 29659, 51051, 29659, 80373, 29659, 118971, 29659, 31917, 103770, \n",
      "84422, 46746, 29659, 29659, 125366, 29659, 50572, 29659, 45813, 29659, 128878, 29659, 27733, 29659, 109428, 29659, 5142, 103770, 29659, 57121, \n",
      "29659, 57639, 99687, 29659, 40304, 29659, 71390, 29659, 81638, 29659, 8930, 29659, 56095, 121287, 130036, 29659, 75557, 23354, 29659, 38125, \n",
      "29659, 27541, 29659, 76394, 29659, 96781, 29659, 35389, 29659, 7555, 29659, 118971, 29659, 129262, 29659, 24495, 90001, 63964, 80898, 84422, \n",
      "46746, 29659, 29659, 63150, 84422, 58811, 84422, 7020, 29659, 97444, 29659, 105072, 29659, 1301, 29659, 125726, 120401, 46746, 29659, 29659, \n",
      "129251, 29659, 23322, 104505, 29659, 108962, 29659, 97661, 29659, 2105, 29659, 81173, 29659, 90575, 29659, 131072, 19473, 84422, 14333, 125160, \n",
      "82505, 131072, 29659, 37480, 29659, 131072, 15330, 125160, 82505, 131072, 29659, 61855, 131072, 24337, 29659, 51398, 29659, 24487, 29659, 63075, \n",
      "29659, 60257, 23354, 29659, 97661, 29659, 122907, 29659, 112143, 29659, 49417, 29659, 99343, 29659, 26845, 29659, 24487, 29659, 69608, 84422, \n",
      "46746, 46746, 46746, 29659, 29659, 1379, 29659, 19387, 72382, 104505, 29659, 108962, 29659, 76394, 29659, 35389, 29659, 17132, 29659, 119250, \n",
      "29659, 76394, 29659, 112669, 29659, 72109, 29659, 37143, 13377, 29659, 37480, 29659, 108278, 23354, 29659, 97713, 84422, 113040, 100523, 29659, \n",
      "131072, 24763, 125160, 53609, 90001, 86690, 64848, 131072, 54433, 29659, 24487, 29659, 99481, 29659, 18549, 29659, 43646, 29659, 108194, 84422, \n",
      "29659, 17736, 23354, 29659, 97661, 29659, 93894, 29659, 902, 29659, 84359, 29659, 112143, 29659, 54150, 29659, 113299, 29659, 51372, 29659, \n",
      "107309, 29659, 108084, 29659, 27541, 29659, 68852, 29659, 114835, 84422, 29659, 95058, 29659, 76394, 29659, 94340, 29659, 49155, 29659, 118971, \n",
      "29659, 120538, 29659, 128878, 29659, 44405, 29659, 52964, 29659, 27541, 29659, 43954, 29659, 27597, 29659, 56359, 23354, 29659, 131072, 90001, \n",
      "131072, 23354, 29659, 131072, 64848, 131072, 23354, 29659, 44946, 23354, 29659, 72110, 23354, 29659, 101053, 84422, 29659, 82761, 29659, 76394, \n",
      "29659, 94340, 29659, 120538, 29659, 121913, 29659, 44405, 29659, 127333, 29659, 77197, 29659, 81638, 29659, 24487, 29659, 29829, 29659, 27541, \n",
      "29659, 16258, 29659, 125880, 29659, 81638, 29659, 3935, 84422, 29659, 108349, 23354, 29659, 51398, 29659, 111384, 29659, 50342, 23354, 29659, \n",
      "76394, 29659, 128237, 90001, 25108, 29659, 128878, 29659, 95769, 29659, 125299, 29659, 42626, 29659, 45370, 29659, 37143, 65611, 29659, 35292, \n",
      "29659, 27541, 29659, 24487, 29659, 108112, 29659, 64018, 29659, 111384, 29659, 7025, 23354, 29659, 125299, 29659, 120747, 29659, 81638, 29659, \n",
      "37143, 81111, 29659, 124461, 29659, 42626, 29659, 120894, 29659, 58999, 29659, 81638, 29659, 76394, 29659, 38802, 29659, 100192, 29659, 99481, \n",
      "54433, 29659, 112143, 29659, 35292, 29659, 27541, 29659, 24487, 29659, 38882, 29659, 64758, 29659, 64018, 29659, 3935, 29659, 24487, 29659, \n",
      "66084, 29659, 81638, 29659, 81111, 29659, 7025, 29659, 95769, 29659, 125299, 29659, 61084, 55972, 46746, 46746, 46746, 29659, 29659, 4653, \n",
      "29659, 106423, 23131, 20261, 104505, 29659, 108962, 29659, 24487, 29659, 114210, 29659, 44405, 29659, 49417, 29659, 108867, 29659, 112143, 29659, \n",
      "121041, 90001, 18636, 23354, 29659, 71390, 29659, 18549, 29659, 21528, 29659, 40362, 29659, 51398, 29659, 24487, 29659, 66020, 29659, 22315, \n",
      "29659, 44833, 84422, 29659, 71553, 29659, 124461, 29659, 65611, 29659, 93894, 29659, 13593, 29659, 71390, 29659, 112143, 29659, 47308, 118702, \n",
      "84422, 46746, 46746, 46746, 29659, 29659, 38307, 124403, 104505, 29659, 71553, 29659, 92364, 29659, 117248, 29659, 84010, 29659, 12064, 29659, \n",
      "117535, 29659, 24487, 29659, 120523, 29659, 55641, 29659, 24487, 29659, 114210, 23354, 29659, 97661, 29659, 54800, 29659, 24487, 29659, 98046, \n",
      "29659, 24192, 29659, 10303, 29659, 40304, 29659, 80373, 84422, 29659, 17434, 29659, 121041, 29659, 38032, 29659, 112316, 29659, 10303, 29659, \n",
      "109791, 29659, 80824, 29659, 27541, 29659, 83920, 29659, 24487, 29659, 80373, 29659, 10468, 94498, 29659, 37143, 97713, 84422, 113040, 100523, \n",
      "29659, 76394, 29659, 109791, 29659, 56534, 29659, 81638, 29659, 99679, 125160, 116195, 29659, 44946, 15158, 46746, 29659, 29659, 63150, 84422, \n",
      "58811, 84422, 128544, 29659, 116126, 26171, 29659, 1301, 29659, 62807, 120401, 29659, 51316, 24368, 46746, 29659, 29659, 116126, 26171, 104505, \n",
      "46746, 29659, 29659, 44515, 29659, 95138, 68590, 127999, 104505, 29659, 71553, 29659, 26845, 29659, 24192, 29659, 71390, 29659, 81638, 29659, \n",
      "8930, 29659, 56095, 84422, 46746, 29659, 29659, 127538, 29659, 40726, 11526, 29659, 81638, 29659, 106541, 29659, 46931, 104505, 29659, 108962, \n",
      "29659, 131072, 64848, 129713, 131072, 29659, 44405, 29659, 51398, 29659, 24487, 29659, 60257, 23354, 29659, 97661, 29659, 93894, 29659, 124759, \n",
      "29659, 10089, 29659, 127635, 36937, 29659, 64018, 29659, 24487, 29659, 36097, 84422, 46746, 29659, 29659, 98856, 98733, 104505, 29659, 44515, \n",
      "29659, 72960, 29659, 112143, 29659, 101457, 29659, 96382, 29659, 86867, 84422, 29659, 17736, 29659, 4133, 29659, 51398, 29659, 83324, 29659, \n",
      "95449, 90001, 36097, 84422, 46746, 46746, 46746, 29659, 29659, 62807, 120401, 104505, 46746, 29659, 29659, 98116, 104505, 29659, 27064, 29659, \n",
      "89128, 29659, 81638, 39854, 29659, 112143, 29659, 13377, 29659, 104419, 29659, 37143, 90575, 29659, 131072, 15330, 125160, 82505, 131072, 23354, \n",
      "29659, 131072, 118735, 29659, 20757, 131072, 55972, 29659, 92364, 29659, 42626, 29659, 120894, 29659, 58999, 84422, 46746, 29659, 29659, 3290, \n",
      "104505, 29659, 73596, 29659, 46320, 29659, 103028, 23354, 29659, 119792, 29659, 89128, 29659, 35389, 29659, 112143, 29659, 112669, 29659, 32638, \n",
      "29659, 107370, 29659, 76394, 29659, 22746, 29659, 64018, 29659, 89128, 29659, 121041, 29659, 112143, 29659, 60257, 29659, 71390, 84422, 46746, \n",
      "29659, 29659, 64402, 29659, 17863, 46585, 29659, 112143, 29659, 38453, 90001, 80898, 64143, 29659, 59864, 29659, 92171, 29659, 125299, 29659, \n",
      "78794, 29659, 36407, 23354, 29659, 76631, 29659, 119792, 29659, 44833, 131072, 82505, 29659, 71390, 29659, 18549, 29659, 8704, 29659, 112143, \n",
      "29659, 6634, 84422, 46746, 29659, 29659, 1590, 29659, 6471, 29659, 16770, 23354, 29659, 122612, 29659, 95449, 90001, 8367, 29659, 65255, \n",
      "29659, 45667, 29659, 123538, 29659, 128237, 90001, 25108, 29659, 71390, 29659, 29974, 29659, 127931, 23354, 29659, 127695, 29659, 81935, 83555, \n",
      "29659, 44405, 29659, 17751, 84422, 29659, 73596, 29659, 89128, 29659, 112143, 29659, 4956, 66595, 29659, 130013, 23354, 29659, 78208, 29659, \n",
      "78655, 38509, 29659, 124461, 29659, 62865, 29659, 15706, 29659, 85695, 23354, 29659, 37480, 29659, 24487, 29659, 128237, 90001, 25108, 29659, \n",
      "121913, 29659, 38770, 29659, 76394, 29659, 130909, 29659, 47308, 118702, 29659, 112143, 29659, 96594, 90001, 101490, 84422, 46746, 29659, 29659, \n",
      "63150, 84422, 58811, 84422, 46320, 29659, 94718, 29659, 23494, 29659, 1301, 29659, 40286, 29659, 51329, 98460, 46746, 29659, 29659, 40286, \n",
      "29659, 25138, 104505, 46746, 29659, 29659, 115236, 29659, 24487, 29659, 78655, 38509, 29659, 35102, 29659, 71390, 23354, 29659, 38852, 29659, \n",
      "108194, 29659, 84010, 29659, 24192, 29659, 10303, 29659, 76394, 29659, 131072, 72685, 29659, 54059, 131072, 29659, 64018, 29659, 11307, 29659, \n",
      "128237, 90001, 25108, 29659, 99481, 84422, 29659, 108962, 29659, 71390, 29659, 22061, 29659, 55641, 29659, 8930, 29659, 56095, 23354, 29659, \n",
      "40304, 29659, 61086, 84422, 46746, 46746, 46746, 29659, 29659, 84478, 90001, 43245, 29659, 64018, 29659, 92862, 104505, 46746, 29659, 29659, \n",
      "17434, 29659, 27584, 29659, 65255, 29659, 127291, 29659, 27541, 29659, 108194, 29659, 103652, 29659, 69192, 29659, 27541, 29659, 91808, 90001, \n",
      "114738, 29659, 24487, 29659, 60257, 29659, 112143, 29659, 56095, 29659, 81638, 29659, 89128, 29659, 104998, 125160, 11193, 84422, 29659, 125366, \n",
      "29659, 130953, 29659, 919, 84422, 46746, 46746, 46746, 29659, 29659, 4653, 29659, 51978, 104505, 46746, 29659, 29659, 73596, 29659, 121041, \n",
      "90001, 18636, 29659, 114210, 29659, 37143, 131072, 89073, 29659, 67719, 29659, 64848, 129713, 29659, 116655, 33914, 65496, 82019, 118825, 131072, \n",
      "54433, 29659, 97661, 29659, 124461, 29659, 127291, 29659, 123610, 90001, 29775, 29659, 71231, 84422, 29659, 97713, 84422, 113040, 100523, 29659, \n",
      "110466, 29659, 76394, 29659, 22746, 29659, 44405, 29659, 51398, 29659, 36093, 97424, 107595, 23354, 29659, 97661, 29659, 116511, 29659, 71390, \n",
      "29659, 51398, 29659, 40071, 23354, 29659, 112106, 29659, 51398, 29659, 108867, 84422, 29659, 125366, 29659, 124461, 29659, 38867, 29659, 59827, \n",
      "29659, 51398, 29659, 82671, 84422, 46746, 29659, 29659, 63150, 84422, 58811, 84422, 35158, 29659, 43655, 29659, 51000, 26061, 46746, 29659, \n",
      "29659, 21801, 29659, 45813, 29659, 99481, 29659, 44405, 29659, 84219, 29659, 84665, 29659, 81638, 29659, 51398, 90001, 88932, 23354, 29659, \n",
      "8930, 29659, 56095, 84422, 46746, 29659, 29659, 79207, 29659, 84359, 29659, 112143, 29659, 76394, 29659, 92170, 29659, 38037, 90001, 108084, \n",
      "29659, 124461, 29659, 22057, 29659, 130036, 23354, 29659, 112669, 23354, 29659, 11193, 23354, 29659, 27297, 29659, 71390, 23354, 29659, 44717, \n",
      "76330, 29659, 71390, 23354, 29659, 101053, 84422, 46746, 29659, 29659, 21801, 29659, 128878, 29659, 93799, 29659, 45382, 29659, 66602, 29659, \n",
      "80735, 8967, 29659, 37143, 24495, 29659, 53876, 23354, 29659, 112669, 29659, 71390, 23354, 29659, 63075, 29659, 71390, 55972, 29659, 81638, \n",
      "29659, 119792, 29659, 18456, 76330, 29659, 22315, 84422, 46746, 29659, 29659, 73596, 29659, 95449, 29659, 5976, 29659, 62630, 23354, 29659, \n",
      "124194, 29659, 76394, 29659, 128237, 90001, 25108, 29659, 99481, 29659, 10303, 29659, 78208, 29659, 78655, 38509, 131072, 82505, 29659, 38802, \n",
      "29659, 125831, 29659, 15706, 29659, 124461, 29659, 53024, 29659, 83324, 90001, 72016, 29659, 68880, 84422, 46746, 46746, 46746, 46746, 46746, \n",
      "46746, 46746, 46746, 46746, 46746, 46746, 46746, 46746, 46746, 46746, 46746, 46746, 46746, 46746, 46746, 46746, 46746, 46746, 46746, 46746, \n",
      "46746, 46746, 46746, 46746, 46746, 46746, 46746, 46746, 46746, 46746, 46746, 29659, 29659, 63150, 84422, 63150, 29659, 27043, 90001, 26263, \n",
      "58866, 29659, 78655, 38509, 29659, 90686, 46746, 29659, 29659, 57417, 104505, 29659, 56652, 29659, 78208, 29659, 78655, 38509, 29659, 24487, \n",
      "29659, 112404, 29659, 55641, 29659, 44717, 76330, 29659, 46931, 29659, 27541, 29659, 54225, 90001, 77197, 29659, 46931, 29659, 172, 29659, \n",
      "51088, 29659, 124759, 29659, 27541, 29659, 34004, 29659, 64018, 29659, 14357, 29659, 55641, 29659, 24487, 29659, 93136, 29659, 55153, 84422, \n",
      "46746, 29659, 29659, 63150, 84422, 63150, 84422, 48368, 29659, 68377, 29659, 31308, 46746, 29659, 29659, 71553, 29659, 107734, 29659, 131072, \n",
      "11867, 5226, 79390, 125160, 60422, 90001, 24672, 28518, 90001, 63150, 84422, 48368, 90001, 1472, 131072, 29659, 51398, 29659, 19473, 90001, \n",
      "41606, 29659, 23831, 103770, 84422, 29659, 10347, 131072, 82505, 29659, 101457, 29659, 45370, 29659, 27541, 29659, 83920, 29659, 38453, 90001, \n",
      "80898, 64143, 29659, 117264, 29659, 87042, 40906, 29659, 37480, 29659, 21528, 29659, 108278, 29659, 27541, 29659, 113408, 90001, 57498, 29659, \n",
      "51398, 29659, 37604, 8219, 75174, 29659, 18462, 26143, 29659, 37143, 10303, 29659, 5936, 29659, 104460, 29659, 3602, 15158, 29659, 21801, \n",
      "29659, 23831, 103770, 29659, 37143, 41355, 102360, 109710, 29659, 19473, 90001, 41606, 55972, 29659, 27733, 29659, 32876, 29659, 66636, 29659, \n",
      "62630, 84422, 29659, 99109, 29659, 101273, 29659, 81638, 29659, 96382, 29659, 4583, 29659, 44405, 29659, 88908, 38782, 29659, 123538, 29659, \n",
      "72960, 84422, 46746, 29659, 29659, 63150, 84422, 63150, 84422, 58811, 29659, 71336, 29659, 1301, 29659, 42653, 29659, 110472, 46746, 29659, \n",
      "29659, 71553, 29659, 52741, 40011, 29659, 119792, 29659, 24550, 29659, 114916, 29659, 51398, 29659, 24487, 29659, 6848, 84422, 29659, 75094, \n",
      "29659, 84010, 29659, 24487, 29659, 96382, 29659, 124461, 29659, 78794, 29659, 4329, 29659, 128095, 29659, 44405, 29659, 27541, 29659, 42626, \n",
      "29659, 119381, 29659, 118971, 29659, 124461, 29659, 94344, 29659, 96744, 29659, 24487, 29659, 64480, 84422, 29659, 21801, 29659, 88633, 29659, \n",
      "6848, 29659, 124461, 29659, 42626, 29659, 65208, 29659, 51398, 29659, 24487, 29659, 113408, 85363, 29659, 6848, 29659, 24337, 6458, 84422, \n",
      "46746, 29659, 29659, 71553, 29659, 41946, 29659, 76394, 29659, 92170, 29659, 114916, 29659, 51398, 29659, 24487, 29659, 6848, 29659, 19484, \n",
      "29659, 76631, 29659, 43246, 29659, 37143, 131072, 107316, 125160, 53609, 125160, 86690, 29659, 105102, 86882, 91544, 131072, 15158, 29659, 21801, \n",
      "29659, 24550, 29659, 129785, 29659, 128842, 29659, 48368, 23354, 69317, 29659, 64018, 29659, 6471, 29659, 14357, 23354, 29659, 119792, 29659, \n",
      "51398, 29659, 902, 29659, 84199, 84422, 29659, 21801, 29659, 51080, 29659, 52180, 29659, 44405, 29659, 11999, 29659, 14357, 84422, 46746, \n",
      "29659, 29659, 2876, 131072, 82505, 29659, 78208, 29659, 49762, 29659, 15814, 29659, 64018, 29659, 24487, 29659, 2193, 80251, 29659, 1301, \n",
      "29659, 9202, 29659, 80591, 29659, 40740, 23354, 29659, 109996, 83202, 29659, 84010, 29659, 24487, 29659, 31384, 90001, 17442, 29659, 131072, \n",
      "117229, 131072, 29659, 8404, 29659, 125078, 131072, 24337, 29659, 106119, 29659, 110440, 29659, 37480, 29659, 94190, 23354, 29659, 51398, 29659, \n",
      "4396, 23354, 29659, 53024, 29659, 113378, 29659, 46730, 29659, 37143, 115417, 29659, 7480, 55972, 29659, 25635, 104505, 46746, 29659, 29659, \n",
      "63150, 84422, 63150, 84422, 63150, 29659, 2193, 80251, 29659, 1301, 29659, 26263, 99687, 29659, 109135, 46746, 29659, 29659, 71553, 29659, \n",
      "60294, 29659, 902, 29659, 62472, 29659, 24550, 29659, 4133, 23354, 29659, 85363, 29659, 99663, 29659, 103653, 80251, 29659, 110951, 29659, \n",
      "76631, 29659, 73073, 29659, 74623, 23354, 29659, 98733, 29659, 48280, 23354, 29659, 27026, 88648, 29659, 197, 23354, 29659, 118971, 29659, \n",
      "72695, 29659, 68209, 72863, 84422, 29659, 59421, 29659, 44405, 29659, 76394, 29659, 16550, 29659, 64018, 29659, 35954, 29659, 95879, 29659, \n",
      "4133, 29659, 56129, 29659, 118971, 29659, 769, 84422, 46746, 29659, 29659, 89164, 29659, 128035, 29659, 67281, 29659, 109135, 46746, 29659, \n",
      "29659, 35323, 71031, 29659, 65502, 104505, 29659, 71553, 29659, 90133, 29659, 9202, 29659, 124973, 23354, 29659, 35938, 29659, 124973, 29659, \n",
      "37143, 9460, 54433, 29659, 6973, 29659, 102987, 64393, 23354, 29659, 102987, 64393, 4902, 23354, 29659, 111063, 113004, 23354, 29659, 15490, \n",
      "23354, 29659, 49557, 29659, 123538, 29659, 24487, 29659, 51080, 29659, 52180, 84422, 46746, 29659, 29659, 47460, 102817, 104505, 29659, 76144, \n",
      "48062, 29659, 37143, 7020, 90001, 41606, 55972, 29659, 81638, 29659, 89518, 29659, 66636, 29659, 62630, 84422, 46746, 29659, 29659, 114904, \n",
      "29659, 48133, 29659, 47174, 104505, 29659, 61028, 29659, 37143, 115417, 29659, 92637, 29659, 80365, 29659, 63609, 29659, 107373, 29659, 3533, \n",
      "58866, 29659, 37604, 99126, 29659, 84564, 15158, 46746, 29659, 29659, 96305, 104505, 29659, 86715, 29659, 37143, 109428, 29659, 38453, 90001, \n",
      "88633, 29659, 16377, 15158, 46746, 29659, 29659, 55153, 11609, 87982, 35366, 104505, 29659, 58811, 29659, 37143, 4123, 29659, 54576, 96249, \n",
      "15158, 46746, 29659, 29659, 98809, 90001, 31997, 29659, 74120, 29659, 55980, 29659, 123425, 104505, 29659, 38766, 29659, 112108, 29659, 11460, \n",
      "29659, 123538, 23354, 29659, 118971, 29659, 38852, 29659, 89436, 29659, 27541, 29659, 5936, 29659, 51398, 29659, 75532, 29659, 4133, 29659, \n",
      "11023, 29659, 97661, 29659, 109725, 29659, 97661, 29659, 59675, 29659, 35913, 29659, 18462, 26143, 84422, 46746, 29659, 29659, 34382, 29659, \n",
      "40726, 57600, 104505, 29659, 8902, 29659, 112119, 29659, 93101, 29659, 64018, 29659, 19473, 23354, 29659, 24487, 29659, 115416, 29659, 104460, \n",
      "29659, 3602, 29659, 44405, 29659, 112108, 29659, 74967, 29659, 19473, 29659, 31655, 29659, 123182, 29659, 112143, 29659, 5936, 29659, 74967, \n",
      "29659, 19473, 29659, 31655, 29659, 100368, 84422, 46746, 29659, 29659, 70548, 56146, 104505, 29659, 23416, 29659, 116826, 29659, 27541, 29659, \n",
      "46320, 23354, 29659, 37480, 29659, 101422, 29659, 52180, 29659, 27541, 29659, 14147, 29659, 112143, 29659, 7020, 29659, 51398, 29659, 75532, \n",
      "29659, 4133, 29659, 27541, 29659, 5242, 29659, 36097, 29659, 118971, 29659, 55945, 84422, 46746, 29659, 29659, 118895, 104505, 29659, 106663, \n",
      "112108, 29659, 31655, 29659, 32812, 29659, 37143, 10303, 29659, 104982, 112108, 29659, 31655, 29659, 86715, 55972, 29659, 81638, 29659, 68443, \n",
      "29659, 25629, 29659, 73144, 84422, 46746, 29659, 29659, 112874, 104505, 29659, 46617, 115929, 29659, 81638, 29659, 109868, 65005, 84422, 46746, \n",
      "29659, 29659, 89852, 29659, 115420, 104505, 29659, 48368, 29659, 37143, 87644, 29659, 52842, 15158, 46746, 29659, 29659, 75961, 102206, 29659, \n",
      "115420, 104505, 29659, 17736, 29659, 58811, 23354, 29659, 114324, 29659, 27541, 29659, 19473, 29659, 51398, 29659, 41402, 29659, 4133, 29659, \n",
      "81638, 29659, 36097, 29659, 73281, 84422, 46746, 29659, 29659, 128158, 29659, 21018, 29659, 31308, 29659, 100751, 29659, 106379, 104505, 29659, \n",
      "32812, 23354, 29659, 120044, 29659, 24487, 29659, 120769, 29659, 97988, 35314, 121350, 29659, 52535, 29659, 44405, 29659, 6313, 112613, 84422, \n",
      "46746, 29659, 29659, 9856, 29659, 59286, 104505, 29659, 116933, 29659, 197, 23354, 29659, 108829, 29659, 64270, 197, 29659, 31655, 29659, \n",
      "19473, 84422, 46746, 29659, 29659, 121131, 29659, 81638, 29659, 21018, 29659, 31308, 104505, 29659, 97988, 35314, 121350, 29659, 37143, 79984, \n",
      "29659, 44405, 29659, 90680, 15158, 46746, 29659, 29659, 17902, 104505, 29659, 70055, 29659, 109571, 29659, 129673, 29659, 48848, 29659, 107734, \n",
      "29659, 27541, 29659, 95166, 29659, 24487, 29659, 96382, 29659, 92741, 29659, 37143, 25108, 29659, 123538, 29659, 51080, 29659, 118971, 29659, \n",
      "94344, 29659, 110013, 101946, 29659, 9560, 67771, 129673, 29659, 37143, 2639, 114893, 51080, 23202, 10849, 110013, 8742, 107742, 103122, 29659, \n",
      "31655, 29659, 97988, 35314, 99029, 126343, 125160, 10849, 110013, 101946, 29659, 80502, 84422, 51842, 24763, 84422, 29659, 10347, 29659, 650, \n",
      "91052, 29659, 65458, 29659, 24487, 29659, 97988, 35314, 121350, 29659, 118971, 29659, 24487, 29659, 18134, 107742, 103122, 29659, 64018, 29659, \n",
      "51080, 29659, 27541, 29659, 94344, 29659, 110013, 29659, 37143, 76183, 97198, 29659, 96736, 127158, 15158, 29659, 11307, 29659, 38058, 29659, \n",
      "110585, 29659, 24487, 29659, 120769, 29659, 92741, 29659, 108142, 29659, 18282, 29659, 37143, 237, 23354, 29659, 11307, 29659, 129673, 23354, \n",
      "29659, 11023, 29659, 46730, 23354, 29659, 44405, 29659, 101422, 29659, 11023, 29659, 51982, 29659, 24487, 29659, 39442, 29659, 120495, 29659, \n",
      "18282, 29659, 92171, 29659, 51398, 29659, 19417, 29659, 120769, 29659, 118815, 15158, 29659, 65208, 29659, 27541, 29659, 42626, 29659, 105986, \n",
      "29659, 55641, 29659, 13230, 103770, 29659, 108142, 29659, 3709, 29659, 24550, 29659, 4133, 84422, 46746, 29659, 29659, 16615, 29659, 62472, \n",
      "29659, 15517, 29659, 64018, 29659, 11307, 29659, 44405, 29659, 84010, 29659, 124759, 29659, 101422, 29659, 130035, 29659, 100106, 29659, 123538, \n",
      "29659, 58813, 29659, 101335, 29659, 51398, 29659, 94344, 29659, 110013, 84422, 46746, 29659, 29659, 78208, 29659, 73933, 29659, 44405, 29659, \n",
      "27541, 29659, 82722, 29659, 37143, 97988, 35314, 99029, 126343, 126844, 63758, 37143, 10849, 98903, 127760, 29659, 106666, 29659, 81638, 29659, \n",
      "96636, 29659, 19745, 29659, 55641, 29659, 48368, 29659, 27541, 29659, 63674, 84422, 29659, 125366, 29659, 44405, 29659, 101422, 29659, 76394, \n",
      "29659, 90680, 29659, 129275, 84422, 29659, 56132, 29659, 97988, 35314, 99029, 111478, 29659, 118971, 29659, 10849, 98903, 111478, 29659, 76183, \n",
      "118989, 29659, 24487, 29659, 116223, 29659, 78892, 29659, 100751, 29659, 6319, 29659, 31655, 29659, 47747, 84422, 46746, 29659, 29659, 13740, \n",
      "29659, 25356, 29659, 90680, 29659, 129275, 29659, 16216, 29659, 55641, 29659, 90423, 37143, 80502, 23354, 29659, 63674, 90001, 59617, 55972, \n",
      "29659, 27541, 29659, 63674, 84422, 29659, 29974, 29659, 59617, 29659, 44405, 29659, 76394, 29659, 103653, 90001, 94022, 29659, 58021, 29659, \n",
      "172, 29659, 24487, 29659, 103652, 29659, 25108, 29659, 123538, 29659, 24487, 29659, 79961, 29659, 64018, 29659, 24487, 29659, 24550, 29659, \n",
      "8404, 29659, 118971, 29659, 24487, 29659, 69608, 29659, 64018, 29659, 72016, 29659, 197, 84422, 46746, 29659, 29659, 37268, 29659, 109135, \n",
      "46746, 29659, 29659, 49280, 29659, 44405, 29659, 131072, 127043, 85152, 93430, 99044, 36328, 21162, 74229, 131072, 29659, 118971, 29659, 26797, \n",
      "29659, 44405, 29659, 131072, 127043, 85152, 93430, 99044, 36328, 21162, 95270, 131072, 84422, 46746, 29659, 29659, 59421, 29659, 44405, 29659, \n",
      "78208, 29659, 67320, 29659, 64018, 29659, 24487, 29659, 62472, 29659, 4133, 29659, 37143, 74503, 29659, 48368, 29659, 90882, 29659, 75401, \n",
      "54433, 29659, 6973, 29659, 55803, 29659, 123538, 29659, 76394, 29659, 131072, 117229, 131072, 29659, 8404, 29659, 84010, 29659, 65426, 29659, \n",
      "29436, 29659, 37480, 29659, 21528, 29659, 97793, 29659, 130909, 29659, 13593, 29659, 92741, 84422, 46746, 29659, 29659, 26553, 29659, 48368, \n",
      "46746, 29659, 29659, 27958, 29659, 109801, 104505, 29659, 8408, 90001, 19473, 46746, 29659, 29659, 111293, 88648, 29659, 115420, 104505, 29659, \n",
      "109761, 46746, 29659, 29659, 77361, 29659, 42466, 75731, 104505, 29659, 80502, 84422, 80502, 48368, 46746, 29659, 29659, 72695, 29659, 24238, \n",
      "29659, 35537, 104505, 29659, 87147, 46746, 29659, 29659, 77337, 99479, 104505, 46746, 29659, 29659, 64548, 29659, 93313, 29659, 72695, 29659, \n",
      "37143, 8408, 90001, 19473, 55972, 29659, 10303, 29659, 45484, 8181, 29659, 27026, 88648, 29659, 37143, 109761, 29659, 197, 55972, 29659, \n",
      "118971, 29659, 96021, 29659, 48280, 121287, 36299, 29659, 27541, 29659, 28401, 29659, 11460, 29659, 63775, 84422, 29659, 125366, 29659, 48848, \n",
      "29659, 24487, 29659, 109540, 29659, 81638, 29659, 32656, 84422, 46746, 29659, 29659, 12480, 104505, 46746, 29659, 29659, 9202, 29659, 48848, \n",
      "29659, 127931, 29659, 37480, 29659, 107301, 29659, 126814, 29659, 27541, 29659, 69074, 84422, 46746, 29659, 29659, 18456, 78907, 29659, 84010, \n",
      "29659, 97661, 29659, 71767, 29659, 92559, 29659, 24487, 29659, 72695, 29659, 74158, 29659, 27541, 29659, 36137, 29659, 116826, 29659, 73933, \n",
      "84422, 46746, 29659, 29659, 26553, 29659, 58811, 46746, 29659, 29659, 27958, 29659, 109801, 104505, 29659, 106282, 90001, 19473, 29659, 37143, \n",
      "95352, 29659, 55641, 29659, 121487, 39806, 55972, 46746, 29659, 29659, 111293, 88648, 29659, 115420, 104505, 29659, 109761, 46746, 29659, 29659, \n",
      "77361, 29659, 42466, 75731, 104505, 29659, 80502, 84422, 80502, 48368, 46746, 29659, 29659, 72695, 29659, 24238, 29659, 35537, 104505, 29659, \n",
      "87147, 46746, 29659, 29659, 114903, 38041, 119929, 104505, 29659, 48368, 84422, 80502, 29659, 37143, 27541, 29659, 28401, 29659, 46546, 29659, \n",
      "116505, 55972, 46746, 29659, 29659, 130013, 39349, 79774, 22799, 66636, 104505, 29659, 32812, 29659, 37143, 9684, 29659, 72960, 29659, 33325, \n",
      "55972, 46746, 29659, 29659, 77337, 99479, 104505, 46746, 29659, 29659, 98382, 29659, 121487, 39806, 29659, 48848, 29659, 127931, 29659, 37480, \n",
      "29659, 126814, 23354, 29659, 97661, 29659, 27726, 29659, 72695, 29659, 27541, 29659, 106282, 90001, 19473, 84422, 29659, 71553, 29659, 52964, \n",
      "29659, 27026, 88648, 29659, 100751, 29659, 109761, 29659, 197, 29659, 81638, 29659, 76394, 29659, 58998, 29659, 55757, 29659, 118971, 29659, \n",
      "124063, 29659, 38041, 29659, 109428, 70790, 29659, 74712, 29659, 27541, 29659, 28635, 29659, 24550, 84422, 46746, 46746, 29659, 29659, 12480, \n",
      "104505, 46746, 29659, 29659, 90072, 32315, 29659, 78117, 29659, 41880, 29659, 121487, 39806, 29659, 21030, 29659, 62472, 29659, 78448, 103647, \n",
      "84422, 46746, 29659, 29659, 7113, 71355, 29659, 11241, 29659, 78892, 23354, 29659, 37480, 29659, 97661, 29659, 21528, 29659, 60601, 29659, \n",
      "97661, 29659, 71767, 29659, 92559, 29659, 72695, 29659, 25356, 29659, 38802, 84422, 46746, 29659, 29659, 26553, 29659, 63150, 29659, 37143, \n",
      "125656, 58866, 23354, 29659, 37480, 29659, 41203, 29659, 12561, 29659, 104502, 55972, 46746, 29659, 29659, 27958, 29659, 109801, 104505, 29659, \n",
      "106282, 90001, 19473, 46746, 29659, 29659, 111293, 88648, 29659, 115420, 104505, 29659, 75401, 29659, 37143, 81322, 29659, 79984, 55972, 46746, \n",
      "29659, 29659, 77361, 29659, 42466, 75731, 104505, 29659, 80502, 84422, 80502, 48368, 46746, 29659, 29659, 72695, 29659, 24238, 104505, 29659, \n",
      "92199, 20716, 46746, 29659, 29659, 77337, 99479, 104505, 46746, 29659, 29659, 2876, 23354, 29659, 97661, 29659, 43879, 29659, 76394, 29659, \n",
      "16821, 29659, 27026, 88648, 29659, 27541, 29659, 24619, 29659, 72695, 29659, 116826, 29659, 94340, 29659, 2972, 23354, 29659, 6973, 29659, \n",
      "76394, 29659, 105750, 48225, 29659, 68209, 29659, 81638, 29659, 76394, 29659, 44943, 29659, 75532, 90001, 56325, 29659, 48280, 84422, 29659, \n",
      "21801, 29659, 104358, 29659, 48848, 29659, 27541, 29659, 12174, 29659, 11460, 29659, 73073, 84422, 46746, 29659, 29659, 12480, 104505, 46746, \n",
      "29659, 29659, 21801, 29659, 24550, 29659, 65426, 29659, 29436, 29659, 11460, 29659, 37143, 131072, 117229, 131072, 54433, 29659, 70481, 29659, \n",
      "35292, 29659, 27541, 29659, 78208, 29659, 40854, 29659, 72695, 29659, 24619, 29659, 6973, 29659, 125299, 29659, 45370, 29659, 27026, 88648, \n",
      "29659, 197, 84422, 46746, 29659, 29659, 1379, 29659, 69192, 29659, 97793, 29659, 92741, 125160, 18282, 29659, 84010, 29659, 82470, 29659, \n",
      "92812, 29659, 46730, 29659, 1626, 29659, 124759, 29659, 117229, 23354, 29659, 52692, 29659, 24487, 29659, 96382, 29659, 48848, 29659, 73073, \n",
      "29659, 2972, 29659, 37480, 29659, 123538, 29659, 24487, 29659, 83819, 29659, 64018, 29659, 73144, 84422, 46746, 29659, 29659, 79676, 29659, \n",
      "104739, 83202, 29659, 24487, 29659, 8404, 23354, 29659, 25635, 29659, 82470, 29659, 94528, 29659, 84010, 29659, 82470, 29659, 46730, 29659, \n",
      "37480, 29659, 125299, 29659, 76631, 29659, 46730, 29659, 76631, 29659, 24487, 29659, 117229, 29659, 15814, 84422, 46746, 29659, 29659, 26553, \n",
      "29659, 63150, 29659, 37143, 58857, 29659, 69977, 29659, 10303, 29659, 15596, 124174, 29659, 117294, 29659, 55641, 29659, 120008, 55972, 46746, \n",
      "29659, 29659, 27958, 29659, 109801, 104505, 29659, 48368, 84422, 100875, 90001, 63150, 29659, 37143, 81322, 29659, 74158, 29659, 41880, 29659, \n",
      "106282, 90001, 19473, 55972, 46746, 29659, 29659, 111293, 88648, 29659, 115420, 104505, 29659, 14147, 29659, 37143, 76394, 29659, 41606, 29659, \n",
      "38802, 29659, 41880, 29659, 75401, 23354, 29659, 37480, 29659, 21528, 29659, 16821, 55972, 46746, 29659, 29659, 77361, 29659, 42466, 75731, \n",
      "104505, 29659, 80502, 84422, 53609, 46746, 29659, 29659, 72695, 29659, 24238, 104505, 29659, 92199, 20716, 46746, 29659, 29659, 77337, 99479, \n",
      "104505, 46746, 29659, 29659, 40841, 90001, 131072, 12718, 23354, 131072, 29659, 97661, 29659, 68608, 29659, 78208, 29659, 25356, 29659, 74158, \n",
      "29659, 72695, 29659, 37480, 29659, 61315, 29659, 10303, 29659, 38802, 29659, 98733, 29659, 48280, 29659, 37143, 80502, 84422, 53609, 55972, \n",
      "29659, 27541, 29659, 66548, 29659, 51398, 29659, 85382, 29659, 105868, 52455, 29659, 118971, 29659, 13593, 29659, 112119, 29659, 106090, 84422, \n",
      "29659, 71553, 29659, 17132, 29659, 107734, 29659, 14147, 29659, 27026, 88648, 29659, 197, 23354, 29659, 24560, 29659, 24487, 29659, 102206, \n",
      "81983, 29659, 109550, 29659, 101026, 29659, 91236, 29659, 128917, 23354, 29659, 118971, 29659, 27541, 29659, 17108, 29659, 81638, 29659, 124759, \n",
      "84422, 46746, 29659, 29659, 12480, 104505, 46746, 29659, 29659, 21055, 29659, 128917, 29659, 78117, 29659, 118971, 29659, 97468, 29659, 11241, \n",
      "29659, 25635, 23354, 29659, 37480, 29659, 10303, 29659, 47597, 29659, 51398, 29659, 24487, 29659, 110013, 29659, 12449, 84422, 46746, 29659, \n",
      "29659, 108349, 29659, 71767, 29659, 125299, 29659, 114922, 29659, 24487, 29659, 25635, 29659, 55641, 29659, 24487, 29659, 131072, 12430, 131072, \n",
      "29659, 12718, 29659, 8404, 84422, 46746, 29659, 29659, 26553, 29659, 19473, 46746, 29659, 29659, 27958, 29659, 109801, 104505, 29659, 8408, \n",
      "90001, 63150, 29659, 37143, 89436, 29659, 16661, 55972, 46746, 29659, 29659, 77361, 29659, 42466, 75731, 104505, 29659, 80502, 84422, 86837, \n",
      "29659, 37143, 65622, 29659, 79984, 55972, 46746, 29659, 29659, 111293, 88648, 29659, 115420, 104505, 29659, 46320, 46746, 29659, 29659, 72695, \n",
      "29659, 24238, 104505, 29659, 92199, 20716, 29659, 10303, 29659, 50004, 32086, 46746, 29659, 29659, 113190, 104505, 29659, 7020, 46746, 29659, \n",
      "29659, 77337, 99479, 104505, 46746, 29659, 29659, 50036, 29659, 68219, 23354, 29659, 76394, 29659, 72713, 29659, 72695, 29659, 124461, 29659, \n",
      "12174, 29659, 82118, 29659, 73073, 23354, 29659, 37480, 29659, 60442, 29659, 102688, 29659, 113583, 47570, 84422, 29659, 92199, 20716, 29659, \n",
      "10303, 29659, 57314, 32086, 29659, 85950, 57505, 29659, 24487, 29659, 72695, 29659, 103890, 23354, 29659, 122813, 29659, 107159, 90001, 78117, \n",
      "29659, 110466, 29659, 24487, 29659, 96382, 29659, 14898, 29659, 105868, 52455, 29659, 112143, 29659, 130820, 99044, 29659, 77197, 84422, 29659, \n",
      "21801, 29659, 79984, 29659, 98733, 29659, 48280, 29659, 37143, 80502, 84422, 86837, 55972, 29659, 7867, 29659, 24487, 29659, 26282, 29659, \n",
      "8404, 131072, 82505, 29659, 32968, 29659, 122845, 29659, 37143, 80502, 84422, 53609, 54433, 29659, 81111, 29659, 97793, 29659, 27541, 29659, \n",
      "42626, 29659, 13230, 748, 29659, 122724, 84422, 46746, 29659, 29659, 12480, 104505, 46746, 29659, 29659, 9617, 29659, 82118, 29659, 73933, \n",
      "84422, 46746, 29659, 29659, 7020, 29659, 84199, 29659, 53885, 29659, 24487, 29659, 99481, 23354, 29659, 76394, 29659, 54595, 29659, 94344, \n",
      "29659, 36097, 23354, 29659, 118971, 29659, 76394, 29659, 9684, 29659, 73073, 29659, 74623, 29659, 106364, 29659, 172, 29659, 105750, 48225, \n",
      "29659, 12449, 84422, 46746, 29659, 29659, 17434, 29659, 113583, 47570, 29659, 100751, 29659, 74563, 29659, 84199, 23354, 29659, 37480, 29659, \n",
      "124759, 29659, 108829, 29659, 58498, 29659, 83324, 29659, 24487, 29659, 126869, 84422, 46746, 29659, 29659, 104502, 29659, 17329, 29659, 27541, \n",
      "29659, 84010, 29659, 64018, 29659, 24487, 29659, 117229, 29659, 8404, 46746, 29659, 29659, 16633, 29659, 7480, 29659, 24550, 29659, 8404, \n",
      "29659, 10303, 29659, 38802, 29659, 101335, 23354, 29659, 118971, 29659, 76394, 29659, 79984, 29659, 51080, 29659, 110013, 29659, 27541, 29659, \n",
      "35292, 29659, 105868, 52455, 29659, 75532, 29659, 123538, 29659, 51398, 29659, 24487, 29659, 8404, 84422, 46746, 29659, 29659, 26553, 29659, \n",
      "75401, 46746, 29659, 29659, 27958, 29659, 109801, 104505, 29659, 48368, 84422, 9991, 90001, 63150, 46746, 29659, 29659, 77361, 29659, 42466, \n",
      "75731, 104505, 29659, 80502, 84422, 80502, 93948, 46746, 29659, 29659, 111293, 88648, 29659, 115420, 104505, 29659, 7020, 46746, 29659, 29659, \n",
      "72695, 29659, 24238, 104505, 29659, 92199, 20716, 29659, 10303, 29659, 50004, 32086, 46746, 29659, 29659, 113190, 104505, 29659, 14147, 46746, \n",
      "29659, 29659, 77337, 99479, 104505, 46746, 29659, 29659, 37120, 99687, 29659, 55641, 29659, 121487, 74601, 104505, 46746, 29659, 29659, 15277, \n",
      "100222, 29659, 79984, 29659, 72695, 29659, 37143, 48368, 84422, 9991, 90001, 63150, 55972, 29659, 81638, 29659, 38802, 29659, 73144, 84422, \n",
      "46746, 29659, 29659, 77361, 29659, 48280, 29659, 100751, 29659, 80502, 84422, 80502, 93948, 23354, 29659, 83685, 29659, 24487, 29659, 9799, \n",
      "98569, 29659, 64018, 29659, 80502, 84422, 53609, 29659, 83391, 84422, 29659, 80502, 84422, 86837, 23354, 29659, 46222, 29659, 105868, 52455, \n",
      "84422, 46746, 29659, 29659, 48016, 29659, 69608, 29659, 64018, 29659, 27026, 88648, 29659, 197, 23354, 29659, 81638, 29659, 76394, 29659, \n",
      "9684, 29659, 113485, 29659, 37143, 101273, 29659, 11460, 55972, 29659, 118971, 29659, 74563, 29659, 106364, 29659, 18727, 29659, 93402, 29659, \n",
      "37143, 81638, 29659, 75532, 29659, 73144, 55972, 29659, 64018, 29659, 24487, 29659, 73073, 29659, 74623, 84422, 46746, 29659, 29659, 14147, \n",
      "29659, 84199, 29659, 27541, 29659, 38895, 29659, 24487, 29659, 72016, 29659, 36097, 29659, 16821, 29659, 118971, 29659, 16661, 29659, 24487, \n",
      "29659, 21634, 29659, 64018, 29659, 76394, 29659, 9684, 29659, 117937, 29659, 73073, 29659, 74623, 29659, 123538, 29659, 96382, 29659, 96736, \n",
      "127158, 29659, 118971, 29659, 24550, 29659, 73144, 84422, 46746, 29659, 29659, 12480, 104505, 46746, 29659, 29659, 16615, 29659, 64018, 29659, \n",
      "47633, 29659, 120769, 29659, 109925, 29659, 4133, 23354, 29659, 10303, 29659, 72713, 29659, 92741, 29659, 108142, 29659, 51982, 29659, 18282, \n",
      "23354, 29659, 90680, 29659, 73144, 29659, 118971, 29659, 76394, 29659, 79984, 29659, 96736, 127158, 84422, 46746, 29659, 29659, 84003, 29659, \n",
      "76394, 29659, 90680, 29659, 92741, 29659, 37143, 115016, 55972, 29659, 41880, 29659, 24487, 29659, 117229, 29659, 22251, 29659, 8404, 84422, \n",
      "46746, 29659, 29659, 15853, 29659, 27541, 29659, 42626, 29659, 64677, 32488, 29659, 51398, 29659, 24487, 29659, 109868, 65005, 29659, 4133, \n",
      "46746, 29659, 29659, 106871, 29659, 76631, 29659, 47633, 29659, 109033, 29659, 11241, 29659, 96382, 29659, 81638, 29659, 36994, 29659, 62630, \n",
      "29659, 37143, 117508, 54640, 29659, 114676, 23354, 29659, 4583, 29659, 75862, 23354, 29659, 22647, 23354, 29659, 101053, 15158, 46746, 46746, \n",
      "29659, 29659, 94718, 29659, 116293, 29659, 26263, 99687, 46746, 29659, 29659, 45841, 28114, 29659, 109135, 104505, 29659, 60496, 29659, 26845, \n",
      "29659, 58811, 114854, 63150, 29659, 84199, 29659, 110466, 29659, 97661, 29659, 59089, 29659, 76394, 29659, 21003, 29659, 73933, 29659, 113583, \n",
      "29659, 24487, 29659, 45111, 23354, 29659, 118971, 29659, 76394, 29659, 21601, 29659, 9684, 29659, 102206, 81983, 29659, 81126, 29659, 81638, \n",
      "29659, 76394, 29659, 21601, 29659, 38802, 29659, 127931, 29659, 8404, 84422, 46746, 29659, 29659, 17596, 90820, 29659, 71336, 104505, 29659, \n",
      "108962, 29659, 24487, 29659, 55153, 29659, 6795, 29659, 27541, 29659, 37604, 81161, 29659, 14357, 23354, 29659, 97661, 29659, 18549, 29659, \n",
      "58297, 29659, 25356, 29659, 38802, 29659, 84199, 29659, 112143, 29659, 30685, 29659, 76103, 84422, 46746, 29659, 29659, 63150, 84422, 63150, \n",
      "84422, 19473, 29659, 9202, 29659, 116610, 29659, 1301, 29659, 80591, 46746, 29659, 29659, 105002, 29659, 6471, 29659, 63301, 29659, 82470, \n",
      "29659, 119381, 29659, 123538, 29659, 78208, 29659, 66263, 29659, 4949, 29659, 72960, 84422, 29659, 88109, 29659, 1333, 104505, 46746, 29659, \n",
      "29659, 115420, 125160, 113190, 104505, 29659, 37604, 48714, 23354, 29659, 120495, 29659, 78208, 29659, 115416, 29659, 104460, 29659, 3602, 29659, \n",
      "64018, 29659, 100368, 29659, 37143, 5936, 29659, 129806, 29659, 73030, 29659, 40920, 29659, 19473, 29659, 112119, 29659, 93101, 15158, 46746, \n",
      "29659, 29659, 116610, 125160, 113190, 104505, 29659, 37604, 58811, 29659, 124430, 29659, 109761, 29659, 63758, 29659, 37143, 43191, 29659, 33581, \n",
      "54433, 29659, 125890, 29659, 51398, 29659, 65458, 29659, 24744, 125160, 124804, 29659, 84359, 29659, 118971, 29659, 13593, 29659, 120100, 84422, \n",
      "46746, 29659, 29659, 75961, 102206, 29659, 115420, 104505, 29659, 37604, 101548, 29659, 119792, 29659, 18861, 23354, 29659, 4842, 29659, 37604, \n",
      "36333, 29659, 120829, 29659, 119792, 29659, 88590, 29659, 37604, 9175, 29659, 63758, 29659, 72016, 29659, 81638, 29659, 51080, 84422, 46746, \n",
      "29659, 29659, 84547, 29659, 81638, 29659, 14147, 29659, 70548, 56146, 104505, 29659, 37604, 100541, 29659, 33581, 29659, 75175, 63150, 29659, \n",
      "124430, 29659, 59268, 29659, 63758, 55972, 46746, 29659, 29659, 73596, 29659, 109761, 29659, 6319, 29659, 37143, 120769, 29659, 6319, 29659, \n",
      "1626, 29659, 96736, 127158, 29659, 30384, 23354, 29659, 172, 29659, 109571, 29659, 129673, 101946, 29659, 37604, 109761, 29659, 74967, 29659, \n",
      "43191, 29659, 125160, 29659, 48714, 29659, 88590, 29659, 37604, 29659, 118867, 29659, 33581, 29659, 118971, 29659, 109761, 125160, 58811, 29659, \n",
      "31655, 29659, 46320, 29659, 51080, 29659, 197, 29659, 37604, 29659, 46320, 29659, 74967, 29659, 36333, 29659, 88590, 29659, 37604, 107316, \n",
      "29659, 63758, 29659, 81638, 29659, 51080, 84422, 29659, 2153, 29659, 76394, 29659, 72016, 29659, 114490, 29659, 64018, 29659, 37604, 29659, \n",
      "44880, 29659, 33581, 84422, 46746, 29659, 29659, 117612, 29659, 80591, 29659, 8099, 46746, 29659, 29659, 64193, 29659, 24487, 29659, 34682, \n",
      "131072, 82505, 29659, 130176, 29659, 96382, 29659, 38032, 29659, 131072, 11193, 131072, 104505, 46746, 29659, 29659, 80591, 29659, 64018, 29659, \n",
      "4949, 104505, 29659, 58811, 84422, 19473, 29659, 11193, 125160, 62346, 29659, 31655, 29659, 80502, 84422, 113868, 29659, 63075, 125160, 109731, \n",
      "84422, 46746, 29659, 29659, 48368, 29659, 95510, 24337, 29659, 31655, 29659, 64848, 46320, 125160, 127635, 29659, 31655, 29659, 46320, 29659, \n",
      "42880, 84422, 46746, 29659, 29659, 14147, 90001, 113190, 29659, 9202, 29659, 75175, 100541, 29659, 33581, 101946, 46746, 29659, 29659, 100541, \n",
      "29659, 63758, 29659, 40920, 29659, 80502, 84422, 113868, 29659, 63075, 125160, 63758, 29659, 40920, 29659, 46320, 29659, 43892, 125160, 63075, \n",
      "29659, 31655, 29659, 64848, 80502, 84422, 78460, 46746, 29659, 29659, 37604, 29659, 88870, 84422, 29659, 22727, 29659, 37143, 110466, 29659, \n",
      "64848, 48368, 29659, 37604, 29659, 88870, 84422, 29659, 31440, 84422, 75401, 15158, 46746, 29659, 29659, 31308, 29659, 583, 70779, 29659, \n",
      "75175, 44880, 29659, 63758, 29659, 27541, 29659, 18849, 29659, 120769, 29659, 13593, 29659, 6319, 101946, 46746, 29659, 29659, 44880, 29659, \n",
      "63758, 29659, 40920, 29659, 80502, 84422, 113868, 29659, 40920, 29659, 80502, 84422, 48368, 29659, 31655, 29659, 64848, 80502, 84422, 1194, \n",
      "29659, 75175, 88870, 84422, 29659, 112108, 84422, 105010, 15158, 46746, 29659, 29659, 84547, 29659, 81638, 29659, 902, 29659, 4133, 29659, \n",
      "112143, 29659, 53628, 29659, 59830, 29659, 37143, 72016, 29659, 75401, 29659, 62472, 29659, 4133, 29659, 8824, 29659, 48368, 29659, 109868, \n",
      "65005, 29659, 8404, 29659, 8824, 29659, 48368, 29659, 117229, 29659, 8404, 29659, 8824, 29659, 19929, 29659, 39442, 29659, 4133, 29659, \n",
      "31655, 29659, 3709, 29659, 4133, 101946, 29659, 81649, 128544, 84422, 105010, 29659, 75175, 88870, 84422, 29659, 443, 84422, 19473, 55972, \n",
      "29659, 110466, 29659, 97661, 29659, 56717, 29659, 116826, 29659, 17405, 29659, 63301, 84422, 46746, 29659, 29659, 97430, 29659, 80319, 70779, \n",
      "104505, 46746, 29659, 29659, 21801, 29659, 96382, 29659, 111432, 29659, 51398, 29659, 37604, 75401, 114854, 38782, 29659, 64018, 29659, 84623, \n",
      "29659, 118971, 29659, 110832, 29659, 16821, 29659, 4583, 29659, 72576, 29659, 100751, 29659, 32876, 29659, 109942, 84422, 46746, 29659, 29659, \n",
      "74584, 29659, 24550, 23354, 29659, 97661, 29659, 44203, 29659, 24487, 29659, 34612, 16375, 29659, 124268, 29659, 119970, 29659, 24487, 29659, \n",
      "45111, 29659, 96382, 29659, 118971, 29659, 83628, 23818, 29659, 27541, 29659, 127047, 130595, 8942, 29659, 37143, 117508, 54640, 15158, 46746, \n",
      "29659, 29659, 71553, 29659, 124461, 29659, 46157, 29659, 123538, 29659, 37548, 29659, 9823, 29659, 26711, 127254, 84422, 31456, 23354, 29659, \n",
      "97477, 26088, 29659, 109428, 29659, 4744, 29659, 72960, 29659, 56404, 29659, 63739, 29659, 123538, 29659, 76394, 29659, 83874, 29659, 79333, \n",
      "92109, 29659, 34713, 76480, 29659, 22802, 84422, 29659, 21801, 29659, 96382, 29659, 111432, 29659, 51398, 29659, 37604, 86832, 29659, 64018, \n",
      "29659, 84623, 29659, 37143, 21030, 29659, 114854, 47385, 14596, 23354, 29659, 81111, 29659, 54762, 29659, 93402, 23791, 29659, 27541, 29659, \n",
      "12381, 23354, 29659, 34815, 29659, 4583, 29659, 18374, 55972, 29659, 35292, 29659, 27541, 29659, 66385, 29659, 93402, 23791, 29659, 118971, \n",
      "29659, 66636, 29659, 22758, 84422, 29659, 125366, 29659, 128878, 29659, 117014, 56259, 29659, 29523, 29659, 72960, 90001, 25108, 29659, 24550, \n",
      "29659, 10303, 29659, 83874, 29659, 37548, 90001, 25108, 29659, 4583, 84422, 46746, 29659, 29659, 113036, 29659, 92453, 29659, 36097, 29659, \n",
      "90001, 29659, 22388, 23354, 29659, 96382, 29659, 85790, 23354, 29659, 91147, 29659, 84422, 109777, 29659, 78072, 23354, 29659, 101053, 100523, \n",
      "29659, 37604, 105010, 90001, 14147, 29659, 33581, 29659, 64018, 29659, 82118, 29659, 121873, 29659, 36097, 29659, 10303, 29659, 88908, 29659, \n",
      "107316, 29659, 120829, 29659, 64018, 29659, 96382, 29659, 39349, 29659, 36097, 29659, 123538, 29659, 62598, 29659, 37143, 96382, 29659, 39349, \n",
      "29659, 36097, 29659, 118971, 29659, 91147, 29659, 24487, 29659, 117022, 29659, 6848, 29659, 81638, 29659, 11158, 29659, 51398, 29659, 84623, \n",
      "15158, 29659, 79333, 92109, 29659, 86154, 29659, 54674, 29659, 24487, 29659, 86154, 29659, 123538, 29659, 44880, 29659, 14273, 29659, 64018, \n",
      "29659, 109428, 29659, 62630, 23354, 29659, 38852, 29659, 24487, 29659, 13945, 29659, 121873, 29659, 44405, 29659, 58385, 29659, 101273, 29659, \n",
      "123538, 29659, 23791, 84422, 29659, 29386, 29659, 36097, 29659, 103965, 29659, 4583, 29659, 44405, 29659, 47440, 29659, 89518, 29659, 35292, \n",
      "29659, 27541, 29659, 6848, 29659, 11158, 29659, 118971, 29659, 38453, 90001, 78561, 84422, 29659, 37143, 85315, 29659, 51398, 29659, 24487, \n",
      "29659, 44946, 29659, 26845, 29659, 54929, 29659, 3465, 46344, 29659, 92171, 29659, 64856, 29659, 118971, 29659, 237, 29659, 124759, 29659, \n",
      "44405, 29659, 25641, 29659, 27541, 29659, 76009, 29659, 123538, 29659, 74158, 29659, 104893, 29659, 65255, 23354, 29659, 109621, 29659, 65255, \n",
      "29659, 10303, 29659, 7020, 29659, 112143, 29659, 112108, 29659, 3465, 46344, 23354, 29659, 109550, 29659, 82935, 29659, 81322, 29659, 9684, \n",
      "29659, 25635, 55972, 46746, 29659, 29659, 63150, 84422, 63150, 84422, 75401, 29659, 104502, 29659, 1301, 29659, 94718, 29659, 55572, 46746, \n",
      "29659, 29659, 40726, 11526, 104505, 46746, 29659, 29659, 73596, 29659, 92637, 29659, 56095, 29659, 37143, 18646, 125160, 61210, 125160, 58074, \n",
      "58074, 23354, 29659, 64848, 129713, 23354, 29659, 111461, 127714, 54433, 29659, 83324, 29659, 21452, 29659, 81935, 83555, 84422, 46746, 29659, \n",
      "29659, 31308, 29659, 65502, 29659, 100751, 29659, 52535, 104505, 46746, 29659, 29659, 10849, 110013, 104505, 29659, 80502, 84422, 84084, 46746, \n",
      "29659, 29659, 97988, 35314, 121350, 104505, 29659, 80502, 84422, 35158, 83173, 46746, 29659, 29659, 41428, 104505, 29659, 80502, 84422, 24763, \n",
      "41514, 46746, 29659, 29659, 5492, 104505, 29659, 80502, 84422, 78659, 28105, 46746, 29659, 29659, 72048, 104505, 29659, 80502, 84422, 97921, \n",
      "23370, 46746, 29659, 29659, 115110, 45052, 104505, 29659, 26148, 84422, 80502, 68080, 46746, 29659, 29659, 115110, 45052, 4902, 104505, 29659, \n",
      "73854, 84422, 8842, 28382, 46746, 29659, 29659, 9560, 67771, 129673, 29659, 37143, 2639, 114893, 51080, 23202, 10849, 110013, 8742, 107742, \n",
      "103122, 29659, 31655, 29659, 97988, 35314, 99029, 126343, 125160, 10849, 110013, 101946, 29659, 80502, 84422, 51842, 24763, 46746, 29659, 29659, \n",
      "77393, 50312, 29659, 123538, 104505, 46746, 29659, 29659, 51410, 29659, 112143, 29659, 20093, 29659, 35389, 29659, 22414, 29659, 112143, 29659, \n",
      "59827, 29659, 92832, 29659, 11193, 84422, 46746, 29659, 29659, 21055, 29659, 101457, 29659, 59864, 29659, 75175, 102735, 8824, 29659, 84564, \n",
      "55972, 46746, 29659, 29659, 56474, 38032, 29659, 96251, 29659, 17329, 29659, 72110, 29659, 118971, 29659, 101457, 29659, 27297, 29659, 71390, \n",
      "46746, 29659, 29659, 53813, 90001, 43177, 29659, 37143, 24487, 29659, 96382, 29659, 18549, 29659, 38895, 29659, 53829, 29659, 87501, 29659, \n",
      "12064, 29659, 24604, 55972, 46746, 29659, 29659, 129289, 29659, 16821, 23354, 29659, 24487, 29659, 113408, 90001, 116468, 29659, 128878, 29659, \n",
      "44405, 29659, 80716, 29659, 118971, 29659, 76930, 29659, 38802, 29659, 68946, 29659, 41880, 29659, 24487, 29659, 128237, 90001, 25108, 29659, \n",
      "128878, 29659, 37143, 110466, 29659, 124759, 29659, 48848, 29659, 51398, 29659, 24487, 29659, 24550, 29659, 130013, 15158, 29659, 85315, 23354, \n",
      "29659, 110466, 29659, 124759, 29659, 128842, 29659, 81173, 29659, 98022, 29659, 88849, 29659, 75977, 29659, 24550, 29659, 88932, 23354, 29659, \n",
      "124759, 29659, 18549, 29659, 129560, 29659, 112143, 29659, 5142, 22670, 84422, 46746, 46746, 46746, 46746, 46746, 46746, 46746, 46746, 46746, \n",
      "46746, 46746, 46746, 46746, 46746, 46746, 29659, 29659, 45242, 39847, 46015, 104111, 29659, 8404, 29659, 16367, 104505, 46746, 46746, 46746, \n",
      "46746, 46746, 46746, 46746, 29659, 29659, 19473, 84422, 29659, 9202, 29659, 1301, 29659, 35938, 29659, 65502, 46746, 29659, 29659, 19473, \n",
      "84422, 48368, 29659, 65502, 29659, 106871, 46746, 29659, 29659, 71553, 29659, 25486, 29659, 34293, 104505, 46746, 29659, 29659, 9202, 29659, \n",
      "124973, 29659, 1301, 29659, 75961, 102206, 29659, 124973, 104505, 29659, 114260, 29659, 83882, 29659, 64018, 29659, 35954, 29659, 43246, 29659, \n",
      "24487, 29659, 96382, 29659, 16032, 29659, 24487, 29659, 130013, 84422, 46746, 29659, 29659, 111063, 113004, 104505, 29659, 113340, 29659, 88169, \n",
      "90001, 88988, 38077, 29659, 93443, 84422, 29659, 12561, 29659, 81638, 29659, 64567, 29659, 109925, 29659, 129262, 29659, 6634, 84422, 46746, \n",
      "29659, 29659, 102987, 64393, 125160, 102987, 64393, 4902, 104505, 29659, 107768, 90001, 25108, 29659, 38778, 90001, 115251, 23354, 29659, 13532, \n",
      "29659, 51398, 29659, 88314, 29659, 21002, 83216, 29659, 112143, 29659, 64913, 29659, 103028, 84422, 46746, 29659, 29659, 49557, 104505, 29659, \n",
      "107768, 29659, 97444, 29659, 109801, 23354, 29659, 63674, 84422, 97713, 100523, 29659, 120241, 29659, 46705, 29659, 100751, 29659, 24487, 29659, \n",
      "17730, 29659, 29775, 84422, 29659, 108962, 29659, 49557, 29659, 44405, 29659, 80502, 84422, 24763, 23354, 29659, 124759, 29659, 100905, 29659, \n",
      "24763, 36937, 29659, 64018, 29659, 24487, 29659, 91412, 29659, 101213, 114087, 29659, 55641, 29659, 24487, 29659, 48813, 84422, 46746, 29659, \n",
      "29659, 15490, 104505, 29659, 113340, 29659, 97444, 29659, 109801, 84422, 29659, 58237, 29659, 37604, 80502, 84422, 77812, 114854, 80502, 84422, \n",
      "48368, 23354, 29659, 73602, 29659, 7020, 114854, 46320, 36937, 29659, 64018, 29659, 24487, 29659, 59604, 29659, 101213, 114087, 29659, 55641, \n",
      "29659, 48813, 84422, 46746, 29659, 29659, 124686, 29659, 31384, 29659, 122612, 91220, 29659, 20932, 29659, 13593, 29659, 71390, 29659, 18549, \n",
      "29659, 18849, 29659, 76394, 29659, 72713, 29659, 111063, 113004, 29659, 37480, 29659, 52567, 29659, 100751, 29659, 24487, 29659, 17730, 29659, \n",
      "29775, 84422, 29659, 102987, 64393, 23354, 29659, 49557, 23354, 29659, 15490, 29659, 119792, 29659, 90078, 29659, 119552, 29659, 26163, 52170, \n",
      "29659, 1333, 29659, 123538, 29659, 49480, 29659, 10303, 29659, 24487, 29659, 48813, 84422, 46746, 29659, 29659, 19473, 84422, 58811, 29659, \n",
      "12667, 29659, 121131, 29659, 98526, 3887, 46746, 29659, 29659, 64548, 29659, 131072, 120769, 29659, 52535, 131072, 29659, 55641, 29659, 121487, \n",
      "77091, 29659, 18549, 29659, 69084, 104505, 46746, 29659, 29659, 10849, 110013, 29659, 31655, 29659, 80502, 84422, 84084, 46746, 29659, 29659, \n",
      "97988, 35314, 121350, 29659, 31655, 29659, 80502, 84422, 113640, 46746, 29659, 29659, 72048, 29659, 31655, 29659, 80502, 84422, 82670, 46746, \n",
      "29659, 29659, 115110, 45052, 29659, 31655, 29659, 26148, 84422, 80502, 107316, 46746, 29659, 29659, 115110, 45052, 4902, 29659, 31655, 29659, \n",
      "73854, 84422, 60696, 46746, 29659, 29659, 41428, 29659, 31655, 29659, 80502, 84422, 43242, 46746, 29659, 29659, 5492, 29659, 31655, 29659, \n",
      "80502, 84422, 36581, 46746, 29659, 29659, 22292, 104505, 29659, 37604, 24763, 84422, 63150, 36937, 29659, 64018, 29659, 91412, 29659, 101213, \n",
      "114087, 29659, 55641, 29659, 48813, 23354, 29659, 81111, 29659, 44405, 29659, 91236, 29659, 97468, 29659, 81638, 29659, 101457, 29659, 38453, \n",
      "90001, 80898, 64143, 29659, 71390, 84422, 29659, 21801, 29659, 96382, 29659, 44405, 29659, 126159, 29659, 40304, 29659, 37604, 45480, 114854, \n",
      "95236, 36937, 29659, 64018, 29659, 24487, 29659, 36097, 29659, 81638, 29659, 63205, 29659, 71390, 84422, 46746, 29659, 29659, 19473, 84422, \n",
      "63150, 29659, 49055, 29659, 86555, 46746, 29659, 29659, 71553, 29659, 17132, 29659, 34293, 104505, 46746, 29659, 29659, 38041, 119929, 104505, \n",
      "29659, 27541, 29659, 2105, 29659, 110466, 29659, 24487, 29659, 116505, 29659, 82470, 29659, 46546, 29659, 112143, 29659, 125299, 84422, 46746, \n",
      "29659, 29659, 73073, 72842, 104505, 29659, 7644, 29659, 24487, 29659, 20686, 131072, 82505, 29659, 20797, 84422, 46746, 46746, 29659, 29659, \n",
      "75401, 84422, 29659, 31308, 29659, 45159, 29659, 1301, 29659, 117508, 54640, 29659, 58253, 46746, 29659, 29659, 75401, 84422, 48368, 29659, \n",
      "124686, 29659, 120966, 29659, 27541, 29659, 117508, 54640, 91220, 46746, 29659, 29659, 52482, 29659, 24487, 29659, 96382, 29659, 44405, 29659, \n",
      "113408, 90001, 116468, 23354, 29659, 97661, 29659, 59089, 29659, 27541, 29659, 46157, 29659, 124759, 29659, 81638, 29659, 4583, 29659, 51398, \n",
      "29659, 76394, 29659, 63739, 90001, 83874, 29659, 21559, 29659, 37143, 90575, 29659, 37548, 90001, 26845, 29659, 79333, 92109, 29659, 34713, \n",
      "76480, 29659, 121438, 15158, 29659, 21801, 29659, 131072, 92776, 34444, 131072, 29659, 81638, 39854, 23354, 29659, 107734, 29659, 172, 29659, \n",
      "26711, 127254, 84422, 31456, 23354, 29659, 44405, 29659, 76394, 29659, 37548, 90001, 87849, 29659, 83628, 23818, 29659, 81638, 39854, 29659, \n",
      "84010, 104505, 46746, 29659, 29659, 126698, 46344, 29659, 66636, 29659, 62630, 29659, 16661, 29659, 41880, 29659, 92637, 29659, 72960, 29659, \n",
      "23831, 103770, 84422, 46746, 29659, 29659, 126461, 75977, 29659, 83324, 29659, 95449, 90001, 36097, 29659, 4583, 29659, 81638, 29659, 16821, \n",
      "29659, 124432, 43644, 29659, 123538, 29659, 58811, 29659, 37548, 29659, 3465, 46344, 84422, 46746, 29659, 29659, 22477, 91052, 29659, 56404, \n",
      "29659, 63739, 84422, 29659, 37143, 71553, 29659, 68852, 29659, 686, 29659, 123448, 29659, 51398, 47765, 54748, 46746, 29659, 29659, 27750, \n",
      "29659, 97661, 29659, 107734, 29659, 11867, 5226, 79390, 131072, 82505, 29659, 45649, 90001, 51398, 29659, 37987, 29659, 92559, 23202, 1159, \n",
      "67679, 80331, 100439, 29659, 10303, 29659, 23831, 84067, 121913, 69601, 103472, 120572, 49659, 80210, 29659, 125366, 29659, 67265, 29659, 76394, \n",
      "29659, 37604, 75401, 114854, 38782, 29659, 96382, 29659, 19039, 84422, 46746, 29659, 29659, 75401, 84422, 58811, 29659, 70503, 29659, 121438, \n",
      "29659, 92453, 29659, 51398, 29659, 20972, 46746, 29659, 29659, 75401, 84422, 58811, 84422, 48368, 29659, 39786, 29659, 50674, 46746, 29659, \n",
      "29659, 71553, 29659, 39285, 29659, 76394, 29659, 68935, 29659, 90575, 29659, 113489, 95769, 90001, 81009, 62012, 125160, 127043, 85152, 90001, \n",
      "88579, 90001, 124590, 90001, 35893, 21978, 29659, 123538, 29659, 79333, 92109, 29659, 34713, 76480, 84422, 29659, 116806, 29659, 78072, 29659, \n",
      "71951, 104505, 46746, 29659, 29659, 62124, 84422, 117656, 29659, 37143, 30335, 119867, 29659, 62124, 101946, 46746, 46746, 46746, 29659, 29659, \n",
      "64548, 29659, 114210, 29659, 80365, 29659, 81638, 29659, 24487, 29659, 103652, 84422, 46746, 29659, 29659, 64548, 29659, 37987, 29659, 17651, \n",
      "116384, 29659, 84010, 29659, 107665, 29659, 24487, 29659, 6848, 29659, 37143, 24487, 29659, 81882, 29659, 131072, 114894, 29659, 8824, 29659, \n",
      "80365, 131072, 29659, 107734, 29659, 51398, 29659, 24550, 15158, 46746, 29659, 29659, 114539, 51328, 29659, 11307, 29659, 6848, 29659, 27541, \n",
      "29659, 24487, 29659, 90757, 29659, 110921, 29659, 9823, 29659, 76394, 29659, 86622, 29659, 60894, 29659, 103093, 29659, 27541, 29659, 87338, \n",
      "6251, 74565, 104505, 90525, 113360, 125160, 62452, 84422, 46746, 29659, 29659, 108333, 37561, 29659, 24487, 29659, 4210, 29659, 114210, 29659, \n",
      "27541, 29659, 24487, 29659, 103652, 84422, 46746, 29659, 29659, 55895, 84422, 109777, 104505, 46746, 46746, 46746, 29659, 29659, 79017, 28365, \n",
      "29659, 26711, 127254, 84422, 31456, 84422, 46746, 29659, 29659, 32107, 82505, 29659, 26711, 127254, 90001, 110921, 84422, 46746, 29659, 29659, \n",
      "98829, 29659, 24487, 29659, 84422, 92776, 34444, 29659, 96382, 29659, 55641, 29659, 79333, 92109, 29659, 34713, 76480, 84422, 46746, 29659, \n",
      "29659, 84339, 29659, 26711, 127254, 90001, 110921, 29659, 123538, 29659, 61303, 29659, 90525, 113360, 84422, 46746, 29659, 29659, 51398, 18168, \n",
      "84422, 109777, 104505, 46746, 46746, 46746, 29659, 29659, 108962, 29659, 24487, 29659, 21559, 29659, 57314, 32086, 23354, 29659, 107159, 90001, \n",
      "95166, 29659, 110466, 29659, 26711, 127254, 84422, 31456, 29659, 44405, 29659, 64856, 23354, 29659, 107159, 90001, 32563, 29659, 24487, 29659, \n",
      "110921, 84422, 46746, 29659, 29659, 18795, 84422, 504, 104505, 29659, 22477, 748, 29659, 22339, 29659, 73762, 29659, 37143, 72576, 29659, \n",
      "88039, 29659, 44405, 29659, 24487, 29659, 26845, 29659, 113299, 15158, 46746, 46746, 46746, 29659, 29659, 75401, 84422, 58811, 84422, 58811, \n",
      "29659, 33747, 29659, 94373, 29659, 82218, 46746, 29659, 29659, 79676, 29659, 83874, 29659, 121438, 23354, 29659, 24487, 29659, 98653, 29659, \n",
      "54674, 29659, 12064, 29659, 84165, 118813, 84422, 29659, 71553, 29659, 107734, 29659, 78208, 29659, 21559, 29659, 18927, 29659, 44305, 118101, \n",
      "10007, 29659, 27541, 29659, 122907, 29659, 107159, 90001, 43625, 29659, 119792, 29659, 36097, 84422, 29659, 31869, 29659, 110466, 29659, 24487, \n",
      "29659, 98653, 29659, 44405, 29659, 98022, 29659, 107159, 19422, 23354, 29659, 124759, 29659, 2021, 29659, 27541, 29659, 10170, 90186, 84422, \n",
      "29659, 55621, 29659, 18549, 29659, 17178, 29659, 76394, 29659, 36915, 29659, 33581, 29659, 81638, 29659, 24487, 29659, 110921, 29659, 27541, \n",
      "29659, 19167, 84422, 29659, 74584, 29659, 84010, 23354, 29659, 72576, 29659, 92171, 29659, 112488, 29659, 21003, 29659, 81638, 29659, 16821, \n",
      "29659, 59864, 84422, 46746, 29659, 29659, 21801, 29659, 120047, 29659, 121873, 29659, 15538, 29659, 125880, 29659, 105010, 90001, 14147, 29659, \n",
      "33581, 29659, 27541, 29659, 20129, 84422, 29659, 79676, 29659, 23791, 29659, 24487, 29659, 96382, 29659, 55148, 23354, 29659, 124759, 29659, \n",
      "21998, 29659, 81638, 29659, 24487, 29659, 51208, 29659, 44880, 29659, 14273, 23354, 29659, 112106, 29659, 79333, 92109, 29659, 34713, 76480, \n",
      "29659, 28421, 29659, 34262, 29659, 124759, 29659, 93402, 84422, 29659, 98382, 29659, 109428, 29659, 99828, 29659, 66385, 29659, 44405, 29659, \n",
      "76041, 13745, 29659, 27541, 29659, 124759, 29659, 28421, 29659, 39285, 29659, 27541, 29659, 130449, 29659, 110466, 29659, 19422, 29659, 12064, \n",
      "29659, 84010, 84422, 29659, 117513, 23354, 29659, 24487, 29659, 96382, 29659, 15538, 29659, 90807, 29659, 41880, 29659, 107316, 29659, 120829, \n",
      "29659, 27541, 29659, 39349, 84422, 46746, 29659, 29659, 129289, 29659, 24487, 29659, 83874, 29659, 104717, 29659, 76394, 29659, 51372, 29659, \n",
      "50871, 29659, 44405, 29659, 4583, 29659, 18374, 23354, 29659, 81638, 29659, 81111, 29659, 97661, 29659, 39285, 29659, 38194, 29659, 93402, \n",
      "29659, 24487, 29659, 78393, 29659, 93402, 23791, 29659, 12586, 29659, 37143, 101218, 29659, 24487, 29659, 120047, 29659, 96382, 29659, 27541, \n",
      "29659, 42626, 29659, 41448, 29659, 123538, 29659, 84623, 54433, 29659, 30107, 29659, 38453, 90001, 78561, 29659, 118971, 29659, 21049, 29659, \n",
      "6848, 29659, 11158, 29659, 81638, 29659, 24487, 29659, 114894, 23354, 29659, 114916, 29659, 99992, 29659, 64018, 29659, 24487, 29659, 6848, \n",
      "29659, 27541, 29659, 42626, 29659, 91938, 112613, 23354, 29659, 81638, 29659, 21003, 29659, 23791, 29659, 64018, 29659, 24487, 29659, 96382, \n",
      "84422, 46746, 29659, 29659, 75401, 84422, 58811, 84422, 63150, 29659, 106379, 90001, 18278, 29659, 102017, 46746, 29659, 29659, 103808, 90001, \n",
      "36097, 104505, 29659, 21801, 29659, 110921, 29659, 62374, 29659, 121873, 29659, 37143, 110466, 29659, 125299, 29659, 41003, 54433, 29659, 112143, \n",
      "29659, 24487, 29659, 96382, 29659, 44405, 29659, 88051, 29659, 116826, 29659, 81638, 29659, 29712, 29659, 118971, 29659, 24487, 29659, 6848, \n",
      "29659, 32481, 29659, 37143, 110466, 29659, 124759, 29659, 44405, 29659, 41003, 29659, 121873, 15158, 46746, 29659, 29659, 18278, 104505, 29659, \n",
      "131072, 128484, 29659, 39285, 29659, 76394, 29659, 66084, 104505, 29659, 109761, 125160, 29379, 125160, 7598, 29659, 105102, 86882, 29659, 88423, \n",
      "48275, 102260, 4882, 118825, 29659, 62964, 20757, 29659, 88423, 5063, 29659, 13447, 124934, 29659, 99602, 11126, 4882, 11126, 29659, 69957, \n",
      "116655, 11126, 99001, 11126, 73743, 131072, 46746, 29659, 29659, 6420, 29659, 96903, 29659, 131072, 6433, 131072, 29659, 112143, 29659, 39079, \n",
      "29659, 96163, 29659, 123538, 29659, 24487, 29659, 80365, 29659, 118135, 84422, 46746, 29659, 29659, 21801, 29659, 113590, 104505, 29659, 116029, \n",
      "29659, 24487, 29659, 51372, 29659, 6848, 29659, 118971, 29659, 40153, 29659, 86622, 29659, 13404, 130013, 29659, 647, 93846, 126970, 93846, \n",
      "95739, 29659, 87338, 6251, 74565, 104505, 90525, 113360, 125160, 62452, 84422, 46746, 29659, 29659, 26711, 127254, 90001, 110921, 104505, 29659, \n",
      "111432, 29659, 24487, 29659, 96382, 29659, 51398, 29659, 37548, 29659, 37143, 41003, 29659, 41448, 29659, 11023, 29659, 108110, 29659, 27541, \n",
      "29659, 24487, 29659, 103652, 54433, 29659, 4133, 29659, 24487, 29659, 4583, 23354, 29659, 27304, 29659, 24487, 29659, 54225, 90001, 77197, \n",
      "29659, 114210, 84422, 46746, 29659, 29659, 21801, 29659, 113590, 104505, 29659, 63324, 29659, 81173, 29659, 90575, 29659, 131072, 1253, 11126, \n",
      "14212, 29659, 102343, 116655, 83932, 116655, 11126, 29659, 99001, 86882, 29659, 54516, 95110, 54829, 116655, 29659, 96423, 118825, 69575, 14212, \n",
      "29659, 105102, 86882, 29659, 88423, 48275, 102260, 4882, 118825, 29659, 65496, 54829, 107765, 99602, 29659, 14212, 67301, 29659, 116655, 96090, \n",
      "65496, 82019, 118825, 29659, 88423, 5063, 29659, 96423, 11126, 4882, 29659, 65496, 54829, 92995, 102260, 111070, 29659, 99602, 11126, 4882, \n",
      "11126, 29659, 69957, 116655, 11126, 99001, 11126, 73743, 131072, 46746, 29659, 29659, 18278, 104505, 29659, 131072, 57267, 117656, 29659, 110172, \n",
      "131072, 29659, 54680, 29659, 27541, 29659, 18849, 29659, 124759, 29659, 51398, 29659, 24487, 29659, 113736, 84422, 46746, 46746, 46746, 46746, \n",
      "29659, 29659, 105010, 84422, 29659, 83149, 29659, 32813, 29659, 1301, 29659, 124778, 46746, 29659, 29659, 105010, 84422, 48368, 29659, 35277, \n",
      "29659, 33339, 29659, 14653, 21106, 29659, 117647, 91220, 46746, 29659, 29659, 35753, 51175, 104505, 46746, 29659, 29659, 45772, 104505, 29659, \n",
      "111087, 29659, 53628, 29659, 121041, 29659, 112143, 29659, 24550, 84422, 29659, 14265, 29659, 6848, 29659, 98681, 84422, 46746, 29659, 29659, \n",
      "38499, 104505, 29659, 129289, 105986, 23354, 29659, 109428, 29659, 127695, 29659, 15706, 23354, 29659, 22582, 29659, 81638, 29659, 38453, 90001, \n",
      "80898, 64143, 29659, 71390, 84422, 46746, 29659, 29659, 42765, 76330, 104505, 46746, 29659, 29659, 45772, 104505, 29659, 72492, 33749, 23354, \n",
      "29659, 92985, 29659, 40304, 29659, 81638, 29659, 106541, 29659, 56095, 23354, 29659, 72263, 29659, 27541, 29659, 73768, 84422, 46746, 29659, \n",
      "29659, 38499, 104505, 29659, 113020, 29659, 101457, 23354, 29659, 2341, 90001, 2533, 29659, 115661, 29659, 64018, 29659, 7025, 84422, 29659, \n",
      "75744, 46344, 29659, 77197, 90001, 64018, 90001, 123364, 29659, 56095, 84422, 46746, 29659, 29659, 27043, 90001, 26263, 58866, 29659, 78655, \n",
      "38509, 104505, 46746, 29659, 29659, 45772, 104505, 29659, 76144, 86813, 29659, 27541, 29659, 4123, 29659, 81638, 39854, 29659, 117294, 29659, \n",
      "112143, 29659, 13593, 29659, 26311, 123531, 62769, 23354, 29659, 38802, 29659, 131072, 33061, 131072, 29659, 15706, 29659, 110466, 29659, 124759, \n",
      "131072, 82505, 29659, 51398, 29659, 88932, 84422, 46746, 29659, 29659, 38499, 104505, 29659, 108962, 29659, 124759, 29659, 32921, 29659, 81173, \n",
      "29659, 517, 23354, 29659, 124759, 29659, 18549, 29659, 129560, 29659, 112143, 29659, 5142, 22670, 84422, 29659, 17434, 29659, 4123, 29659, \n",
      "44717, 76330, 29659, 46602, 23354, 29659, 75557, 29659, 81638, 29659, 14823, 29659, 72110, 29659, 112143, 29659, 27297, 82505, 84422, 46746, \n",
      "29659, 29659, 129289, 29659, 21518, 29659, 36968, 23354, 29659, 64548, 29659, 99481, 29659, 18549, 29659, 82722, 29659, 76394, 29659, 85958, \n",
      "29659, 128878, 104505, 29659, 115236, 29659, 24487, 29659, 78655, 38509, 29659, 54695, 29659, 71390, 23354, 29659, 38852, 29659, 115428, 29659, \n",
      "124759, 29659, 90882, 29659, 24487, 29659, 128237, 90001, 25108, 29659, 72685, 23354, 29659, 118971, 29659, 81638, 29659, 113299, 23354, 29659, \n",
      "73747, 29659, 115428, 29659, 24487, 29659, 80365, 29659, 27541, 29659, 76394, 29659, 128237, 90001, 25108, 29659, 72685, 29659, 112143, 29659, \n",
      "81935, 90526, 84422, 29659, 83653, 23354, 29659, 39285, 29659, 41402, 29659, 48148, 29659, 64018, 29659, 114210, 29659, 35409, 29659, 99481, \n",
      "23354, 29659, 21074, 29659, 100530, 90001, 25108, 29659, 112143, 29659, 78655, 38509, 90001, 25108, 23354, 29659, 27541, 29659, 45149, 29659, \n",
      "81111, 29659, 44405, 29659, 90680, 84422, 46746, 29659, 29659, 105010, 84422, 58811, 29659, 93280, 29659, 64018, 29659, 123724, 23354, 29659, \n",
      "65805, 118813, 23354, 29659, 1301, 29659, 6333, 65005, 46746, 29659, 29659, 21801, 29659, 128237, 90001, 25108, 29659, 128878, 29659, 44405, \n",
      "29659, 29523, 29659, 27541, 29659, 8404, 29659, 37480, 29659, 115726, 29659, 27541, 29659, 36191, 29659, 27541, 29659, 89128, 29659, 120351, \n",
      "84422, 46746, 29659, 29659, 21801, 29659, 113408, 90001, 85363, 29659, 128878, 29659, 110128, 29659, 76394, 29659, 93313, 29659, 36482, 29659, \n",
      "37143, 81649, 46320, 29659, 81638, 29659, 24550, 54433, 29659, 37480, 29659, 55148, 29659, 119381, 23354, 29659, 124759, 29659, 44405, 29659, \n",
      "72263, 29659, 27541, 29659, 46157, 29659, 51398, 29659, 37548, 29659, 81638, 39854, 84422, 46746, 29659, 29659, 21801, 29659, 52170, 94056, \n",
      "29659, 128878, 29659, 44405, 29659, 72263, 29659, 27541, 29659, 67296, 29659, 37480, 29659, 107373, 29659, 123442, 29659, 5976, 90001, 29775, \n",
      "29659, 68880, 84422, 46746, 29659, 29659, 64548, 29659, 128226, 29659, 121913, 29659, 44405, 29659, 29396, 29659, 78208, 29659, 123925, 90001, \n",
      "42233, 29659, 96382, 29659, 81638, 29659, 74519, 23354, 29659, 37480, 29659, 84010, 29659, 37636, 131072, 24337, 29659, 118584, 29659, 24242, \n",
      "29659, 24001, 29659, 55641, 29659, 11307, 29659, 46425, 131072, 82505, 29659, 1577, 29659, 118971, 29659, 109550, 29659, 118584, 29659, 76394, \n",
      "29659, 83684, 29659, 6848, 84422, 29659, 2153, 23354, 29659, 84010, 29659, 44405, 29659, 125299, 29659, 108285, 29659, 56132, 84422, 46746, \n",
      "46746, 46746, 46746, 46746, 46746, 46746, 46746, 46746, 46746, 46746, 46746, 46746, 46746, 29659, 29659, 14147, 84422, 29659, 62933, 29659, \n",
      "81638, 29659, 82213, 46746, 29659, 29659, 71553, 29659, 2105, 29659, 902, 29659, 99523, 29659, 81638, 29659, 102312, 29659, 71390, 104505, \n",
      "46746, 29659, 29659, 85692, 29659, 1301, 29659, 16633, 29659, 6293, 109531, 29659, 117220, 46746, 29659, 29659, 57870, 29659, 55641, 29659, \n",
      "48368, 23354, 64360, 29659, 27541, 29659, 81161, 29659, 112143, 29659, 68096, 29659, 1025, 23354, 29659, 120044, 29659, 38802, 29659, 130894, \n",
      "29659, 104998, 29659, 37143, 42035, 23354, 29659, 107124, 23354, 29659, 110204, 76516, 29659, 38802, 29659, 27584, 29659, 92832, 29659, 11193, \n",
      "29659, 37143, 109207, 23354, 29659, 39133, 122016, 23354, 29659, 4750, 54433, 29659, 118971, 29659, 38802, 29659, 40095, 29659, 35389, 29659, \n",
      "22414, 84422, 29659, 58237, 29659, 71951, 29659, 121041, 90001, 38194, 29659, 130013, 29659, 112143, 29659, 69450, 29659, 26311, 123531, 62769, \n",
      "84422, 46746, 29659, 29659, 120077, 29659, 24487, 29659, 55153, 29659, 123538, 29659, 120776, 29659, 112197, 29659, 1626, 104505, 46746, 29659, \n",
      "29659, 127364, 29659, 59864, 46746, 29659, 29659, 65805, 29659, 102183, 46746, 29659, 29659, 96586, 29659, 25108, 29659, 99412, 29659, 72110, \n",
      "46746, 29659, 29659, 85692, 29659, 68946, 29659, 64018, 29659, 72110, 46746, 29659, 29659, 86693, 29659, 68626, 63938, 29659, 35389, 29659, \n",
      "22414, 46746, 29659, 29659, 49163, 74623, 29659, 95449, 29659, 112143, 29659, 49417, 29659, 95449, 29659, 130013, 29659, 37143, 90575, 29659, \n",
      "63622, 29659, 109217, 29659, 72615, 29659, 112143, 29659, 104182, 29659, 3254, 55972, 29659, 27541, 29659, 42948, 29659, 24487, 29659, 93136, \n",
      "29659, 37160, 23354, 29659, 112143, 29659, 82722, 29659, 130013, 29659, 55641, 29659, 902, 29659, 75070, 84422, 46746, 29659, 29659, 75725, \n",
      "90001, 82123, 29659, 112143, 29659, 2176, 29659, 26263, 99687, 46746, 29659, 29659, 86238, 83202, 29659, 124590, 29659, 103382, 29659, 10303, \n",
      "29659, 39442, 29659, 117264, 29659, 90575, 29659, 81893, 23354, 29659, 88141, 51548, 103770, 23354, 29659, 112143, 29659, 35409, 29659, 124461, \n",
      "29659, 53024, 29659, 76394, 29659, 38802, 29659, 100192, 29659, 96382, 84422, 46746, 29659, 29659, 81889, 29659, 44550, 29659, 76394, 29659, \n",
      "131072, 127346, 90001, 64018, 90001, 120747, 131072, 29659, 128878, 29659, 10303, 29659, 6319, 90001, 172, 90001, 6319, 29659, 71390, 29659, \n",
      "81638, 29659, 119792, 29659, 44717, 76330, 29659, 124590, 84422, 46746, 29659, 29659, 64548, 29659, 108988, 29659, 123925, 29659, 85363, 29659, \n",
      "128878, 29659, 95769, 29659, 17132, 29659, 42626, 29659, 68608, 23354, 29659, 81111, 29659, 124461, 29659, 38852, 29659, 42626, 29659, 16661, \n",
      "29659, 113408, 116468, 29659, 81638, 29659, 124590, 29659, 35893, 21978, 29659, 46425, 84422, 29659, 37143, 85315, 29659, 11307, 29659, 109550, \n",
      "29659, 63739, 29659, 81322, 29659, 38802, 55972, 46746, 29659, 29659, 27043, 90001, 55068, 24271, 29659, 36566, 29659, 98116, 46746, 29659, \n",
      "29659, 27096, 29659, 76394, 29659, 14823, 29659, 68946, 29659, 64018, 29659, 130013, 29659, 97713, 84422, 113040, 100523, 29659, 110856, 29659, \n",
      "104419, 29659, 18549, 29659, 39285, 29659, 61461, 29659, 11193, 29659, 90575, 29659, 131072, 27987, 125160, 34203, 23354, 131072, 29659, 131072, \n",
      "39133, 122016, 23354, 131072, 29659, 131072, 62751, 23354, 131072, 29659, 131072, 118435, 125160, 58643, 23354, 131072, 29659, 81111, 29659, 92171, \n",
      "29659, 107373, 29659, 107734, 29659, 51398, 29659, 108719, 29659, 112143, 29659, 108988, 29659, 120351, 84422, 29659, 46424, 29659, 104419, 29659, \n",
      "124461, 29659, 18849, 29659, 75977, 29659, 12618, 29659, 71390, 84422, 46746, 29659, 29659, 40286, 29659, 25138, 46746, 29659, 29659, 115236, \n",
      "29659, 24487, 29659, 96382, 29659, 93894, 29659, 75977, 29659, 120769, 23354, 29659, 38852, 29659, 58360, 29659, 112143, 29659, 40304, 29659, \n",
      "124759, 29659, 10303, 29659, 76394, 29659, 128237, 90001, 25108, 29659, 128878, 84422, 29659, 108962, 29659, 24487, 29659, 96382, 131072, 82505, \n",
      "29659, 71390, 29659, 22061, 29659, 55641, 29659, 8930, 29659, 56095, 23354, 29659, 24487, 29659, 99481, 29659, 124461, 29659, 3822, 29659, \n",
      "112143, 29659, 4004, 29659, 61086, 23354, 29659, 237, 29659, 5025, 29659, 24487, 29659, 120769, 29659, 64018, 29659, 65458, 29659, 17583, \n",
      "84422, 46746, 29659, 29659, 36688, 29659, 4653, 29659, 131064, 46746, 29659, 29659, 17434, 29659, 103652, 29659, 24001, 29659, 18549, 29659, \n",
      "39285, 29659, 121041, 90001, 18636, 29659, 114210, 23354, 29659, 97713, 84422, 113040, 100523, 29659, 131072, 89073, 29659, 70419, 29659, 64848, \n",
      "129713, 29659, 88423, 5063, 29659, 26199, 113040, 29659, 99001, 33914, 75919, 84422, 131072, 29659, 21801, 29659, 99481, 29659, 92364, 29659, \n",
      "83920, 29659, 13593, 29659, 108867, 29659, 118971, 29659, 13593, 29659, 40071, 84422, 29659, 64548, 29659, 100192, 29659, 121041, 90001, 21523, \n",
      "29659, 71231, 29659, 121913, 29659, 18549, 29659, 42626, 29659, 101273, 84422, 29659, 37143, 125366, 29659, 44405, 29659, 2345, 29659, 27541, \n",
      "29659, 24533, 29659, 108988, 69755, 29659, 172, 29659, 76394, 29659, 33402, 55972, 46746, 46746, 46746, 29659, 29659, 7020, 84422, 29659, \n",
      "88261, 29659, 1301, 29659, 53722, 60217, 46746, 29659, 29659, 7020, 84422, 48368, 29659, 88261, 46746, 29659, 29659, 8270, 29659, 11307, \n",
      "29659, 126225, 23354, 29659, 97661, 29659, 87749, 29659, 54919, 29659, 24487, 29659, 104419, 29659, 64018, 29659, 38453, 90001, 80898, 64143, \n",
      "29659, 124590, 29659, 103382, 29659, 81638, 29659, 18456, 76330, 29659, 103028, 121287, 9679, 29659, 123538, 29659, 130036, 23354, 29659, 104998, \n",
      "23354, 29659, 118971, 29659, 92832, 29659, 11193, 84422, 29659, 71553, 29659, 94528, 29659, 76394, 29659, 93136, 29659, 55153, 29659, 64018, \n",
      "29659, 48368, 23354, 64360, 29659, 14357, 23354, 29659, 28093, 29659, 108142, 29659, 46320, 29659, 103028, 29659, 118971, 29659, 902, 29659, \n",
      "120351, 23354, 29659, 119792, 29659, 45382, 29659, 92962, 29659, 119970, 29659, 94344, 29659, 118971, 29659, 51080, 29659, 115661, 84422, 29659, \n",
      "83653, 29659, 97661, 29659, 21049, 104505, 46746, 29659, 29659, 35753, 51175, 29659, 33339, 104505, 29659, 21039, 29659, 122428, 23354, 29659, \n",
      "112316, 96023, 84422, 46746, 29659, 29659, 42765, 76330, 29659, 33339, 104505, 29659, 74803, 90001, 25108, 23354, 29659, 84219, 29659, 62705, \n",
      "29659, 81638, 29659, 106541, 29659, 56095, 29659, 37480, 29659, 82328, 29659, 15706, 84422, 46746, 29659, 29659, 27043, 90001, 26263, 58866, \n",
      "29659, 78655, 38509, 104505, 29659, 64548, 29659, 19473, 90001, 41606, 29659, 78655, 76394, 124081, 29659, 63150, 84422, 48368, 29659, 96382, \n",
      "29659, 42233, 29659, 123538, 29659, 47633, 29659, 55153, 84422, 29659, 125366, 29659, 128878, 29659, 124790, 29659, 72713, 29659, 19122, 29659, \n",
      "123538, 29659, 24487, 29659, 120100, 29659, 52180, 84422, 29659, 10347, 29659, 21528, 29659, 59675, 29659, 41402, 29659, 13777, 90001, 64630, \n",
      "29659, 4998, 29659, 37143, 27297, 29659, 71390, 23354, 29659, 13377, 29659, 35389, 29659, 46931, 23354, 29659, 101053, 48464, 46746, 29659, \n",
      "29659, 54907, 23354, 29659, 97661, 29659, 96230, 29659, 24487, 29659, 93494, 29659, 96382, 29659, 27541, 29659, 76394, 29659, 117508, 54640, \n",
      "29659, 81638, 39854, 29659, 81638, 29659, 37548, 29659, 56404, 23354, 29659, 93585, 29659, 123538, 29659, 76394, 29659, 79333, 92109, 29659, \n",
      "34713, 76480, 29659, 22802, 29659, 10303, 29659, 76394, 29659, 30335, 119867, 29659, 47587, 29659, 126869, 29659, 118971, 29659, 26711, 127254, \n",
      "84422, 31456, 114854, 25108, 29659, 110921, 84422, 29659, 21801, 29659, 122976, 29659, 75193, 29659, 44405, 29659, 1199, 29659, 63739, 90001, \n",
      "115416, 23354, 29659, 10303, 29659, 72016, 29659, 24550, 29659, 63739, 29659, 69556, 29659, 64848, 46320, 29659, 123538, 29659, 78208, 29659, \n",
      "4949, 29659, 72960, 29659, 118971, 29659, 125880, 29659, 68895, 80502, 84422, 113360, 29659, 81638, 29659, 113299, 29659, 8404, 54433, 29659, \n",
      "118971, 29659, 83874, 29659, 37548, 29659, 4583, 29659, 123538, 29659, 70503, 29659, 121438, 84422, 46746, 29659, 29659, 129962, 29659, 115420, \n",
      "104505, 29659, 71553, 29659, 104358, 29659, 27541, 29659, 20579, 29659, 38802, 29659, 130013, 23354, 29659, 83920, 29659, 121041, 90001, 18636, \n",
      "29659, 114210, 23354, 29659, 101026, 29659, 24487, 29659, 123364, 29659, 81638, 29659, 24487, 29659, 45813, 29659, 128878, 23354, 29659, 118971, \n",
      "29659, 55687, 29659, 76394, 29659, 131072, 85958, 29659, 128878, 131072, 29659, 84010, 29659, 10593, 119030, 29659, 78655, 38509, 29659, 76958, \n",
      "29659, 10303, 29659, 128237, 90001, 25108, 29659, 92679, 29659, 81638, 29659, 83324, 90001, 38379, 29659, 15706, 29659, 51398, 29659, 95449, \n",
      "29659, 5976, 84422, 46746, 29659, 29659, 7020, 84422, 58811, 29659, 50197, 29659, 1301, 29659, 85566, 46746, 29659, 29659, 99665, 29659, \n",
      "85566, 104505, 46746, 29659, 29659, 15836, 29659, 117220, 104505, 29659, 113489, 95769, 90001, 81009, 62012, 125160, 127043, 85152, 90001, 124590, \n",
      "90001, 14892, 90001, 89431, 100216, 90001, 58811, 84422, 80502, 90001, 81652, 90001, 73258, 90001, 80502, 48368, 90001, 8219, 90001, 4269, \n",
      "90001, 124761, 46746, 29659, 29659, 27043, 90001, 26263, 58866, 29659, 31308, 104505, 29659, 113489, 95769, 90001, 81009, 62012, 125160, 127043, \n",
      "85152, 90001, 124590, 90001, 35893, 21978, 90001, 26711, 127254, 90001, 63150, 84422, 48368, 90001, 56274, 46746, 29659, 29659, 117508, 54640, \n",
      "29659, 31308, 104505, 29659, 113489, 95769, 90001, 81009, 62012, 125160, 127043, 85152, 90001, 124590, 90001, 35893, 21978, 90001, 26711, 127254, \n",
      "90001, 63150, 84422, 48368, 90001, 56274, 90001, 92776, 34444, 46746, 29659, 29659, 70503, 29659, 121438, 29659, 45159, 104505, 29659, 113489, \n",
      "95769, 90001, 81009, 62012, 125160, 127043, 85152, 90001, 88579, 90001, 124590, 90001, 35893, 21978, 46746, 29659, 29659, 18225, 29659, 85566, \n",
      "104505, 46746, 29659, 29659, 1401, 29659, 31308, 29659, 24550, 29659, 125694, 29659, 10303, 29659, 51982, 29659, 112949, 29659, 78269, 104505, \n",
      "29659, 53573, 6251, 3102, 121923, 84422, 112331, 84422, 83780, 84422, 73248, 125160, 92353, 125160, 112108, 70033, 90001, 45831, 61910, 118586, \n",
      "31655, 35772, 46746, 29659, 29659, 59069, 102896, 29659, 50084, 123728, 29659, 81638, 29659, 94344, 29659, 4133, 29659, 37143, 105010, 29659, \n",
      "62472, 29659, 75595, 29659, 77197, 29659, 64018, 29659, 37369, 29659, 72016, 101946, 29659, 53573, 6251, 4271, 84422, 97169, 102896, 84422, \n",
      "88579, 125160, 75979, 125160, 87248, 95769, 3020, 107159, 71026, 90001, 18416, 90001, 129324, 54856, 37855, 125160, 46746, 29659, 29659, 31308, \n",
      "29659, 122591, 75711, 29659, 122589, 6458, 29659, 125694, 104505, 29659, 53573, 6251, 3102, 121923, 84422, 112331, 84422, 83780, 84422, 73248, \n",
      "125160, 92353, 125160, 45831, 61910, 118586, 31655, 35772, 46746, 29659, 29659, 117508, 54640, 29659, 31308, 29659, 122591, 75711, 29659, 122589, \n",
      "6458, 29659, 125694, 104505, 29659, 53573, 6251, 3102, 121923, 84422, 112331, 84422, 83780, 84422, 73248, 125160, 92353, 25140, 61910, 118586, \n",
      "31655, 35772, 46746, 29659, 29659, 15836, 29659, 117220, 29659, 80199, 29659, 61337, 29659, 37143, 849, 29659, 125259, 29659, 57029, 101946, \n",
      "29659, 53573, 6251, 8841, 99648, 84422, 83780, 84422, 73248, 125160, 62124, 125160, 122428, 91220, 121770, 114817, 25665, 36937, 36937, 59268, \n",
      "17058, 69819, 36937, 36937, 59268, 36937, 96779, 123506, 59268, 109772, 36937, 59268, 17058, 59268, 129231, 36937, 59268, 123506, 59268, 11129, \n",
      "36937, 59268, 17058, 41033, 544, 66399, 73847, 27930, 32443, 29379, 125953, 36937, 59268, 123506, 59268, 46189, 30445, 36937, 59268, 17058, \n",
      "25665, 36937, 94693, 36937, 94693, 1301, 61910, 118586, 31655, 35772, 46746, 29659, 29659, 70503, 29659, 121438, 29659, 45159, 29659, 14196, \n",
      "29659, 45242, 20402, 104505, 29659, 53573, 6251, 90132, 84422, 73248, 125160, 113489, 95769, 90001, 81009, 8901, 71026, 125160, 117508, 54640, \n",
      "90001, 70503, 90001, 74519, 46746, 29659, 29659, 117508, 54640, 29659, 114676, 29659, 125694, 104505, 29659, 53573, 6251, 3102, 121923, 84422, \n",
      "112331, 84422, 83780, 84422, 73248, 125160, 92353, 25140, 61910, 118586, 31655, 35772, 46746, 29659, 29659, 43867, 29659, 31308, 29659, 45242, \n",
      "39847, 46015, 104111, 29659, 17902, 5996, 104505, 29659, 53573, 6251, 3102, 121923, 84422, 112331, 84422, 83780, 84422, 73248, 125160, 92353, \n",
      "25140, 61910, 118586, 31655, 35772, 46746, 29659, 29659, 59069, 102896, 29659, 50084, 123728, 29659, 81638, 29659, 45242, 39847, 46015, 104111, \n",
      "29659, 8404, 104505, 29659, 53573, 6251, 4271, 84422, 97169, 102896, 84422, 88579, 125160, 75979, 125160, 87248, 95769, 3020, 107159, 71026, \n",
      "90001, 18416, 90001, 129324, 54856, 37855, 125160, 46746, 29659, 29659, 52223, 29659, 77824, 29659, 125694, 104505, 29659, 53573, 6251, 3102, \n",
      "121923, 84422, 112331, 84422, 83780, 84422, 73248, 125160, 92353, 25140, 61910, 118586, 31655, 35772, 46746, 29659, 29659, 117508, 54640, 29659, \n",
      "31308, 29659, 125732, 29659, 19039, 104505, 29659, 53573, 6251, 92353, 84422, 83780, 84422, 73248, 125160, 19039, 125160, 40011, 48102, 127622, \n",
      "46746, 29659, 29659, 103188, 90001, 59604, 29659, 125732, 29659, 19039, 104505, 29659, 53573, 6251, 92353, 84422, 83780, 84422, 73248, 125160, \n",
      "19039, 125160, 40011, 48102, 127622, 91220, 61910, 118586, 31655, 35772, 46746, 29659, 29659, 31308, 29659, 129213, 29659, 37143, 51080, 23354, \n",
      "29659, 94344, 23354, 29659, 118971, 29659, 72016, 29659, 130013, 29659, 32868, 127161, 23354, 29659, 51398, 29659, 122262, 29659, 118971, 29659, \n",
      "7133, 10468, 29659, 81638, 39854, 23354, 29659, 81111, 29659, 92171, 29659, 61910, 69292, 90001, 7020, 29659, 118971, 29659, 61910, 69292, \n",
      "90001, 7020, 90001, 82505, 50164, 29659, 67803, 84422, 29659, 3236, 29659, 28229, 29659, 61910, 69292, 90001, 7020, 90001, 82505, 50164, \n",
      "29659, 78794, 23354, 29659, 237, 29659, 46730, 29659, 81638, 29659, 104138, 23354, 29659, 61910, 69292, 90001, 7020, 29659, 44405, 29659, \n",
      "24487, 29659, 96392, 29659, 120480, 29659, 121913, 29659, 107734, 29659, 81638, 29659, 68276, 29659, 58598, 23354, 29659, 237, 29659, 124759, \n",
      "29659, 44405, 29659, 120495, 29659, 76631, 29659, 43246, 101946, 46746, 29659, 29659, 48368, 84422, 29659, 75961, 102206, 104505, 23435, 23435, \n",
      "23435, 23435, 23435, 23435, 23435, 23435, 23435, 23435, 23435, 29659, 29659, 29659, 29659, 29659, 29659, 90001, 29659, 51080, 11424, 21553, \n",
      "122823, 22647, 84422, 49897, 29659, 37143, 61910, 69292, 90001, 7020, 29659, 67803, 101946, 29659, 53573, 6251, 92353, 84422, 83780, 84422, \n",
      "73248, 125160, 19039, 125160, 40011, 24211, 8181, 125160, 127622, 91220, 61910, 118586, 31655, 35772, 29659, 29659, 29659, 29659, 29659, 29659, \n",
      "29659, 29659, 29659, 29659, 90001, 29659, 51080, 11424, 21553, 122823, 128252, 54918, 88064, 84422, 49897, 29659, 37143, 61910, 69292, 90001, \n",
      "7020, 90001, 82505, 50164, 29659, 67803, 101946, 29659, 53573, 6251, 92353, 84422, 83780, 84422, 73248, 125160, 19039, 125160, 40011, 48102, \n",
      "127622, 91220, 61910, 118586, 31655, 35772, 46746, 29659, 29659, 58811, 84422, 29659, 74120, 104505, 23435, 23435, 23435, 23435, 23435, 23435, \n",
      "23435, 23435, 23435, 23435, 23435, 29659, 29659, 29659, 29659, 29659, 29659, 90001, 29659, 10849, 73130, 86837, 59835, 46363, 84422, 49897, \n",
      "29659, 37143, 61910, 69292, 90001, 7020, 29659, 67803, 101946, 29659, 53573, 6251, 92353, 84422, 83780, 84422, 73248, 125160, 19039, 125160, \n",
      "40011, 125160, 48368, 13404, 62691, 127622, 91220, 61910, 118586, 31655, 35772, 29659, 29659, 29659, 29659, 29659, 29659, 29659, 29659, 29659, \n",
      "90001, 29659, 10849, 73130, 86837, 59835, 80735, 54918, 88064, 84422, 49897, 29659, 37143, 61910, 69292, 90001, 7020, 90001, 82505, 50164, \n",
      "29659, 67803, 101946, 29659, 53573, 6251, 92353, 84422, 83780, 84422, 73248, 125160, 19039, 125160, 40011, 74689, 125160, 127622, 91220, 61910, \n",
      "118586, 31655, 35772, 46746, 29659, 29659, 63150, 84422, 29659, 84547, 104505, 23435, 23435, 23435, 23435, 23435, 23435, 23435, 23435, 23435, \n",
      "23435, 23435, 29659, 29659, 29659, 29659, 29659, 29659, 90001, 29659, 73130, 86837, 59835, 46363, 84422, 49897, 29659, 37143, 61910, 69292, \n",
      "90001, 7020, 29659, 67803, 101946, 29659, 53573, 6251, 92353, 84422, 83780, 84422, 73248, 125160, 19039, 125160, 40011, 48102, 127622, 91220, \n",
      "61910, 118586, 31655, 35772, 29659, 29659, 29659, 29659, 29659, 29659, 29659, 29659, 29659, 29659, 90001, 29659, 73130, 86837, 59835, 80735, \n",
      "54918, 88064, 84422, 49897, 29659, 37143, 61910, 69292, 90001, 7020, 90001, 82505, 50164, 29659, 67803, 101946, 29659, 53573, 6251, 92353, \n",
      "84422, 83780, 84422, 73248, 125160, 19039, 125160, 40011, 74689, 125160, 127622, 91220, 61910, 118586, 31655, 35772, 46746, 29659, 29659]\n"
     ]
    }
   ],
   "source": [
    "tokeniser.visualise_token_ids(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
